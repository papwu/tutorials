<!DOCTYPE html>
<html lang="en-US">
<head>
<title>Python Digital Forensics - Quick Guide</title>
<meta charset="utf-8">
<meta name="description" content="Python Digital Forensics - Quick Guide - This chapter will give you an introduction to what digital forensics is all about, and its historical review. You will also understand where you can apply digit"/>
<meta name="keywords" content="C, C++, Python, Java, HTML, CSS, JavaScript, SQL, PHP, jQuery, XML, DOM, Bootstrap, Tutorials, Articles, Programming, training, learning, quiz, preferences, examples, code"/>
<link rel="canonical" href="https://www.tutorialspoint.com/python_digital_forensics/python_digital_forensics_quick_guide.htm" />
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<meta name="viewport" content="width=device-width,initial-scale=1.0,user-scalable=yes">
<script src="/theme/js/script-min-v2.js?v=3"></script>
<link rel="stylesheet" href="/theme/css/style-min-v2.css?v=6">
<script src="//services.bilsyndication.com/adv1/?d=901" defer="" async=""></script>
<script> var vitag = vitag || {};</script>
<script> vitag.outStreamConfig = { enablePC: false, enableMobile: false };</script>  
<style>
.right-menu .mui-btn {
    background-color:#e84802;
}
a.demo {
    background:#e84802;
}
li.heading {
    background:#e84802;
}
.course-box{background:#e84802}
.home-intro-sub p{color:#e84802}
</style>
</head>
<body>
<header id="header">
<!-- Top sub-menu Starts Here -->
<div class="mui-appbar mui-container-fulid top-menu">
<div class="mui-container">
<div class="top-menu-item home">
<a href="https://www.tutorialspoint.com/index.htm" target="_blank" title="TutorialsPoint - Home"><i class="fal fa-home"></i> <span>Home</span></a>
</div>
<div class="top-menu-item qa">
<a href="https://www.tutorialspoint.com/about/about_careers.htm" target="_blank" title="Job @ Tutorials Point"><i class="fa fa-suitcase"></i> <span>Jobs</span></a>
</div>
<div class="top-menu-item tools">
<a href="https://www.tutorialspoint.com/online_dev_tools.htm" target="_blank" title="Tools - Online Development and Testing Tools"><i class="fal fa-cogs"></i> <span>Tools</span></a>
</div>
<div class="top-menu-item coding-ground">
<a href="https://www.tutorialspoint.com/codingground.htm" target="_blank" title="Coding Ground - Free Online IDE and Terminal"><i class="fal fa-code"></i> <span>Coding Ground </span></a> 
</div>
<div class="top-menu-item current-affairs">
<a href="https://www.tutorialspoint.com/current_affairs.htm" target="_blank" title="Daily Current Affairs"><i class="fal fa-layer-plus"></i> <span>Current Affairs</span></a>
</div>
<div class="top-menu-item upsc-notes">
<a href="https://www.tutorialspoint.com/upsc_ias_exams.htm" target="_blank" title="UPSC IAS Exams Notes - TutorialsPoint"><i class="fal fa-user-tie"></i> <span>UPSC Notes</span></a>
</div>      
<div class="top-menu-item online-tutoris">
<a href="https://www.tutorialspoint.com/tutor_connect/index.php" target="_blank" title="Top Online Tutors - Tutor Connect"><i class="fal fa-user"></i> <span>Online Tutors</span></a>
</div>
<div class="top-menu-item whiteboard">
<a href="https://www.tutorialspoint.com/whiteboard.htm" target="_blank" title="Free Online Whiteboard"><i class="fal fa-chalkboard"></i> <span>Whiteboard</span></a>
</div>
<div class="top-menu-item net-meeting">
<a href="https://www.tutorialspoint.com/netmeeting.php" target="_blank" title="A free tool for online video conferencing"><i class="fal fa-chalkboard-teacher"></i> <span>Net Meeting</span></a> 
</div>
<div class="top-menu-item articles">
<a href="https://www.tutorix.com" target="_blank" title="Tutorx - The Best Learning App" rel="nofollow"><i class="fal fa-video"></i> <span>Tutorix</span></a> 
</div>        
<div class="social-menu-item">
<a href="https://www.facebook.com/tutorialspointindia" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Facebook"><i class="fab fa-facebook-f"></i></a> 
<a href="https://www.twitter.com/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Twitter"><i class="fab fa-twitter"></i></a>
<a href="https://www.linkedin.com/company/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Linkedin"><i class="fab fa-linkedin-in"></i></a>
<a href="https://www.youtube.com/channel/UCVLbzhxVTiTLiVKeGV7WEBg" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint YouTube"><i class="fab fa-youtube"></i></a>
</div>        
</div>
</div>
<!-- Top sub-menu Ends Here -->
<!-- Top main-menu Starts Here -->
<div class="mui-appbar mui-container-fulid mui--appbar-line-height mui--z1" id="logo-menu">
<div class="mui-container">
<div class="left-menu">
<a href="https://www.tutorialspoint.com/index.htm" title="Tutorialspoint">
<img class="tp-logo" alt="tutorialspoint" src="/python_digital_forensics/images/logo.png">
</a>
<div class="mui-dropdown">
<a class="mui-btn mui-btn--primary categories" data-mui-toggle="dropdown"><i class="fa fa-th-large"></i> 
<span>Categories <span class="mui-caret"></span></span></a>            
<ul class="mui-dropdown__menu cat-menu">
<li>
<ul>
<li><a href="/academic_tutorials.htm"><i class="fa fa-caret-right"></i> Academic Tutorials</a></li>
<li><a href="/big_data_tutorials.htm"><i class="fa fa-caret-right"></i> Big Data &amp; Analytics </a></li>
<li><a href="/computer_programming_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Programming </a></li>
<li><a href="/computer_science_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Science </a></li>
<li><a href="/database_tutorials.htm"><i class="fa fa-caret-right"></i> Databases </a></li>
<li><a href="/devops_tutorials.htm"><i class="fa fa-caret-right"></i> DevOps </a></li>
<li><a href="/digital_marketing_tutorials.htm"><i class="fa fa-caret-right"></i> Digital Marketing </a></li>
<li><a href="/engineering_tutorials.htm"><i class="fa fa-caret-right"></i> Engineering Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> Exams Syllabus </a></li>
<li><a href="/famous_monuments.htm"><i class="fa fa-caret-right"></i> Famous Monuments </a></li>
<li><a href="/gate_exams_tutorials.htm"><i class="fa fa-caret-right"></i> GATE Exams Tutorials</a></li>
<li><a href="/latest_technologies.htm"><i class="fa fa-caret-right"></i> Latest Technologies </a></li>
<li><a href="/machine_learning_tutorials.htm"><i class="fa fa-caret-right"></i> Machine Learning </a></li>
<li><a href="/mainframe_tutorials.htm"><i class="fa fa-caret-right"></i> Mainframe Development </a></li>
<li><a href="/management_tutorials.htm"><i class="fa fa-caret-right"></i> Management Tutorials </a></li>
<li><a href="/maths_tutorials.htm"><i class="fa fa-caret-right"></i> Mathematics Tutorials</a></li>
<li><a href="/microsoft_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Microsoft Technologies </a></li>
<li><a href="/misc_tutorials.htm"><i class="fa fa-caret-right"></i> Misc tutorials </a></li>
<li><a href="/mobile_development_tutorials.htm"><i class="fa fa-caret-right"></i> Mobile Development </a></li>
<li><a href="/java_technology_tutorials.htm"><i class="fa fa-caret-right"></i> Java Technologies </a></li>
<li><a href="/python_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Python Technologies </a></li>
<li><a href="/sap_tutorials.htm"><i class="fa fa-caret-right"></i> SAP Tutorials </a></li>
<li><a href="/scripting_lnaguage_tutorials.htm"><i class="fa fa-caret-right"></i>Programming Scripts </a></li>
<li><a href="/selected_reading.htm"><i class="fa fa-caret-right"></i> Selected Reading </a></li>
<li><a href="/software_quality_tutorials.htm"><i class="fa fa-caret-right"></i> Software Quality </a></li>
<li><a href="/soft_skill_tutorials.htm"><i class="fa fa-caret-right"></i> Soft Skills </a></li>
<li><a href="/telecom_tutorials.htm"><i class="fa fa-caret-right"></i> Telecom Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> UPSC IAS Exams </a></li>
<li><a href="/web_development_tutorials.htm"><i class="fa fa-caret-right"></i> Web Development </a></li>
<li><a href="/sports_tutorials.htm"><i class="fa fa-caret-right"></i> Sports Tutorials </a></li>
<li><a href="/xml_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> XML Technologies </a></li>
<li><a href="/multi_language_tutorials.htm"><i class="fa fa-caret-right"></i> Multi-Language Tutorials</a></li>
<li><a href="/questions_and_answers.htm"><i class="fa fa-caret-right"></i> Interview Questions</a></li>
</ul>
</li>
</ul>
<div class="clear"></div>
</div> 
</div>
<div class="right-menu">
<div class="toc-toggle">
<a href="javascript:void(0);"><i class="fa fa-bars"></i></a>
</div>
<div class="mobile-search-btn">
<a href="https://www.tutorialspoint.com/search.htm"><i class="fal fa-search"></i></a>
</div>
<div class="search-box">
<form method="get" class="" name="searchform" action="https://www.google.com/search" target="_blank" novalidate="">
<input type="hidden" name="sitesearch" value="www.tutorialspoint.com" class="user-valid valid">
<input class="header-search-box" type="text" id="search-string" name="q" placeholder="Search your favorite tutorials..." onfocus="if (this.value == 'Search your favorite tutorials...') {this.value = '';}" onblur="if (this.value == '') {this.value = 'Search your favorite tutorials...';}" autocomplete="off">
<button><i class="fal fa-search"></i></button>
</form>
</div>
<div class="menu-btn library-btn">
<a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a>
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a> 
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/questions/index.php"><i class="fa fa-location-arrow"></i> <span>Q/A</span></a>
</div>
<div class="menu-btn ebooks-btn">
<a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a>
</div>
<div class="mui-dropdown">
<button class="mui-btn mui-btn--primary" data-mui-toggle="dropdown">
<span class="mui-caret"></span>
</button>
<ul class="mui-dropdown__menu">
<li><a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a></li>
<li><a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a></li>
<li><a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a></li>
</ul>
</div>
</div>
</div>
</div>
<!-- Top main-menu Ends Here -->
</header>
<div class="mui-container-fluid content">
<div class="mui-container">
<!-- Tutorial ToC Starts Here -->
<div class="mui-col-md-3 tutorial-toc">
<div class="mini-logo">
<img src="/python_digital_forensics/images/python-digital-forensics-mini-logo.jpg" alt="Python Digital Forensics Tutorial" />
</div>
<ul class="toc chapters">
<li class="heading">Python Digital Forensics</li>
<li><a href="/python_digital_forensics/index.htm">Python Digital Forensics - Home</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_introduction.htm">Introduction</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_getting_started.htm">Getting Started With Python</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_artifact_report.htm">Artifact Report</a></li>
<li><a href="/python_digital_forensics/python_digital_mobile_device_forensics.htm">Mobile Device Forensics</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_investigating_embedded_metadata.htm">Investigating Embedded Metadata</a></li>
<li><a href="/python_digital_forensics/python_digital_network_forensics.htm">Network Forensics-I</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_network.htm">Network Forensics-II</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_investigation_using_emails.htm">Investigation Using Emails</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_important_artifacts_in_windows.htm">Important Artifacts In Windows-I</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_important_artifacts_in_ms_windows.htm">Important Artifacts In Windows-II</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_important_artifacts_in_microsoft_windows.htm">Important Artifacts In Windows-III</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_investigation_of_log_based_artifacts.htm">Investigation Of Log Based Artifacts</a></li>
</ul>
<ul class="toc chapters">
<li class="heading">Python Digital Forensics Resources</li>
<li><a href="/python_digital_forensics/python_digital_forensics_quick_guide.htm">Quick Guide</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_useful_resources.htm">Python Digital Forensics - Resources</a></li>
<li><a href="/python_digital_forensics/python_digital_forensics_discussion.htm">Python Digital Forensics - Discussion</a></li>
</ul>
<ul class="toc reading">
<li class="sreading">Selected Reading</li>
<li><a target="_top" href="/upsc_ias_exams.htm">UPSC IAS Exams Notes</a></li>
<li><a target="_top" href="/developers_best_practices/index.htm">Developer's Best Practices</a></li>
<li><a target="_top" href="/questions_and_answers.htm">Questions and Answers</a></li>
<li><a target="_top" href="/effective_resume_writing.htm">Effective Resume Writing</a></li>
<li><a target="_top" href="/hr_interview_questions/index.htm">HR Interview Questions</a></li>
<li><a target="_top" href="/computer_glossary.htm">Computer Glossary</a></li>
<li><a target="_top" href="/computer_whoiswho.htm">Who is Who</a></li>
</ul>
</div>
<!-- Tutorial ToC Ends Here -->
<!-- Tutorial Content Starts Here -->
<div class="mui-col-md-6 tutorial-content">
<h1>Python Digital Forensics - Quick Guide</h1>
<hr />
<div class="top-ad-heading">Advertisements</div>
<div style="text-align: center;">
<script><!--
google_ad_client = "pub-7133395778201029";
var width = document.getElementsByClassName("tutorial-content")[0].clientWidth - 40;
google_ad_width = width;
google_ad_height = 150;
google_ad_format = width + "x150_as";
google_ad_type = "image";
google_ad_channel = "";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="mui-container-fluid button-borders">
<div class="pre-btn">
<a href="/python_digital_forensics/python_digital_forensics_investigation_of_log_based_artifacts.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/python_digital_forensics/python_digital_forensics_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="clearer"></div>
<h1>Python Digital Forensics - Introduction</h1>
<p>This chapter will give you an introduction to what digital forensics is all about, and its historical review. You will also understand where you can apply digital forensics in real life and its limitations.</p>
<h2>What is Digital Forensics?</h2>
<p>Digital forensics may be defined as the branch of forensic science that analyzes, examines, identifies and recovers the digital evidences residing on electronic devices. It is commonly used for criminal law and private investigations.</p>
<p>For example, you can rely on digital forensics extract evidences in case somebody steals some data on an electronic device.</p>
<h2>Brief Historical Review of Digital Forensics</h2>
<p>The history of computer crimes and the historical review of digital forensics is explained in this section as given below &minus;</p>
<h3>1970s-1980s: First Computer Crime</h3>
<p>Prior to this decade, no computer crime has been recognized. However, if it is supposed to happen, the then existing laws dealt with them. Later, in 1978 the first computer crime was recognized in Florida Computer Crime Act, which included legislation against unauthorized modification or deletion of data on a computer system. But over the time, due to the advancement of technology, the range of computer crimes being committed also increased. To deal with crimes related to copyright, privacy and child pornography, various other laws were passed.</p>
<h3>1980s-1990s: Development Decade</h3>
<p>This decade was the development decade for digital forensics, all because of the first ever investigation (1986) in which Cliff Stoll tracked the hacker named Markus Hess. During this period, two kind of digital forensics disciplines developed – first was with the help of ad-hoc tools and techniques developed by practitioners who took it as a hobby, while the second being developed by scientific community. In 1992, the term <b>“Computer Forensics”</b>was used in academic literature.</p>
<h3>2000s-2010s: Decade of Standardization</h3>
<p>After the development of digital forensics to a certain level, there was a need of making some specific standards that can be followed while performing investigations. Accordingly, various scientific agencies and bodies have published guidelines for digital forensics. In 2002, Scientific Working Group on Digital Evidence (SWGDE) published a paper named “Best practices for Computer Forensics”. Another feather in the cap was a European led international treaty namely <b>“The Convention on Cybercrime”</b> was signed by 43 nations and ratified by 16 nations. Even after such standards, still there is a need to resolve some issues which has been identified by researchers.</p>
<h2>Process of Digital Forensics</h2>
<p>Since first ever computer crime in 1978, there is a huge increment in digital criminal activities. Due to this increment, there is a need for structured manner to deal with them. In 1984, a formalized process has been introduced and after that a great number of new and improved computer forensics investigation processes have been developed.</p>
<p>A computer forensics investigation process involves three major phases as explained below &minus;</p>
<h3>Phase 1: Acquisition or Imaging of Exhibits</h3>
<p>The first phase of digital forensics involves saving the state of the digital system so that it can be analyzed later. It is very much similar to taking photographs, blood samples etc. from a crime scene. For example, it involves capturing an image of allocated and unallocated areas of a hard disk or RAM.</p>
<h3>Phase 2: Analysis</h3>
<p>The input of this phase is the data acquired in the acquisition phase. Here, this data was examined to identify evidences. This phase gives three kinds of evidences as follows &minus;</p>
<ul class="list">
<li><p><b>Inculpatory evidences</b> &minus; These evidences support a given history.</p></li>
<li><p><b>Exculpatory evidences</b> &minus; These evidences contradict a given history.</p></li>
<li><p><b>Evidence of tampering</b> &minus; These evidences show that the system was tempered to avoid identification. It includes examining the files and directory content for recovering the deleted files.</p></li>
</ul>
<h3>Phase 3: Presentation or Reporting</h3>
<p>As the name suggests, this phase presents the conclusion and corresponding evidences from the investigation.</p>
<h2>Applications of Digital Forensics</h2>
<p>Digital forensics deals with gathering, analyzing and preserving the evidences that are contained in any digital device. The use of digital forensics depends on the application. As mentioned earlier, it is used mainly in the following two applications &minus;</p>
<h3>Criminal Law</h3>
<p>In criminal law, the evidence is collected to support or oppose a hypothesis in the court. Forensics procedures are very much similar to those used in criminal investigations but with different legal requirements and limitations.</p>
<h3>Private Investigation</h3>
<p>Mainly corporate world uses digital forensics for private investigation. It is used when companies are suspicious that employees may be performing an illegal activity on their computers that is against company policy. Digital forensics provides one of the best routes for company or person to take when investigating someone for digital misconduct.</p>
<h2>Branches of Digital Forensics</h2>
<p>The digital crime is not restricted to computers alone, however hackers and criminals are using small digital devices such as tablets, smart-phones etc. at a very large scale too. Some of the devices have volatile memory, while others have non-volatile memory. Hence depending upon type of devices, digital forensics has the following branches &minus;</p>
<h3>Computer Forensics</h3>
<p>This branch of digital forensics deals with computers, embedded systems and static memories such as USB drives. Wide range of information from logs to actual files on drive can be investigated in computer forensics.</p>
<h3>Mobile Forensics</h3>
<p>This deals with investigation of data from mobile devices. This branch is different from computer forensics in the sense that mobile devices have an inbuilt communication system which is useful for providing useful information related to location.</p>
<h3>Network Forensics</h3>
<p>This deals with the monitoring and analysis of computer network traffic, both local and WAN(wide area network) for the purposes of information gathering, evidence collection, or intrusion detection.</p>
<h3>Database Forensics</h3>
<p>This branch of digital forensics deals with forensics study of databases and their metadata.</p>
<h2>Skills Required for Digital Forensics Investigation</h2>
<p>Digital forensics examiners help to track hackers, recover stolen data, follow computer attacks back to their source, and aid in other types of investigations involving computers. Some of the key skills required to become digital forensics examiner as discussed below &minus;</p>
<h3>Outstanding Thinking Capabilities</h3>
<p>A digital forensics investigator must be an outstanding thinker and should be capable of applying different tools and methodologies on a particular assignment for obtaining the output. He/she must be able to find different patterns and make correlations among them.</p>
<h3>Technical Skills</h3>
<p>A digital forensics examiner must have good technological skills because this field requires the knowledge of network, how digital system interacts.</p>
<h3>Passionate about Cyber Security</h3>
<p>Because the field of digital forensics is all about solving cyber-crimes and this is a tedious task, it needs lot of passion for someone to become an ace digital forensic investigator.</p>
<h3>Communication Skills</h3>
<p>Good communication skills are a must to coordinate with various teams and to extract any missing data or information.</p>
<h3>Skillful in Report Making</h3>
<p>After successful implementation of acquisition and analysis, a digital forensic examiner must mention all the findings the final report and presentation. Hence he/she must have good skills of report making and an attention to detail.</p>
<h2>Limitations</h2>
<p>Digital forensic investigation offers certain limitations as discussed here &minus;</p>
<h3>Need to produce convincing evidences</h3>
<p>One of the major setbacks of digital forensics investigation is that the examiner must have to comply with standards that are required for the evidence in the court of law, as the data can be easily tampered. On the other hand, computer forensic investigator must have complete knowledge of legal requirements, evidence handling and documentation procedures to present convincing evidences in the court of law.</p>
<h3>Investigating Tools</h3>
<p>The effectiveness of digital investigation entirely lies on the expertise of digital forensics examiner and the selection of proper investigation tool. If the tool used is not according to specified standards then in the court of law, the evidences can be denied by the judge.</p>
<h3>Lack of technical knowledge among the audience</h3>
<p>Another limitation is that some individuals are not completely familiar with computer forensics; therefore, many people do not understand this field. Investigators have to be sure to communicate their findings with the courts in such a way to help everyone understand the results.</p>
<h3>Cost</h3>
<p>Producing digital evidences and preserving them is very costly. Hence this process may not be chosen by many people who cannot afford the cost.</p>
<h1>Python Digital Forensics - Getting Started</h1>
<p>In the previous chapter, we learnt the basics of digital forensics, its advantages and limitations. This chapter will make you comfortable with Python, the essential tool that we are using in this digital forensics investigation.</p>
<h2>Why Python for Digital Forensics?</h2>
<p>Python is a popular programming language and is used as tool for cyber security, penetration testing as well as digital forensic investigations. When you choose Python as your tool for digital forensics, you do not need any other third party software for completing the task.</p>
<p>Some of the unique features of Python programming language that makes it a good fit for digital forensics projects are given below &minus;</p>
<ul class="list">
<li><p><b>Simplicity of Syntax</b> &minus; Python’s syntax is simple compared to other languages, that makes it easier for one to learn and put into use for digital forensics.</p></li>
<li><p><b>Comprehensive inbuilt modules</b> &minus; Python’s comprehensive inbuilt modules are an excellent aid for performing a complete digital forensic investigation.</p></li>
<li><p><b>Help and Support</b> &minus; Being an open source programming language, Python enjoys excellent support from the developer’s and users’ community.</p></li>
</ul>
<h2>Features of Python</h2>
<p>Python, being a high-level, interpreted, interactive and object-oriented scripting language, provides the following features &minus;</p>
<ul class="list">
<li><p><b>Easy to Learn</b> &minus; Python is a developer friendly and easy to learn language, because it has fewer keywords and simplest structure.</p></li>
<li><p><b>Expressive and Easy to read</b> &minus; Python language is expressive in nature; hence its code is more understandable and readable.</p></li>
<li><p><b>Cross-platform Compatible</b> &minus; Python is a cross-platform compatible language which means it can run efficiently on various platforms such as UNIX, Windows, and Macintosh.</p></li>
<li><p><b>Interactive Mode Programming</b> &minus; We can do interactive testing and debugging of code because Python supports an interactive mode for programming.</p></li>
<li><p><b>Provides Various Modules and Functions</b> &minus; Python has large standard library which allows us to use rich set of modules and functions for our script.</p></li>
<li><p><b>Supports Dynamic Type Checking</b> &minus; Python supports dynamic type checking and provides very high-level dynamic data types.</p></li>
<li><p><b>GUI Programming</b> &minus; Python supports GUI programming to develop Graphical user interfaces.</p></li>
<li><p><b>Integration with other programming languages</b> &minus; Python can be easily integrated with other programming languages like C, C++, JAVA etc.</p></li>
</ul>
<h2>Installing Python</h2>
<p>Python distribution is available for various platforms such as Windows, UNIX, Linux, and Mac. We only need to download the binary code as per our platform. In case if the binary code for any platform is not available, we must have a C compiler so that source code can be compiled manually.</p>
<p>This section will make you familiar with installation of Python on various platforms&minus;</p>
<h3>Python Installation on Unix and Linux</h3>
<p>You can follow following the steps shown below to install Python on Unix/Linux machine.</p>
<p><b>Step 1</b> &minus; Open a Web browser. Type and enter <a target="_blank" rel="nofollow" href="https://www.python.org/downloads/">www.python.org/downloads/</a></p>
<p><b>Step 2</b> &minus; Download zipped source code available for Unix/Linux.</p>
<p><b>Step 3</b> &minus; Extract the downloaded zipped files.</p>
<p><b>Step 4</b> &minus; If you wish to customize some options, you can edit the <b>Modules/Setup file</b>.</p>
<p><b>Step 5</b> &minus; Use the following commands for completing the installation &minus;</p>
<pre class="result notranslate">
run ./configure script
make
make install
</pre>
<p>Once you have successfully completed the steps given above, Python will be installed at its standard location <b>/usr/local/bin</b> and its libraries at <b>/usr/local/lib/pythonXX</b> where XX is the version of Python.</p>
<h3>Python Installation on Windows</h3>
<p>We can follow following simple steps to install Python on Windows machine.</p>
<p><b>Step 1</b> &minus; Open a web browser. Type and enter <a target="_blank" rel="nofollow" href="https://www.python.org/downloads/">www.python.org/downloads/</a></p>
<p><b>Step 2</b> &minus; Download the Windows installer <b>python-XYZ.msi</b> file, where XYZ is the version we need to install.</p>
<p><b>Step 3</b> &minus; Now run that MSI file after saving the installer file to your local machine.</p>
<p><b>Step 4</b> &minus; Run the downloaded file which will bring up the Python installation wizard.</p>
<h3>Python Installation on Macintosh</h3>
<p>For installing Python 3 on Mac OS X, we must use a package installer named <b>Homebrew</b>.</p>
<p>You can use the following command to install Homebrew, incase you do not have it on your system &minus;</p>
<pre class="result notranslate">
$ ruby -e "$(curl -fsSL
https://raw.githubusercontent.com/Homebrew/install/master/install)"
</pre>
<p>If you need to update the package manager, then it can be done with the help of following command &minus;</p>
<pre class="result notranslate">
$ brew update
</pre>
<p>Now, use the following command to install Python3 on your system &minus;</p>
<pre class="result notranslate">
$ brew install python3
</pre>
<h2>Setting the PATH</h2>
<p>We need to set the path for Python installation and this differs with platforms such as UNIX, WINDOWS, or MAC.</p>
<h3>Path setting at Unix/Linux</h3>
<p>You can use the following options to set the path on Unix/Linux &minus;</p>
<ul class="result notranslate">
<li><p><b>If using csh shell</b> - Type <b>setenv PATH "$PATH:/usr/local/bin/python"</b> and then press Enter.</p></li>
<li><p><b>If using bash shell (Linux)</b> − Type <b>export ATH="$PATH:/usr/local/bin/python"</b> and then press Enter.</p></li>
<li><p><b>If using sh or ksh shell</b> - Type <b>PATH="$PATH:/usr/local/bin/python"</b> and then press Enter.</p></li>
</ul>
<h3>Path Setting at Windows</h3>
<p>Type <b>path %path%;C:\Python</b> at the command prompt and then press Enter.</p>
<h2>Running Python</h2>
<p>You can choose any of the following three methods to start the Python interpreter  &minus;</p>
<h3>Method 1: Using Interactive Interpreter</h3>
<p>A system that provides a command-line interpreter or shell can easily be used for starting Python. For example, Unix, DOS etc. You can follow the steps given below to start coding in interactive interpreter &minus;</p>
<p><b>Step 1</b> &minus; Enter <b>python</b> at the command line.</p>
<p><b>Step 2</b> &minus; Start coding right away in the interactive interpreter using the commands shown below &minus;</p>
<pre class="result notranslate">
$python # Unix/Linux
or
python% # Unix/Linux
or
C:> python # Windows/DOS
</pre>
<h3>Method 2: Using Script from the Command-line</h3>
<p>We can also execute a Python script at command line by invoking the interpreter on our application. You can use commands shown below &minus;</p>
<pre class="result notranslate">
$python script.py # Unix/Linux
or
python% script.py # Unix/Linux
or
C: >python script.py # Windows/DOS
</pre>
<h3>Method 3: Integrated Development Environment</h3>
<p>If a system has GUI application that supports Python, then Python can be run from that GUI environment. Some of the IDE for various platforms are given below &minus;</p>
<ul class="list">
<li><p><b>Unix IDE</b> &minus; UNIX has IDLE IDE for Python.</p></li>
<li><p><b>Windows IDE</b> &minus; Windows has PythonWin, the first Windows interface for Python along with GUI.</p></li>
<li><p><b>Macintosh IDE</b> &minus; Macintosh has IDLE IDE which is available from the main website, downloadable as either MacBinary or BinHex'd files.</p></li>
</ul>
<h1>Artifact Report</h1>
<p>Now that you are comfortable with installation and running Python commands on your local system, let us move into the concepts of forensics in detail. This chapter will explain various concepts involved in dealing with artifacts in Python digital forensics.</p>
<h2>Need of Report Creation</h2>
<p>The process of digital forensics includes reporting as the third phase. This is one of the most important parts of digital forensic process. Report creation is necessary due to the following reasons &minus;</p>
<ul class="list">
<li><p>It is the document in which digital forensic examiner outlines the investigation process and its findings.</p></li>
<li><p>A good digital forensic report can be referenced by another examiner to achieve same result by given same repositories.</p></li>
<li><p>It is a technical and scientific document that contains facts found within the 1s and 0s of digital evidence.</p></li>
</ul>
<h2>General Guidelines for Report Creation</h2>
<p>The reports are written to provide information to the reader and must start with a solid foundation. investigators can face difficulties in efficiently presenting their findings if the report is prepared without some general guidelines or standards. Some general guidelines which must be followed while creating digital forensic reports are given below &minus;</p>
<ul class="list">
<li><p><b>Summary</b> &minus; The report must contain the brief summary of information so that the reader can ascertain the report’s purpose.</p></li>
<li><p><b>Tools used</b> &minus; We must mention the tools which have been used for carrying the process of digital forensics, including their purpose.</p></li>
<li><p><b>Repository</b> &minus; Suppose, we investigated someone’s computer then the summary of evidence and analysis of relevant material like email, internal search history etc., then they must be included in the report so that the case may be clearly presented.</p></li>
<li><p><b>Recommendations for counsel</b> &minus; The report must have the recommendations for counsel to continue or cease investigation based on the findings in report.</p></li>
</ul>
<h2>Creating Different Type of Reports</h2>
<p>In the above section, we came to know about the importance of report in digital forensics along with the guidelines for creating the same. Some of the formats in Python for creating different kind of reports are discussed below &minus;</p>
<h3>CSV Reports</h3>
<p>One of the most common output formats of reports is a CSV spreadsheet report. You can create a CSV to create a report of processed data using the Python code as shown below &minus;</p>
<p>First, import useful libraries for writing the spreadsheet &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import csv
import os
import sys
</pre>
<p>Now, call the following method &minus;</p>
<pre class="result notranslate">
Write_csv(TEST_DATA_LIST, ["Name", "Age", "City", "Job description"], os.getcwd())
</pre>
<p>We are using the following global variable to represent sample data types &minus;</p>
<pre class="prettyprint notranslate">
TEST_DATA_LIST = [["Ram", 32, Bhopal, Manager], 
   ["Raman", 42, Indore, Engg.],
   ["Mohan", 25, Chandigarh, HR], 
   ["Parkash", 45, Delhi, IT]]
</pre>
<p>Next, let us define the method to proceed for further operations. We open the file in the “w” mode and set the newline keyword argument to an empty string.</p>
<pre class="prettyprint notranslate">
def Write_csv(data, header, output_directory, name = None):
   if name is None:
      name = "report1.csv"
   print("[+] Writing {} to {}".format(name, output_directory))
   
   with open(os.path.join(output_directory, name), "w", newline = "") as \ csvfile:
      writer = csv.writer(csvfile)
      writer.writerow(header)
      writer.writerow(data)
</pre>
<p>If you run the above script, you will get the following details stored in report1.csv file.</p>
<table class="table table-bordered" style="text-align:center;">
<tr>
<th style="text-align:center;">Name</th>
<th style="text-align:center;">Age</th>
<th style="text-align:center;">City</th>
<th style="text-align:center;">Designation</th>
</tr>
<tr>
<td>Ram</td>
<td>32</td>
<td>Bhopal</td>
<td>Managerh</td>
</tr>
<tr>
<td>Raman</td>
<td>42</td>
<td>Indore</td>
<td>Engg</td>
</tr>
<tr>
<td>Mohan</td>
<td>25</td>
<td>Chandigarh</td>
<td>HR</td>
</tr>
<tr>
<td>Parkash</td>
<td>45</td>
<td>Delhi</td>
<td>IT</td>
</tr>
</table>
<h3>Excel Reports</h3>
<p>Another common output format of reports is Excel (.xlsx) spreadsheet report. We can create table and also plot the graph by using Excel. We can create report of processed data in Excel format using Python code as shown below&minus;</p>
<p>First, import XlsxWriter module for creating spreadsheet &minus;</p>
<pre class="result notranslate">
import xlsxwriter
</pre>
<p>Now, create a workbook object. For this, we need to use Workbook() constructor.</p>
<pre class="result notranslate">
workbook = xlsxwriter.Workbook('report2.xlsx')
</pre>
<p>Now, create a new worksheet by using add_worksheet() module.</p>
<pre class="result notranslate">
worksheet = workbook.add_worksheet()
</pre>
<p>Next, write the following data into the worksheet &minus;</p>
<pre class="prettyprint notranslate">
report2 = (['Ram', 32, ‘Bhopal’],['Mohan',25, ‘Chandigarh’] ,['Parkash',45, ‘Delhi’])

row = 0
col = 0
</pre>
<p>You can iterate over this data and write it as follows &minus;</p>
<pre class="prettyprint notranslate">
for item, cost in (a):
   worksheet.write(row, col, item)
   worksheet.write(row, col+1, cost)
   row + = 1
</pre>
<p>Now, let us close this Excel file by using close() method.</p>
<pre class="result notranslate">
workbook.close()
</pre>
<p>The above script will create an Excel file named report2.xlsx having the following data &minus;</p>
<table class="table table-bordered" style="width:60%; margin:auto; text-align:center;">
<tr>
<td style="text-align:center;">Ram</td>
<td style="text-align:center;">32</td>
<td style="text-align:center;">Bhopal</td>
</tr>
<tr>
<td>Mohan</td>
<td>25</td>
<td>Chandigarh</td>
</tr>
<tr>
<td>Parkash</td>
<td>45</td>
<td>Delhi</td>
</tr>
</table>
<h2>Investigation Acquisition Media</h2>
<p>It is important for an investigator to have the detailed investigative notes to accurately recall the findings or put together all the pieces of investigation. A screenshot is very useful to keep track of the steps taken for a particular investigation. With the help of the following Python code, we can take the screenshot and save it on hard disk for future use.</p>
<p>First, install Python module named pyscreenshot by using following command &minus;</p>
<pre class="result notranslate">
Pip install pyscreenshot
</pre>
<p>Now, import the necessary modules as shown &minus;</p>
<pre class="result notranslate">
import pyscreenshot as ImageGrab
</pre>
<p>Use the following line of code to get the screenshot &minus;</p>
<pre class="result notranslate">
image = ImageGrab.grab()
</pre>
<p>Use the following line of code to save the screenshot to the given location &minus;</p>
<pre class="result notranslate">
image.save('d:/image123.png')
</pre>
<p>Now, if you want to pop up the screenshot as a graph, you can use the following Python code &minus;</p>
<pre class="prettyprint notranslate">
import numpy as np
import matplotlib.pyplot as plt
import pyscreenshot as ImageGrab
imageg = ImageGrab.grab()
plt.imshow(image, cmap='gray', interpolation='bilinear')
plt.show()
</pre>
<h1>Python Digital Mobile Device Forensics</h1>
<p>This chapter will explain Python digital forensics on mobile devices and the concepts involved.</p>
<h2>Introduction</h2>
<p>Mobile device forensics is that branch of digital forensics which deals with the acquisition and analysis of mobile devices to recover digital evidences of investigative interest. This branch is different from computer forensics because mobile devices have an inbuilt communication system which is useful for providing useful information related to location.</p>
<p>Though the use of smartphones is increasing in digital forensics day-by-day, still it is considered to be non-standard due to its heterogeneity. On the other hand, computer hardware, such as hard disk, is considered to be standard and developed as a stable discipline too. In digital forensic industry, there is a lot of debate on the techniques used for non-standards devices, having transient evidences, such as smartphones.</p>
<h2>Artifacts Extractible from Mobile Devices</h2>
<p>Modern mobile devices possess lot of digital information in comparison with the older phones having only a call log or SMS messages. Thus, mobile devices can supply investigators with lots of insights about its user. Some artifacts that can be extracted from mobile devices are as mentioned below &minus;</p>
<ul class="list">
<li><p><b>Messages</b> &minus; These are the useful artifacts which can reveal the state of mind of the owner and can even give some previous unknown information to the investigator.</p></li>
<li><p><b>Location History</b>&minus; The location history data is a useful artifact which can be used by investigators to validate about the particular location of a person.</p></li>
<li><p><b>Applications Installed</b> &minus; By accessing the kind of applications installed, investigator get some insight into the habits and thinking of the mobile user.</p></li>
</ul>
<h2>Evidence Sources and Processing in Python</h2>
<p>Smartphones have SQLite databases and PLIST files as the major sources of evidences. In this section we are going to process the sources of evidences in python.</p>
<h3>Analyzing PLIST files</h3>
<p>A PLIST (Property List) is a flexible and convenient format for storing application data especially on iPhone devices. It uses the extension <b>.plist</b>. Such kind of files used to store information about bundles and applications. It can be in two formats: <b>XML</b> and <b>binary</b>. The following Python code will open and read PLIST file. Note that before proceeding into this, we must create our own <b>Info.plist</b> file.</p>
<p>First, install a third party library named <b>biplist</b> by the following command &minus;</p>
<pre class="result notranslate">
Pip install biplist
</pre>
<p>Now, import some useful libraries to process plist files &minus;</p>
<pre class="prettyprint notranslate">
import biplist
import os
import sys
</pre>
<p>Now, use the following command under main method can be used to read plist file into a variable &minus;</p>
<pre class="prettyprint notranslate">
def main(plist):
   try:
      data = biplist.readPlist(plist)
   except (biplist.InvalidPlistException,biplist.NotBinaryPlistException) as e:
print("[-] Invalid PLIST file - unable to be opened by biplist")
sys.exit(1)
</pre>
<p>Now, we can either read the data on the console or directly print it, from this variable.</p>
<h3>SQLite Databases</h3>
<p>SQLite serves as the primary data repository on mobile devices. SQLite an in-process library that implements a self-contained, server-less, zero-configuration, transactional SQL database engine. It is a database, which is zero-configured, you need not configure it in your system, unlike other databases.</p>
<p>If you are a novice or unfamiliar with SQLite databases, you can follow the link <a href="/sqlite/index.htm">www.tutorialspoint.com/sqlite/index.htm</a> Additionally, you can follow the link <a href="/sqlite/sqlite_python.htm">www.tutorialspoint.com/sqlite/sqlite_python.htm</a> in case you want to get into detail of SQLite with Python.</p>
<p>During mobile forensics, we can interact with the <b>sms.db</b> file of a mobile device and can extract valuable information from <b>message</b> table. Python has a built in library named <b>sqlite3</b> for connecting with SQLite database. You can import the same with the following command &minus;</p>
<pre class="result notranslate">
import sqlite3
</pre>
<p>Now, with the help of following command, we can connect with the database, say <b>sms.db</b> in case of mobile devices &minus;</p>
<pre class="prettyprint notranslate">
Conn = sqlite3.connect(‘sms.db’)
C = conn.cursor()
</pre>
<p>Here, C is the cursor object with the help of which we can interact with the database.</p>
<p>Now, suppose if we want to execute a particular command, say to get the details from the <b>abc table</b>, it can be done with the help of following command &minus;</p>
<pre class="prettyprint notranslate">
c.execute(“Select * from abc”)
c.close()
</pre>
<p>The result of the above command would be stored in the <b>cursor</b> object. Similarly we can use <b>fetchall()</b> method to dump the result into a variable we can manipulate.</p>
<p>We can use the following command to get column names data of message table in <b>sms.db</b> &minus;</p>
<pre class="prettyprint notranslate">
c.execute(“pragma table_info(message)”)
table_data = c.fetchall()
columns = [x[1] for x in table_data
</pre>
<p>Observe that here we are using SQLite PRAGMA command which is special command to be used to control various environmental variables and state flags within SQLite environment. In the above command, the <b>fetchall()</b> method returns a tuple of results. Each column’s name is stored in the first index of each tuple.</p>
<p>Now, with the help of following command we can query the table for all of its data and store it in the variable named <b>data_msg</b> &minus;</p>
<pre class="prettyprint notranslate">
c.execute(“Select * from message”)
data_msg = c.fetchall()
</pre>
<p>The above command will store the data in the variable and further we can also write the above data in CSV file by using <b>csv.writer()</b> method.</p>
<h2>iTunes Backups</h2>
<p>iPhone mobile forensics can be performed on the backups made by iTunes. Forensic examiners rely on analyzing the iPhone logical backups acquired through iTunes. AFC (Apple file connection) protocol is used by iTunes to take the backup. Besides, the backup process does not modify anything on the iPhone except the escrow key records.</p>
<p>Now, the question arises that why it is important for a digital forensic expert to understand the techniques on iTunes backups? It is important in case we get access to the suspect’s computer instead of iPhone directly because when a computer is used to sync with iPhone, then most of the information on iPhone is likely to be backed up on the computer.</p>
<h3>Process of Backup and its Location</h3>
<p>Whenever an Apple product is backed up to the computer, it is in sync with iTunes and there will be a specific folder with device’s unique ID. In the latest backup format, the files are stored in subfolders containing the first two hexadecimal characters of the file name. From these back up files, there are some files like info.plist which are useful along with the database named Manifest.db. The following table shows the backup locations, that vary with operating systems of iTunes backups &minus;</p>
<table class="table table-bordered">
<tr>
<th style="width:30%; text-align:center;">OS</th>
<th style="text-align:center;">Backup Location</th>
</tr>
<tr>
<td>Win7</td>
<td>C:\Users\[username]\AppData\Roaming\AppleComputer\MobileSync\Backup\</td>
</tr>
<tr>
<td>MAC OS X</td>
<td>~/Library/Application Suport/MobileSync/Backup/</td>
</tr>
</table>
<p>For processing the iTunes backup with Python, we need to first identify all the backups in backup location as per our operating system. Then we will iterate through each backup and read the database Manifest.db.</p>
<p>Now, with the help of following Python code we can do the same &minus;</p>
<p>First, import the necessary libraries as follows &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import argparse
import logging
import os

from shutil import copyfile
import sqlite3
import sys
logger = logging.getLogger(__name__)
</pre>
<p>Now, provide two positional arguments namely INPUT_DIR and OUTPUT_DIR which is representing iTunes backup and desired output folder &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == "__main__":
   parser.add_argument("INPUT_DIR",help = "Location of folder containing iOS backups, ""e.g. ~\Library\Application Support\MobileSync\Backup folder")
   parser.add_argument("OUTPUT_DIR", help = "Output Directory")
   parser.add_argument("-l", help = "Log file path",default = __file__[:-2] + "log")
   parser.add_argument("-v", help = "Increase verbosity",action = "store_true") args = parser.parse_args()
</pre>
<p>Now, setup the log as follows &minus;</p>
<pre class="prettyprint notranslate">
if args.v:
   logger.setLevel(logging.DEBUG)
else:
   logger.setLevel(logging.INFO)
</pre>
<p>Now, setup the message format for this log as follows &minus;</p>
<pre class="prettyprint notranslate">
msg_fmt = logging.Formatter("%(asctime)-15s %(funcName)-13s""%(levelname)-8s %(message)s")
strhndl = logging.StreamHandler(sys.stderr)
strhndl.setFormatter(fmt = msg_fmt)

fhndl = logging.FileHandler(args.l, mode = 'a')
fhndl.setFormatter(fmt = msg_fmt)

logger.addHandler(strhndl)
logger.addHandler(fhndl)
logger.info("Starting iBackup Visualizer")
logger.debug("Supplied arguments: {}".format(" ".join(sys.argv[1:])))
logger.debug("System: " + sys.platform)
logger.debug("Python Version: " + sys.version)
</pre>
<p>The following line of code will create necessary folders for the desired output directory by using <b>os.makedirs()</b> function &minus;</p>
<pre class="prettyprint notranslate">
if not os.path.exists(args.OUTPUT_DIR):
   os.makedirs(args.OUTPUT_DIR)
</pre>
<p>Now, pass the supplied input and output directories to the main() function as follows &minus;</p>
<pre class="prettyprint notranslate">
if os.path.exists(args.INPUT_DIR) and os.path.isdir(args.INPUT_DIR):
   main(args.INPUT_DIR, args.OUTPUT_DIR)
else:
   logger.error("Supplied input directory does not exist or is not ""a directory")
   sys.exit(1)
</pre>
<p>Now, write <b>main()</b> function which will further call <b>backup_summary()</b> function to identify all the backups present in input folder &minus;</p>
<pre class="prettyprint notranslate">
def main(in_dir, out_dir):
   backups = backup_summary(in_dir)
def backup_summary(in_dir):
   logger.info("Identifying all iOS backups in {}".format(in_dir))
   root = os.listdir(in_dir)
   backups = {}
   
   for x in root:
      temp_dir = os.path.join(in_dir, x)
      if os.path.isdir(temp_dir) and len(x) == 40:
         num_files = 0
         size = 0
         
         for root, subdir, files in os.walk(temp_dir):
            num_files += len(files)
            size += sum(os.path.getsize(os.path.join(root, name))
               for name in files)
         backups[x] = [temp_dir, num_files, size]
   return backups
</pre>
<p>Now, print the summary of each backup to the console as follows &minus;</p>
<pre class="prettyprint notranslate">
print("Backup Summary")
print("=" * 20)

if len(backups) > 0:
   for i, b in enumerate(backups):
      print("Backup No.: {} \n""Backup Dev. Name: {} \n""# Files: {} \n""Backup Size (Bytes): {}\n".format(i, b, backups[b][1], backups[b][2]))
</pre>
<p>Now, dump the contents of the Manifest.db file to the variable named db_items.</p>
<pre class="prettyprint notranslate">
try:
   db_items = process_manifest(backups[b][0])
   except IOError:
      logger.warn("Non-iOS 10 backup encountered or " "invalid backup. Continuing to next backup.")
continue
</pre>
<p>Now, let us define a function that will take the directory path of the backup &minus;</p>
<pre class="prettyprint notranslate">
def process_manifest(backup):
   manifest = os.path.join(backup, "Manifest.db")
   
   if not os.path.exists(manifest):
      logger.error("Manifest DB not found in {}".format(manifest))
      raise IOError
</pre>
<p>Now, using SQLite3 we will connect to the database by cursor named c &minus;</p>
<pre class="prettyprint notranslate">
c = conn.cursor()
items = {}

for row in c.execute("SELECT * from Files;"):
   items[row[0]] = [row[2], row[1], row[3]]
return items

create_files(in_dir, out_dir, b, db_items)
   print("=" * 20)
else:
   logger.warning("No valid backups found. The input directory should be
      " "the parent-directory immediately above the SHA-1 hash " "iOS device backups")
      sys.exit(2)
</pre>
<p>Now, define the <b>create_files()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
def create_files(in_dir, out_dir, b, db_items):
   msg = "Copying Files for backup {} to {}".format(b, os.path.join(out_dir, b))
   logger.info(msg)
</pre>
<p>Now, iterate through each key in the <b>db_items</b> dictionary &minus;</p>
<pre class="prettyprint notranslate">
for x, key in enumerate(db_items):
   if db_items[key][0] is None or db_items[key][0] == "":
      continue
   else:
      dirpath = os.path.join(out_dir, b,
os.path.dirname(db_items[key][0]))
   filepath = os.path.join(out_dir, b, db_items[key][0])
   
   if not os.path.exists(dirpath):
      os.makedirs(dirpath)
      original_dir = b + "/" + key[0:2] + "/" + key
   path = os.path.join(in_dir, original_dir)
   
   if os.path.exists(filepath):
      filepath = filepath + "_{}".format(x)
</pre>
<p>Now, use <b>shutil.copyfile()</b> method to copy the backed-up file as follows &minus;</p>
<pre class="prettyprint notranslate">
try:
   copyfile(path, filepath)
   except IOError:
      logger.debug("File not found in backup: {}".format(path))
         files_not_found += 1
   if files_not_found > 0:
      logger.warning("{} files listed in the Manifest.db not" "found in
backup".format(files_not_found))
   copyfile(os.path.join(in_dir, b, "Info.plist"), os.path.join(out_dir, b,
"Info.plist"))
   copyfile(os.path.join(in_dir, b, "Manifest.db"), os.path.join(out_dir, b,
"Manifest.db"))
   copyfile(os.path.join(in_dir, b, "Manifest.plist"), os.path.join(out_dir, b,
"Manifest.plist"))
   copyfile(os.path.join(in_dir, b, "Status.plist"),os.path.join(out_dir, b,
"Status.plist"))
</pre>
<p>With the above Python script, we can get the updated back up file structure in our output folder. We can use <b>pycrypto</b> python library to decrypt the backups.</p>
<h2>Wi - Fi</h2>
<p>Mobile devices can be used to connect to the outside world by connecting through Wi-Fi networks which are available everywhere. Sometimes the device gets connected to these open networks automatically.</p>
<p>In case of iPhone, the list of open Wi-Fi connections with which the device has got connected is stored in a PLIST file named <b>com.apple.wifi.plist</b>. This file will contain the Wi-Fi SSID, BSSID and connection time.</p>
<p>We need to extract Wi-Fi details from standard Cellebrite XML report using Python. For this, we need to use API from Wireless Geographic Logging Engine (WIGLE), a popular platform which can be used for finding the location of a device using the names of Wi-Fi networks.</p>
<p>We can use Python library named <b>requests</b> to access the API from WIGLE. It can be installed as follows &minus;</p>
<pre class="result notranslate">
pip install requests
</pre>
<h3>API from WIGLE</h3>
<p>We need to register on WIGLE’s website <a target="_blank" rel="nofollow" href="https://wigle.net/account">https://wigle.net/account</a> to get a free API from WIGLE. The Python script for getting the information about user device and its connection through WIGEL’s API is discussed below &minus;</p>
<p>First, import the following libraries for handling different things &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function

import argparse
import csv
import os
import sys
import xml.etree.ElementTree as ET
import requests
</pre>
<p>Now, provide two positional arguments namely <b>INPUT_FILE</b> and <b>OUTPUT_CSV</b> which will represent the input file with Wi-Fi MAC address and the desired output CSV file respectively &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == "__main__":
   parser.add_argument("INPUT_FILE", help = "INPUT FILE with MAC Addresses")
   parser.add_argument("OUTPUT_CSV", help = "Output CSV File")
   parser.add_argument("-t", help = "Input type: Cellebrite XML report or TXT
file",choices = ('xml', 'txt'), default = "xml")
   parser.add_argument('--api', help = "Path to API key
   file",default = os.path.expanduser("~/.wigle_api"),
   type = argparse.FileType('r'))
   args = parser.parse_args()
</pre>
<p>Now following lines of code will check if the input file exists and is a file. If not, it exits the script &minus;</p>
<pre class="prettyprint notranslate">
if not os.path.exists(args.INPUT_FILE) or \ not os.path.isfile(args.INPUT_FILE):
   print("[-] {} does not exist or is not a
file".format(args.INPUT_FILE))
   sys.exit(1)
directory = os.path.dirname(args.OUTPUT_CSV)
if directory != '' and not os.path.exists(directory):
   os.makedirs(directory)
api_key = args.api.readline().strip().split(":")
</pre>
<p>Now, pass the argument to main as follows &minus;</p>
<pre class="prettyprint notranslate">
main(args.INPUT_FILE, args.OUTPUT_CSV, args.t, api_key)
def main(in_file, out_csv, type, api_key):
   if type == 'xml':
      wifi = parse_xml(in_file)
   else:
      wifi = parse_txt(in_file)
query_wigle(wifi, out_csv, api_key)
</pre>
<p>Now, we will parse the XML file as follows &minus;</p>
<pre class="prettyprint notranslate">
def parse_xml(xml_file):
   wifi = {}
   xmlns = "{http://pa.cellebrite.com/report/2.0}"
   print("[+] Opening {} report".format(xml_file))
   
   xml_tree = ET.parse(xml_file)
   print("[+] Parsing report for all connected WiFi addresses")
   
   root = xml_tree.getroot()
</pre>
<p>Now, iterate through the child element of the root as follows &minus;</p>
<pre class="prettyprint notranslate">
for child in root.iter():
   if child.tag == xmlns + "model":
      if child.get("type") == "Location":
         for field in child.findall(xmlns + "field"):
            if field.get("name") == "TimeStamp":
               ts_value = field.find(xmlns + "value")
               try:
               ts = ts_value.text
               except AttributeError:
continue
</pre>
<p>Now, we will check that ‘ssid’ string is present in the value’s text or not &minus;</p>
<pre class="prettyprint notranslate">
if "SSID" in value.text:
   bssid, ssid = value.text.split("\t")
   bssid = bssid[7:]
   ssid = ssid[6:]
</pre>
<p>Now, we need to add BSSID, SSID and timestamp to the wifi dictionary as follows &minus;</p>
<pre class="prettyprint notranslate">
if bssid in wifi.keys():

wifi[bssid]["Timestamps"].append(ts)
   wifi[bssid]["SSID"].append(ssid)
else:
   wifi[bssid] = {"Timestamps": [ts], "SSID":
[ssid],"Wigle": {}}
return wifi
</pre>
<p>The text parser which is much simpler that XML parser is shown below &minus;</p>
<pre class="prettyprint notranslate">
def parse_txt(txt_file):
   wifi = {}
   print("[+] Extracting MAC addresses from {}".format(txt_file))
   
   with open(txt_file) as mac_file:
      for line in mac_file:
         wifi[line.strip()] = {"Timestamps": ["N/A"], "SSID":
["N/A"],"Wigle": {}}
return wifi
</pre>
<p>Now, let us use requests module to make <b>WIGLE API</b>calls and need to move on to the <b>query_wigle()</b> method &minus;</p>
<pre class="prettyprint notranslate">
def query_wigle(wifi_dictionary, out_csv, api_key):
   print("[+] Querying Wigle.net through Python API for {} "
"APs".format(len(wifi_dictionary)))
   for mac in wifi_dictionary:

   wigle_results = query_mac_addr(mac, api_key)
def query_mac_addr(mac_addr, api_key):

   query_url = "https://api.wigle.net/api/v2/network/search?" \
"onlymine = false&amp;freenet = false&amp;paynet = false" \ "&amp;netid = {}".format(mac_addr)
   req = requests.get(query_url, auth = (api_key[0], api_key[1]))
   return req.json()
</pre>
<p>Actually there is a limit per day for WIGLE API calls, if that limit exceeds then it must show an error as follows &minus;</p>
<pre class="prettyprint notranslate">
try:
   if wigle_results["resultCount"] == 0:
      wifi_dictionary[mac]["Wigle"]["results"] = []
         continue
   else:
      wifi_dictionary[mac]["Wigle"] = wigle_results
except KeyError:
   if wigle_results["error"] == "too many queries today":
      print("[-] Wigle daily query limit exceeded")
      wifi_dictionary[mac]["Wigle"]["results"] = []
      continue
   else:
      print("[-] Other error encountered for " "address {}: {}".format(mac,
wigle_results['error']))
   wifi_dictionary[mac]["Wigle"]["results"] = []
   continue
prep_output(out_csv, wifi_dictionary)
</pre>
<p>Now, we will use <b>prep_output()</b> method to flattens the dictionary into easily writable chunks &minus;</p>
<pre class="prettyprint notranslate">
def prep_output(output, data):
   csv_data = {}
   google_map = https://www.google.com/maps/search/
</pre>
<p>Now, access all the data we have collected so far as follows &minus;</p>
<pre class="prettyprint notranslate">
for x, mac in enumerate(data):
   for y, ts in enumerate(data[mac]["Timestamps"]):
      for z, result in enumerate(data[mac]["Wigle"]["results"]):
         shortres = data[mac]["Wigle"]["results"][z]
         g_map_url = "{}{},{}".format(google_map, shortres["trilat"],shortres["trilong"])
</pre>
<p>Now, we can write the output in CSV file as we have done in earlier scripts in this chapter by using <b>write_csv()</b> function.</p>
<h1>Investigating Embedded Metadata</h1>
<p>In this chapter, we will learn in detail about investigating embedded metadata using Python digital forensics.</p>
<h2>Introduction</h2>
<p>Embedded metadata is the information about data stored in the same file which is having the object described by that data. In other words, it is the information about a digital asset stored in the digital file itself. It is always associated with the file and can never be separated.</p>
<p>In case of digital forensics, we cannot extract all the information about a particular file. On the other side, embedded metadata can provide us information critical to the investigation. For example, a text file’s metadata may contain information about the author, its length, written date and even a short summary about that document. A digital image may include the metadata such as the length of the image, the shutter speed etc.</p>
<h2>Artifacts Containing Metadata Attributes and their Extraction</h2>
<p>In this section, we will learn about various artifacts containing metadata attributes and their extraction process using Python.</p>
<h3>Audio and Video</h3>
<p>These are the two very common artifacts which have the embedded metadata. This metadata can be extracted for the purpose of investigation.</p>
<p>You can use the following Python script to extract common attributes or metadata from audio or MP3 file and a video or a MP4 file.</p>
<p>Note that for this script, we need to install a third party python library named mutagen which allows us to extract metadata from audio and video files. It can be installed with the help of the following command &minus;</p>
<pre class="result notranslate">
pip install mutagen
</pre>
<p>Some of the useful libraries we need to import for this Python script are as follows &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function

import argparse
import json
import mutagen
</pre>
<p>The command line handler will take one argument which represents the path to the MP3 or MP4 files. Then, we will use <b>mutagen.file()</b> method to open a handle to the file as follows &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('Python Metadata Extractor')
   parser.add_argument("AV_FILE", help="File to extract metadata from")
   args = parser.parse_args()
   av_file = mutagen.File(args.AV_FILE)
   file_ext = args.AV_FILE.rsplit('.', 1)[-1]
   
   if file_ext.lower() == 'mp3':
      handle_id3(av_file)
   elif file_ext.lower() == 'mp4':
      handle_mp4(av_file)
</pre>
<p>Now, we need to use two handles, one to extract the data from MP3 and one to extract data from MP4 file. We can define these handles as follows &minus;</p>
<pre class="prettyprint notranslate">
def handle_id3(id3_file):
   id3_frames = {'TIT2': 'Title', 'TPE1': 'Artist', 'TALB': 'Album','TXXX':
      'Custom', 'TCON': 'Content Type', 'TDRL': 'Date released','COMM': 'Comments',
         'TDRC': 'Recording Date'}
   print("{:15} | {:15} | {:38} | {}".format("Frame", "Description","Text","Value"))
   print("-" * 85)
   
   for frames in id3_file.tags.values():
      frame_name = id3_frames.get(frames.FrameID, frames.FrameID)
      desc = getattr(frames, 'desc', "N/A")
      text = getattr(frames, 'text', ["N/A"])[0]
      value = getattr(frames, 'value', "N/A")
      
      if "date" in frame_name.lower():
         text = str(text)
      print("{:15} | {:15} | {:38} | {}".format(
         frame_name, desc, text, value))
def handle_mp4(mp4_file):
   cp_sym = u"\u00A9"
   qt_tag = {
      cp_sym + 'nam': 'Title', cp_sym + 'art': 'Artist',
      cp_sym + 'alb': 'Album', cp_sym + 'gen': 'Genre',
      'cpil': 'Compilation', cp_sym + 'day': 'Creation Date',
      'cnID': 'Apple Store Content ID', 'atID': 'Album Title ID',
      'plID': 'Playlist ID', 'geID': 'Genre ID', 'pcst': 'Podcast',
      'purl': 'Podcast URL', 'egid': 'Episode Global ID',
      'cmID': 'Camera ID', 'sfID': 'Apple Store Country',
      'desc': 'Description', 'ldes': 'Long Description'}
genre_ids = json.load(open('apple_genres.json'))
</pre>
<p>Now, we need to iterate through this MP4 file as follows &minus;</p>
<pre class="prettyprint notranslate">
print("{:22} | {}".format('Name', 'Value'))
print("-" * 40)

for name, value in mp4_file.tags.items():
   tag_name = qt_tag.get(name, name)
   
   if isinstance(value, list):
      value = "; ".join([str(x) for x in value])
   if name == 'geID':
      value = "{}: {}".format(
      value, genre_ids[str(value)].replace("|", " - "))
   print("{:22} | {}".format(tag_name, value))
</pre>
<p>The above script will give us additional information about MP3 as well as MP4 files.</p>
<h3>Images</h3>
<p>Images may contain different kind of metadata depending upon its file format. However, most of the images embed GPS information. We can extract this GPS information by using third party Python libraries. You can use the following Python script can be used to do the same &minus;</p>
<p>First, download third party python library named <b>Python Imaging Library (PIL)</b> as follows &minus;</p>
<pre class="result notranslate">
pip install pillow
</pre>
<p>This will help us to extract metadata from images.</p>
<p>We can also write the GPS details embedded in images to KML file, but for this we need to download third party Python library named <b>simplekml</b> as follows &minus;</p>
<pre class="result notranslate">
pip install simplekml
</pre>
<p>In this script, first we need to import the following libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import argparse

from PIL import Image
from PIL.ExifTags import TAGS

import simplekml
import sys
</pre>
<p>Now, the command line handler will accept one positional argument which basically represents the file path of the photos.</p>
<pre class="prettyprint notranslate">
parser = argparse.ArgumentParser('Metadata from images')
parser.add_argument('PICTURE_FILE', help = "Path to picture")
args = parser.parse_args()
</pre>
<p>Now, we need to specify the URLs that will populate the coordinate information. The URLs are <b>gmaps</b> and <b>open_maps</b>. We also need a function to convert the degree minute seconds (DMS) tuple coordinate, provided by PIL library, into decimal. It can be done as follows &minus;</p>
<pre class="prettyprint notranslate">
gmaps = "https://www.google.com/maps?q={},{}"
open_maps = "http://www.openstreetmap.org/?mlat={}&amp;mlon={}"

def process_coords(coord):
   coord_deg = 0
   
   for count, values in enumerate(coord):
      coord_deg += (float(values[0]) / values[1]) / 60**count
   return coord_deg
</pre>
<p>Now, we will use <b>image.open()</b> function to open the file as PIL object.</p>
<pre class="prettyprint notranslate">
img_file = Image.open(args.PICTURE_FILE)
exif_data = img_file._getexif()

if exif_data is None:
   print("No EXIF data found")
   sys.exit()
for name, value in exif_data.items():
   gps_tag = TAGS.get(name, name)
   if gps_tag is not 'GPSInfo':
      continue
</pre>
<p>After finding the <b>GPSInfo</b> tag, we will store the GPS reference and process the coordinates with the <b>process_coords()</b> method.</p>
<pre class="prettyprint notranslate">
lat_ref = value[1] == u'N'
lat = process_coords(value[2])

if not lat_ref:
   lat = lat * -1
lon_ref = value[3] == u'E'
lon = process_coords(value[4])

if not lon_ref:
   lon = lon * -1
</pre>
<p>Now, initiate <b>kml</b> object from <b>simplekml</b> library as follows &minus;</p>
<pre class="prettyprint notranslate">
kml = simplekml.Kml()
kml.newpoint(name = args.PICTURE_FILE, coords = [(lon, lat)])
kml.save(args.PICTURE_FILE + ".kml")
</pre>
<p>We can now print the coordinates from processed information as follows &minus;</p>
<pre class="prettyprint notranslate">
print("GPS Coordinates: {}, {}".format(lat, lon))
print("Google Maps URL: {}".format(gmaps.format(lat, lon)))
print("OpenStreetMap URL: {}".format(open_maps.format(lat, lon)))
print("KML File {} created".format(args.PICTURE_FILE + ".kml"))
</pre>
<h3>PDF Documents</h3>
<p>PDF documents have a wide variety of media including images, text, forms etc. When we extract embedded metadata in PDF documents, we may get the resultant data in the format called Extensible Metadata Platform (XMP). We can extract metadata with the help of the following Python code &minus;</p>
<p>First, install a third party Python library named <b>PyPDF2</b> to read metadata stored in XMP format. It can be installed as follows &minus;</p>
<pre class="result notranslate">
pip install PyPDF2
</pre>
<p>Now, import the following libraries for extracting the metadata from PDF files &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser, FileType

import datetime
from PyPDF2 import PdfFileReader
import sys
</pre>
<p>Now, the command line handler will accept one positional argument which basically represents the file path of the PDF file.</p>
<pre class="prettyprint notranslate">
parser = argparse.ArgumentParser('Metadata from PDF')
parser.add_argument('PDF_FILE', help='Path to PDF file',type=FileType('rb'))
args = parser.parse_args()
</pre>
<p>Now we can use <b>getXmpMetadata()</b> method to provide an object containing the available metadata as follows &minus;</p>
<pre class="prettyprint notranslate">
pdf_file = PdfFileReader(args.PDF_FILE)
xmpm = pdf_file.getXmpMetadata()

if xmpm is None:
   print("No XMP metadata found in document.")
   sys.exit()
</pre>
<p>We can use <b>custom_print()</b> method to extract and print the relevant values like title, creator, contributor etc. as follows &minus;</p>
<pre class="prettyprint notranslate">
custom_print("Title: {}", xmpm.dc_title)
custom_print("Creator(s): {}", xmpm.dc_creator)
custom_print("Contributors: {}", xmpm.dc_contributor)
custom_print("Subject: {}", xmpm.dc_subject)
custom_print("Description: {}", xmpm.dc_description)
custom_print("Created: {}", xmpm.xmp_createDate)
custom_print("Modified: {}", xmpm.xmp_modifyDate)
custom_print("Event Dates: {}", xmpm.dc_date)
</pre>
<p>We can also define <b>custom_print()</b> method in case if PDF is created using multiple software as follows &minus;</p>
<pre class="prettyprint notranslate">
def custom_print(fmt_str, value):
   if isinstance(value, list):
      print(fmt_str.format(", ".join(value)))
   elif isinstance(value, dict):
      fmt_value = [":".join((k, v)) for k, v in value.items()]
      print(fmt_str.format(", ".join(value)))
   elif isinstance(value, str) or isinstance(value, bool):
      print(fmt_str.format(value))
   elif isinstance(value, bytes):
      print(fmt_str.format(value.decode()))
   elif isinstance(value, datetime.datetime):
      print(fmt_str.format(value.isoformat()))
   elif value is None:
      print(fmt_str.format("N/A"))
   else:
      print("warn: unhandled type {} found".format(type(value)))
</pre>
<p>We can also extract any other custom property saved by the software as follows &minus;</p>
<pre class="prettyprint notranslate">
if xmpm.custom_properties:
   print("Custom Properties:")
   
   for k, v in xmpm.custom_properties.items():
      print("\t{}: {}".format(k, v))
</pre>
<p>The above script will read the PDF document and will print the metadata stored in XMP format including some custom properties stored by the software with the help of which that PDF has been made.</p>
<h3>Windows Executables Files</h3>
<p>Sometimes we may encounter a suspicious or unauthorized executable file. But for the purpose of investigation it may be useful because of the embedded metadata. We can get the information such as its location, its purpose and other attributes such as the manufacturer, compilation date etc. With the help of following Python script we can get the compilation date, useful data from headers and imported as well as exported symbols.</p>
<p>For this purpose, first install the third party Python library <b>pefile</b>. It can be done as follows &minus;</p>
<pre class="result notranslate">
pip install pefile
</pre>
<p>Once you successfully install this, import the following libraries as follows &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function

import argparse
from datetime import datetime
from pefile import PE
</pre>
<p>Now, the command line handler will accept one positional argument which basically represents the file path of the executable file. You can also choose the style of output, whether you need it in detailed and verbose way or in a simplified manner. For this you need to give an optional argument as shown below &minus;</p>
<pre class="prettyprint notranslate">
parser = argparse.ArgumentParser('Metadata from executable file')
parser.add_argument("EXE_FILE", help = "Path to exe file")
parser.add_argument("-v", "--verbose", help = "Increase verbosity of output",
action = 'store_true', default = False)
args = parser.parse_args()
</pre>
<p>Now, we will load the input executable file by using PE class. We will also dump the executable data to a dictionary object by using <b>dump_dict()</b> method.</p>
<pre class="prettyprint notranslate">
pe = PE(args.EXE_FILE)
ped = pe.dump_dict()
</pre>
<p>We can extract basic file metadata such as embedded authorship, version and compilation time using the code shown below &minus;</p>
<pre class="prettyprint notranslate">
file_info = {}
for structure in pe.FileInfo:
   if structure.Key == b'StringFileInfo':
      for s_table in structure.StringTable:
         for key, value in s_table.entries.items():
            if value is None or len(value) == 0:
               value = "Unknown"
            file_info[key] = value
print("File Information: ")
print("==================")

for k, v in file_info.items():
   if isinstance(k, bytes):
      k = k.decode()
   if isinstance(v, bytes):
      v = v.decode()
   print("{}: {}".format(k, v))
comp_time = ped['FILE_HEADER']['TimeDateStamp']['Value']
comp_time = comp_time.split("[")[-1].strip("]")
time_stamp, timezone = comp_time.rsplit(" ", 1)
comp_time = datetime.strptime(time_stamp, "%a %b %d %H:%M:%S %Y")
print("Compiled on {} {}".format(comp_time, timezone.strip()))
</pre>
<p>We can extract the useful data from headers as follows &minus;</p>
<pre class="prettyprint notranslate">
for section in ped['PE Sections']:
   print("Section '{}' at {}: {}/{} {}".format(
      section['Name']['Value'], hex(section['VirtualAddress']['Value']),
      section['Misc_VirtualSize']['Value'],
      section['SizeOfRawData']['Value'], section['MD5'])
   )
</pre>
<p>Now, extract the listing of imports and exports from executable files as shown below &minus;</p>
<pre class="prettyprint notranslate">
if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
   print("\nImports: ")
   print("=========")
   
   for dir_entry in pe.DIRECTORY_ENTRY_IMPORT:
      dll = dir_entry.dll
      
      if not args.verbose:
         print(dll.decode(), end=", ")
         continue
      name_list = []
      
      for impts in dir_entry.imports:
         if getattr(impts, "name", b"Unknown") is None:
            name = b"Unknown"
         else:
            name = getattr(impts, "name", b"Unknown")
			name_list.append([name.decode(), hex(impts.address)])
      name_fmt = ["{} ({})".format(x[0], x[1]) for x in name_list]
      print('- {}: {}'.format(dll.decode(), ", ".join(name_fmt)))
   if not args.verbose:
      print()
</pre>
<p>Now, print <b>exports</b>, <b>names</b> and <b>addresses</b> using the code as shown below &minus;</p>
<pre class="prettyprint notranslate">
if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
   print("\nExports: ")
   print("=========")
   
   for sym in pe.DIRECTORY_ENTRY_EXPORT.symbols:
      print('- {}: {}'.format(sym.name.decode(), hex(sym.address)))
</pre>
<p>The above script will extract the basic metadata, information from headers from windows executable files.</p>
<h3>Office Document Metadata</h3>
<p>Most of the work in computer is done in three applications of MS Office – Word, PowerPoint and Excel. These files possess huge metadata, which can expose interesting information about their authorship and history.</p>
<p>Note that metadata from 2007 format of word (.docx), excel (.xlsx) and powerpoint (.pptx) is stored in a XML file. We can process these XML files in Python with the help of following Python script shown below &minus;</p>
<p>First, import the required libraries as shown below &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser
from datetime import datetime as dt
from xml.etree import ElementTree as etree

import zipfile
parser = argparse.ArgumentParser('Office Document Metadata’)
parser.add_argument("Office_File", help="Path to office file to read")
args = parser.parse_args()
</pre>
<p>Now, check if the file is a ZIP file. Else, raise an error. Now, open the file and extract the key elements for processing using the following code &minus;</p>
<pre class="prettyprint notranslate">
zipfile.is_zipfile(args.Office_File)
zfile = zipfile.ZipFile(args.Office_File)
core_xml = etree.fromstring(zfile.read('docProps/core.xml'))
app_xml = etree.fromstring(zfile.read('docProps/app.xml'))
</pre>
<p>Now, create a dictionary for initiating the extraction of the metadata &minus;</p>
<pre class="prettyprint notranslate">
core_mapping = {
   'title': 'Title',
   'subject': 'Subject',
   'creator': 'Author(s)',
   'keywords': 'Keywords',
   'description': 'Description',
   'lastModifiedBy': 'Last Modified By',
   'modified': 'Modified Date',
   'created': 'Created Date',
   'category': 'Category',
   'contentStatus': 'Status',
   'revision': 'Revision'
}
</pre>
<p>Use <b>iterchildren()</b> method to access each of the tags within the XML file &minus;</p>
<pre class="prettyprint notranslate">
for element in core_xml.getchildren():
   for key, title in core_mapping.items():
      if key in element.tag:
         if 'date' in title.lower():
            text = dt.strptime(element.text, "%Y-%m-%dT%H:%M:%SZ")
         else:
            text = element.text
         print("{}: {}".format(title, text))
</pre>
<p>Similarly, do this for app.xml file which contains statistical information about the contents of the document &minus;</p>
<pre class="prettyprint notranslate">
app_mapping = {
   'TotalTime': 'Edit Time (minutes)',
   'Pages': 'Page Count',
   'Words': 'Word Count',
   'Characters': 'Character Count',
   'Lines': 'Line Count',
   'Paragraphs': 'Paragraph Count',
   'Company': 'Company',
   'HyperlinkBase': 'Hyperlink Base',
   'Slides': 'Slide count',
   'Notes': 'Note Count',
   'HiddenSlides': 'Hidden Slide Count',
}
for element in app_xml.getchildren():
   for key, title in app_mapping.items():
      if key in element.tag:
         if 'date' in title.lower():
            text = dt.strptime(element.text, "%Y-%m-%dT%H:%M:%SZ")
         else:
            text = element.text
         print("{}: {}".format(title, text))
</pre>
<p>Now after running the above script, we can get the different details about the particular document. Note that we can apply this script on Office 2007 or later version documents only.</p>
<h1>Python Digital Network Forensics-I</h1>
<p>This chapter will explain the fundamentals involved in performing network forensics using Python.</p>
<h2>Understanding Network Forensics</h2>
<p>Network forensics is a branch of digital forensics that deals with the monitoring and analysis of computer network traffic, both local and WAN(wide area network), for the purposes of information gathering, evidence collection, or intrusion detection. Network forensics play a critical role in investigating digital crimes such as theft of intellectual property or leakage of information. A picture of network communications helps an investigator to solve some crucial questions as follows &minus;</p>
<ul class="list">
<li><p>What websites has been accessed?</p></li>
<li><p>What kind of content has been uploaded on our network?</p></li>
<li><p>What kind of content has been downloaded from our network?</p></li>
<li><p>What servers are being accessed?</p></li>
<li><p>Is somebody sending sensitive information outside of company firewalls?</p></li>
</ul>
<h2>Internet Evidence Finder (IEF)</h2>
<p>IEF is a digital forensic tool to find, analyze and present digital evidence found on different digital media like computer, smartphones, tablets etc. It is very popular and used by thousands of forensics professionals.</p>
<h2>Use of IEF</h2>
<p>Due to its popularity, IEF is used by forensics professionals to a great extent. Some of the uses of IEF are as follows &minus;</p>
<ul class="list">
<li><p>Due to its powerful search capabilities, it is used to search multiple files or data media simultaneously.</p></li>
<li><p>It is also used to recover deleted data from the unallocated space of RAM through new carving techniques.</p></li>
<li><p>If investigators want to rebuild web pages in their original format on the date they were opened, then they can use IEF.</p></li>
<li><p>It is also used to search logical or physical disk volumes.</p></li>
</ul>
<h2>Dumping Reports from IEF to CSV using Python</h2>
<p>IEF stores data in a SQLite database and following Python script will dynamically identify result tables within the IEF database and dump them to respective CSV files.</p>
<p>This process is done in the steps shown below</p>
<ul class="list">
<li><p>First, generate IEF result database which will be a SQLite database file ending with .db extension.</p></li>
<li><p>Then, query that database to identify all the tables.</p></li>
<li><p>Lastly, write this result tables to an individual CSV file.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>For Python script, import the necessary libraries as follows &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function

import argparse
import csv
import os
import sqlite3
import sys
</pre>
<p>Now, we need to provide the path to IEF database file &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('IEF to CSV')
   parser.add_argument("IEF_DATABASE", help="Input IEF database")
   parser.add_argument("OUTPUT_DIR", help="Output DIR")
   args = parser.parse_args()
</pre>
<p>Now, we will confirm the existence of IEF database as follows &minus;</p>
<pre class="prettyprint notranslate">
if not os.path.exists(args.OUTPUT_DIR):
   os.makedirs(args.OUTPUT_DIR)
if os.path.exists(args.IEF_DATABASE) and \ os.path.isfile(args.IEF_DATABASE):
   main(args.IEF_DATABASE, args.OUTPUT_DIR)
else:
   print("[-] Supplied input file {} does not exist or is not a " "file".format(args.IEF_DATABASE))
   sys.exit(1)
</pre>
<p>Now, as we did in earlier scripts, make the connection with SQLite database as follows to execute the queries through cursor &minus;</p>
<pre class="prettyprint notranslate">
def main(database, out_directory):
   print("[+] Connecting to SQLite database")
   conn = sqlite3.connect(database)
   c = conn.cursor()
</pre>
<p>The following lines of code will fetch the names of the tables from the database &minus;</p>
<pre class="prettyprint notranslate">
print("List of all tables to extract")
c.execute("select * from sqlite_master where type = 'table'")
tables = [x[2] for x in c.fetchall() if not x[2].startswith('_') and not x[2].endswith('_DATA')]
</pre>
<p>Now, we will select all the data from the table and by using <b>fetchall()</b> method on the cursor object we will store the list of tuples containing the table’s data in its entirety in a variable &minus;</p>
<pre class="prettyprint notranslate">
print("Dumping {} tables to CSV files in {}".format(len(tables), out_directory))

for table in tables:
c.execute("pragma table_info('{}')".format(table))
table_columns = [x[1] for x in c.fetchall()]

c.execute("select * from '{}'".format(table))
table_data = c.fetchall()
</pre>
<p>Now, by using <b>CSV_Writer()</b> method we will write the content in CSV file &minus;</p>
<pre class="prettyprint notranslate">
csv_name = table + '.csv'
csv_path = os.path.join(out_directory, csv_name)
print('[+] Writing {} table to {} CSV file'.format(table,csv_name))

with open(csv_path, "w", newline = "") as csvfile:
   csv_writer = csv.writer(csvfile)
   csv_writer.writerow(table_columns)
   csv_writer.writerows(table_data)
</pre>
<p>The above script will fetch all the data from tables of IEF database and write the contents to the CSV file of our choice.</p>
<h3>Working with Cached Data</h3>
<p>From IEF result database, we can fetch more information that is not necessarily supported by IEF itself. We can fetch the cached data, a bi product for information, from email service provider like Yahoo, Google etc. by using IEF result database.</p>
<p>The following is the Python script for accessing the cached data information from Yahoo mail, accessed on Google Chrome, by using IEF database. Note that the steps would be more or less same as followed in the last Python script.</p>
<p>First, import the necessary libraries for Python as follows &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import argparse
import csv
import os
import sqlite3
import sys
import json
</pre>
<p>Now, provide the path to IEF database file along with two positional arguments accepts by command-line handler as done in the last script &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('IEF to CSV')
   parser.add_argument("IEF_DATABASE", help="Input IEF database")
   parser.add_argument("OUTPUT_DIR", help="Output DIR")
   args = parser.parse_args()
</pre>
<p>Now, confirm the existence of IEF database as follows &minus;</p>
<pre class="prettyprint notranslate">
directory = os.path.dirname(args.OUTPUT_CSV)

if not os.path.exists(directory):os.makedirs(directory)
if os.path.exists(args.IEF_DATABASE) and \ os.path.isfile(args.IEF_DATABASE):
   main(args.IEF_DATABASE, args.OUTPUT_CSV)
   else: print("Supplied input file {} does not exist or is not a " "file".format(args.IEF_DATABASE))
sys.exit(1)
</pre>
<p>Now, make the connection with SQLite database as follows to execute the queries through cursor &minus;</p>
<pre class="prettyprint notranslate">
def main(database, out_csv):
   print("[+] Connecting to SQLite database")
   conn = sqlite3.connect(database)
   c = conn.cursor()
</pre>
<p>You can use the following lines of code to fetch the instances of Yahoo Mail contact cache record &minus;</p>
<pre class="prettyprint notranslate">
print("Querying IEF database for Yahoo Contact Fragments from " "the Chrome Cache Records Table")
   try:
      c.execute("select * from 'Chrome Cache Records' where URL like " "'https://data.mail.yahoo.com" "/classicab/v2/contacts/?format=json%'")
   except sqlite3.OperationalError:
      print("Received an error querying the database --    database may be" "corrupt or not have a Chrome Cache Records table")
      sys.exit(2)
</pre>
<p>Now, the list of tuples returned from above query to be saved into a variable as follows &minus;</p>
<pre class="prettyprint notranslate">
contact_cache = c.fetchall()
contact_data = process_contacts(contact_cache)
write_csv(contact_data, out_csv)
</pre>
<p>Note that here we will use two methods namely <b>process_contacts()</b> for setting up the result list as well as iterating through each contact cache record and <b>json.loads()</b> to store the JSON data extracted from the table into a variable for further manipulation &minus;</p>
<pre class="prettyprint notranslate">
def process_contacts(contact_cache):
   print("[+] Processing {} cache files matching Yahoo contact cache " " data".format(len(contact_cache)))
   results = []
   
   for contact in contact_cache:
      url = contact[0]
      first_visit = contact[1]
      last_visit = contact[2]
      last_sync = contact[3]
      loc = contact[8]
	   contact_json = json.loads(contact[7].decode())
      total_contacts = contact_json["total"]
      total_count = contact_json["count"]
      
      if "contacts" not in contact_json:
         continue
      for c in contact_json["contacts"]:
         name, anni, bday, emails, phones, links = ("", "", "", "", "", "")
            if "name" in c:
            name = c["name"]["givenName"] + " " + \ c["name"]["middleName"] + " " + c["name"]["familyName"]
            
            if "anniversary" in c:
            anni = c["anniversary"]["month"] + \"/" + c["anniversary"]["day"] + "/" + \c["anniversary"]["year"]
            
            if "birthday" in c:
            bday = c["birthday"]["month"] + "/" + \c["birthday"]["day"] + "/" + c["birthday"]["year"]
            
            if "emails" in c:
               emails = ', '.join([x["ep"] for x in c["emails"]])
            
            if "phones" in c:
               phones = ', '.join([x["ep"] for x in c["phones"]])
            
            if "links" in c:
              links = ', '.join([x["ep"] for x in c["links"]])
</pre>
<p>Now for company, title and notes, the get method is used as shown below &minus;</p>
<pre class="prettyprint notranslate">
company = c.get("company", "")
title = c.get("jobTitle", "")
notes = c.get("notes", "")
</pre>
<p>Now, let us append the list of metadata and extracted data elements to the result list as follows &minus;</p>
<pre class="prettyprint notranslate">
results.append([url, first_visit, last_visit, last_sync, loc, name, bday,anni, emails, phones, links, company, title, notes,total_contacts, total_count])
return results   
</pre>
<p>Now, by using <b>CSV_Writer()</b> method, we will write the content in CSV file &minus;</p>
<pre class="prettyprint notranslate">
def write_csv(data, output):
   print("[+] Writing {} contacts to {}".format(len(data), output))
   with open(output, "w", newline="") as csvfile:
      csv_writer = csv.writer(csvfile)
      csv_writer.writerow([
         "URL", "First Visit (UTC)", "Last Visit (UTC)",
         "Last Sync (UTC)", "Location", "Contact Name", "Bday",
         "Anniversary", "Emails", "Phones", "Links", "Company", "Title",
         "Notes", "Total Contacts", "Count of Contacts in Cache"])
      csv_writer.writerows(data)  
</pre>
<p>With the help of above script, we can process the cached data from Yahoo mail by using IEF database.</p>
<h1>Python Digital Network Forensics-II</h1>
<p>The previous chapter dealt with some of the concepts of network forensics using Python. In this chapter, let us understand network forensics using Python at a deeper level.</p>
<h2>Web Page Preservation with Beautiful Soup</h2>
<p>The World Wide Web (WWW) is a unique resource of information. However, its legacy is at high risk due to the loss of content at an alarming rate. A number of cultural heritage and academic institutions, non-profit organizations and private businesses have explored the issues involved and contributed to the development of technical solutions for web archiving.</p>
<p>Web page preservation or web archiving is the process of gathering the data from World Wide Web, ensuring that the data is preserved in an archive and making it available for future researchers, historians and the public. Before proceeding further into the web page preservation, let us discuss some important issues related to web page preservation as given below &minus;</p>
<ul class="list">
<li><p><b>Change in Web Resources</b> &minus; Web resources keep changing everyday which is a challenge for web page preservation.</p></li>
<li><p><b>Large Quantity of Resources</b> &minus; Another issue related to web page preservation is the large quantity of resources which is to be preserved.</p></li>
<li><p><b>Integrity</b> &minus; Web pages must be protected from unauthorized amendments, deletion or removal to protect its integrity.</p></li>
<li><p><b>Dealing with multimedia data</b> &minus; While preserving web pages we need to deal with multimedia data also, and these might cause issues while doing so.</p></li>
<li><p><b>Providing access</b> &minus; Besides preserving, the issue of providing access to web resources and dealing with issues of ownership needs to be solved too.</p></li>
</ul>
<p>In this chapter, we are going to use Python library named <b>Beautiful Soup</b> for web page preservation.</p>
<h2>What is Beautiful Soup?</h2>
<p>Beautiful Soup is a Python library for pulling data out of HTML and XML files. It can be used with <b>urlib</b> because it needs an input (document or url) to create a soup object, as it cannot fetch web page itself. You can learn in detail about this at <a target="_blank" rel="nofollow" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/.">www.crummy.com/software/BeautifulSoup/bs4/doc/</a></p>
<p>Note that before using it, we must install a third party library using the following command &minus;</p>
<pre class="result notranslate">
pip install bs4
</pre>
<p>Next, using Anaconda package manager, we can install Beautiful Soup as follows &minus;</p>
<pre class="result notranslate">
conda install -c anaconda beautifulsoup4
</pre>
<h2>Python Script for Preserving Web Pages</h2>
<p>The Python script for preserving web pages by using third party library called Beautiful Soup is discussed here &minus;</p>
<p>First, import the required libraries as follows &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import argparse

from bs4 import BeautifulSoup, SoupStrainer
from datetime import datetime

import hashlib
import logging
import os
import ssl
import sys
from urllib.request import urlopen

import urllib.error
logger = logging.getLogger(__name__)
</pre>
<p>Note that this script will take two positional arguments, one is URL which is to be preserved and other is the desired output directory as shown below &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == "__main__":
   parser = argparse.ArgumentParser('Web Page preservation')
   parser.add_argument("DOMAIN", help="Website Domain")
   parser.add_argument("OUTPUT_DIR", help="Preservation Output Directory")
   parser.add_argument("-l", help="Log file path",
   default=__file__[:-3] + ".log")
   args = parser.parse_args()
</pre>
<p>Now, setup the logging for the script by specifying a file and stream handler for being in loop and document the acquisition process as shown &minus;</p>
<pre class="prettyprint notranslate">
logger.setLevel(logging.DEBUG)
msg_fmt = logging.Formatter("%(asctime)-15s %(funcName)-10s""%(levelname)-8s %(message)s")
strhndl = logging.StreamHandler(sys.stderr)
strhndl.setFormatter(fmt=msg_fmt)
fhndl = logging.FileHandler(args.l, mode='a')
fhndl.setFormatter(fmt=msg_fmt)

logger.addHandler(strhndl)
logger.addHandler(fhndl)
logger.info("Starting BS Preservation")
logger.debug("Supplied arguments: {}".format(sys.argv[1:]))
logger.debug("System " + sys.platform)
logger.debug("Version " + sys.version)
</pre>
<p>Now, let us do the input validation on the desired output directory as follows &minus;</p>
<pre class="prettyprint notranslate">
if not os.path.exists(args.OUTPUT_DIR):
   os.makedirs(args.OUTPUT_DIR)
main(args.DOMAIN, args.OUTPUT_DIR)
</pre>
<p>Now, we will define the <b>main()</b> function which will extract the base name of the website by removing the unnecessary elements before the actual name along with additional validation on the input URL as follows &minus;</p>
<pre class="prettyprint notranslate">
def main(website, output_dir):
   base_name = website.replace("https://", "").replace("http://", "").replace("www.", "")
   link_queue = set()
   
   if "http://" not in website and "https://" not in website:
      logger.error("Exiting preservation - invalid user input: {}".format(website))
      sys.exit(1)
   logger.info("Accessing {} webpage".format(website))
   context = ssl._create_unverified_context()
</pre>
<p>Now, we need to open a connection with the URL by using urlopen() method. Let us use try-except block as follows &minus;</p>
<pre class="prettyprint notranslate">
try:
   index = urlopen(website, context=context).read().decode("utf-8")
except urllib.error.HTTPError as e:
   logger.error("Exiting preservation - unable to access page: {}".format(website))
   sys.exit(2)
logger.debug("Successfully accessed {}".format(website))
</pre>
<p>The next lines of code include three function as explained below &minus;</p>
<ul class="list">
<li><p><b>write_output()</b> to write the first web page to the output directory</p></li>
<li><p><b>find_links()</b> function to identify the links on this web page</p></li>
<li><p><b>recurse_pages()</b> function to iterate through and discover all links on the web page.</p></li>
</ul>
<pre class="prettyprint notranslate">
write_output(website, index, output_dir)
link_queue = find_links(base_name, index, link_queue)
logger.info("Found {} initial links on webpage".format(len(link_queue)))
recurse_pages(website, link_queue, context, output_dir)
logger.info("Completed preservation of {}".format(website))
</pre>
<p>Now, let us define <b>write_output()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
def write_output(name, data, output_dir, counter=0):
   name = name.replace("http://", "").replace("https://", "").rstrip("//")
   directory = os.path.join(output_dir, os.path.dirname(name))
   
   if not os.path.exists(directory) and os.path.dirname(name) != "":
      os.makedirs(directory)
</pre>
<p>We need to log some details about the web page and then we log the hash of the data by using <b>hash_data()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
logger.debug("Writing {} to {}".format(name, output_dir)) logger.debug("Data Hash: {}".format(hash_data(data)))
path = os.path.join(output_dir, name)
path = path + "_" + str(counter)
with open(path, "w") as outfile:
   outfile.write(data)
logger.debug("Output File Hash: {}".format(hash_file(path)))
</pre>
<p>Now, define <b>hash_data()</b> method with the help of which we read the <b>UTF-8</b> encoded data and then generate the <b>SHA-256</b> hash of it as follows &minus;</p>
<pre class="prettyprint notranslate">
def hash_data(data):
   sha256 = hashlib.sha256()
   sha256.update(data.encode("utf-8"))
   return sha256.hexdigest()
def hash_file(file):
   sha256 = hashlib.sha256()
   with open(file, "rb") as in_file:
      sha256.update(in_file.read())
return sha256.hexdigest()
</pre>
<p>Now, let us create a <b>Beautifulsoup</b> object out of the web page data under <b>find_links()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
def find_links(website, page, queue):
   for link in BeautifulSoup(page, "html.parser",parse_only = SoupStrainer("a", href = True)):
      if website in link.get("href"):
         if not os.path.basename(link.get("href")).startswith("#"):
            queue.add(link.get("href"))
   return queue
</pre>
<p>Now, we need to define <b>recurse_pages()</b> method by providing it the inputs of the website URL, current link queue, the unverified SSL context and the output directory as follows &minus;</p>
<pre class="prettyprint notranslate">
def recurse_pages(website, queue, context, output_dir):
   processed = []
   counter = 0
   
   while True:
      counter += 1
      if len(processed) == len(queue):
         break
      for link in queue.copy(): if link in processed:
         continue
	   processed.append(link)
      try:
      page = urlopen(link,      context=context).read().decode("utf-8")
      except urllib.error.HTTPError as e:
         msg = "Error accessing webpage: {}".format(link)
         logger.error(msg)
         continue
</pre>
<p>Now, write the output of each web page accessed in a file by passing the link name, page data, output directory and the counter as follows &minus;</p>
<pre class="prettyprint notranslate">
write_output(link, page, output_dir, counter)
queue = find_links(website, page, queue)
logger.info("Identified {} links throughout website".format(
   len(queue)))
</pre>
<p>Now, when we run this script by providing the URL of the website, the output directory and a path to the log file, we will get the details about that web page that can be used for future use.</p>
<h2>Virus Hunting</h2>
<p>Have you ever wondered how forensic analysts, security researchers, and incident respondents can understand the difference between useful software and malware? The answer lies in the question itself, because without studying about the malware, rapidly generating by hackers, it is quite impossible for researchers and specialists to tell the difference between useful software and malware. In this section, let us discuss about <b>VirusShare</b>, a tool to accomplish this task.</p>
<h2>Understanding VirusShare</h2>
<p>VirusShare is the largest privately owned collection of malware samples to provide security researchers, incident responders, and forensic analysts the samples of live malicious code. It contains over 30 million samples.</p>
<p>The benefit of VirusShare is the list of malware hashes that is freely available. Anybody can use these hashes to create a very comprehensive hash set and use that to identify potentially malicious files. But before using VirusShare, we suggest you to visit <a target="_blank" rel="nofollow" href="https://virusshare.com">https://virusshare.com</a> for more details.</p>
<h2>Creating Newline-Delimited Hash List from VirusShare using Python</h2>
<p>A hash list from VirusShare can be used by various forensic tools such as X-ways and EnCase. In the script discussed below, we are going to automate downloading lists of hashes from VirusShare to create a newline-delimited hash list.</p>
<p>For this script, we need a third party Python library <b>tqdm</b> which can be downloaded as follows &minus;</p>
<pre class="result notranslate">
pip install tqdm
</pre>
<p>Note that in this script, first we will read the VirusShare hashes page and dynamically identify the most recent hash list. Then we will initialize the progress bar and download the hash list in the desired range.</p>
<p>First, import the following libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function

import argparse
import os
import ssl
import sys
import tqdm

from urllib.request import urlopen
import urllib.error
</pre>
<p>This script will take one positional argument, which would be the desired path for the hash set &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('Hash set from VirusShare')
   parser.add_argument("OUTPUT_HASH", help = "Output Hashset")
   parser.add_argument("--start", type = int, help = "Optional starting location")
   args = parser.parse_args()
</pre>
<p>Now, we will perform the standard input validation as follows &minus;</p>
<pre class="prettyprint notranslate">
directory = os.path.dirname(args.OUTPUT_HASH)
if not os.path.exists(directory):
   os.makedirs(directory)
if args.start:
   main(args.OUTPUT_HASH, start=args.start)
else:
   main(args.OUTPUT_HASH)
</pre>
<p>Now we need to define <b>main()</b> function with <b>**kwargs</b> as an argument because this will create a dictionary we can refer to support supplied key arguments as shown below &minus;</p>
<pre class="prettyprint notranslate">
def main(hashset, **kwargs):
   url = "https://virusshare.com/hashes.4n6"
   print("[+] Identifying hash set range from {}".format(url))
   context = ssl._create_unverified_context()
</pre>
<p>Now, we need to open VirusShare hashes page by using <b>urlib.request.urlopen()</b> method. We will use try-except block as follows &minus;</p>
<pre class="prettyprint notranslate">
try:
   index = urlopen(url, context = context).read().decode("utf-8")
except urllib.error.HTTPError as e:
   print("[-] Error accessing webpage - exiting..")
   sys.exit(1)
</pre>
<p>Now, identify latest hash list from downloaded pages. You can do this by finding the last instance of the HTML <b>href</b> tag to VirusShare hash list. It can be done with the following lines of code &minus;</p>
<pre class="prettyprint notranslate">
tag = index.rfind(r'a href = "hashes/VirusShare_')
stop = int(index[tag + 27: tag + 27 + 5].lstrip("0"))

if "start" not in kwa&lt;rgs:
   start = 0
else:
   start = kwargs["start"]

if start &lt; 0 or start &gt; stop:
   print("[-] Supplied start argument must be greater than or equal ""to zero but less than the latest hash list, ""currently: {}".format(stop))
sys.exit(2)
print("[+] Creating a hashset from hash lists {} to {}".format(start, stop))
hashes_downloaded = 0
</pre>
<p>Now, we will use <b>tqdm.trange()</b> method to create a loop and progress bar as follows &minus;</p>
<pre class="prettyprint notranslate">
for x in tqdm.trange(start, stop + 1, unit_scale=True,desc="Progress"):
   url_hash = "https://virusshare.com/hashes/VirusShare_"\"{}.md5".format(str(x).zfill(5))
   try:
      hashes = urlopen(url_hash, context=context).read().decode("utf-8")
      hashes_list = hashes.split("\n")
   except urllib.error.HTTPError as e:
      print("[-] Error accessing webpage for hash list {}"" - continuing..".format(x))
   continue
</pre>
<p>After performing the above steps successfully, we will open the hash set text file in a+ mode to append to the bottom of text file.</p>
<pre class="prettyprint notranslate">
with open(hashset, "a+") as hashfile:
   for line in hashes_list:
   if not line.startswith("#") and line != "":
      hashes_downloaded += 1
      hashfile.write(line + '\n')
   print("[+] Finished downloading {} hashes into {}".format(
      hashes_downloaded, hashset))
</pre>
<p>After running the above script, you will get the latest hash list containing MD5 hash values in text format.</p>
<h1>Investigation Using Emails</h1>
<p>The previous chapters discussed about the importance and the process of network forensics and the concepts involved. In this chapter, let us learn about the role of emails in digital forensics and their investigation using Python.</p>
<h2>Role of Email in Investigation</h2>
<p>Emails play a very important role in business communications and have emerged as one of the most important applications on internet. They are a convenient mode for sending messages as well as documents, not only from computers but also from other electronic gadgets such as mobile phones and tablets.</p>
<p>The negative side of emails is that criminals may leak important information about their company. Hence, the role of emails in digital forensics has been increased in recent years. In digital forensics, emails are considered as crucial evidences and Email Header Analysis has become important to collect evidence during forensic process.</p>
<p>An investigator has the following goals while performing email forensics &minus;</p>
<ul class="list">
<li>To identify the main criminal</li>
<li>To collect necessary evidences</li>
<li>To presenting the findings</li>
<li>To build the case</li>
</ul>
<h2>Challenges in Email Forensics</h2>
<p>Email forensics play a very important role in investigation as most of the communication in present era relies on emails. However, an email forensic investigator may face the following challenges during the investigation &minus;</p>
<h3>Fake Emails</h3>
<p>The biggest challenge in email forensics is the use of fake e-mails that are created by manipulating and scripting headers etc. In this category criminals also use temporary email which is a service that allows a registered user to receive email at a temporary address that expires after a certain time period.</p>
<h3>Spoofing</h3>
<p>Another challenge in email forensics is spoofing in which criminals used to present an email as someone else’s. In this case the machine will receive both fake as well as original IP address.</p>
<h3>Anonymous Re-emailing</h3>
<p>Here, the Email server strips identifying information from the email message before forwarding it further. This leads to another big challenge for email investigations.</p>
<h2>Techniques Used in Email Forensic Investigation</h2>
<p>Email forensics is the study of source and content of email as evidence to identify the actual sender and recipient of a message along with some other information such as date/time of transmission and intention of sender. It involves investigating metadata, port scanning as well as keyword searching.</p>
<p>Some of the common techniques which can be used for email forensic investigation are</p>
<ul class="list">
<li>Header Analysis</li>
<li>Server investigation</li>
<li>Network Device Investigation</li>
<li>Sender Mailer Fingerprints</li>
<li>Software Embedded Identifiers</li>
</ul>
<p>In the following sections, we are going to learn how to fetch information using Python for the purpose of email investigation.</p>
<h2>Extraction of Information from EML files</h2>
<p>EML files are basically emails in file format which are widely used for storing email messages. They are structured text files that are compatible across multiple email clients such as Microsoft Outlook, Outlook Express, and Windows Live Mail.</p>
<p>An EML file stores email headers, body content, attachment data as plain text. It uses base64 to encode binary data and Quoted-Printable (QP) encoding to store content information. The Python script that can be used to extract information from EML file is given below &minus;</p>
<p>First, import the following Python libraries as shown below &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser, FileType
from email import message_from_file

import os
import quopri
import base64
</pre>
<p>In the above libraries, <b>quopri</b> is used to decode the QP encoded values from EML files. Any base64 encoded data can be decoded with the help of <b>base64</b> library.</p>
<p>Next, let us provide argument for command-line handler. Note that here it will accept only one argument which would be the path to EML file as shown below &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = ArgumentParser('Extracting information from EML file')
   parser.add_argument("EML_FILE",help="Path to EML File", type=FileType('r'))
   args = parser.parse_args()
   main(args.EML_FILE)
</pre>
<p>Now, we need to define <b>main()</b> function in which we will use the method named <b>message_from_file()</b> from email library to read the file like object. Here we will access the headers, body content, attachments and other payload information by using resulting variable named <b>emlfile</b> as shown in the code given below &minus;</p>
<pre class="prettyprint notranslate">
def main(input_file):
   emlfile = message_from_file(input_file)
   for key, value in emlfile._headers:
      print("{}: {}".format(key, value))
print("\nBody\n")

if emlfile.is_multipart():
   for part in emlfile.get_payload():
      process_payload(part)
else:
   process_payload(emlfile[1])
</pre>
<p>Now, we need to define <b>process_payload()</b> method in which we will extract message body content by using <b>get_payload()</b> method. We will decode QP encoded data by using <b>quopri.decodestring()</b> function. We will also check the content MIME type so that it can handle the storage of the email properly. Observe the code given below &minus;</p>
<pre class="prettyprint notranslate">
def process_payload(payload):
   print(payload.get_content_type() + "\n" + "=" * len(payload.get_content_type()))
   body = quopri.decodestring(payload.get_payload())
   
   if payload.get_charset():
      body = body.decode(payload.get_charset())
else:
   try:
      body = body.decode()
   except UnicodeDecodeError:
      body = body.decode('cp1252')

if payload.get_content_type() == "text/html":
   outfile = os.path.basename(args.EML_FILE.name) + ".html"
   open(outfile, 'w').write(body)
elif payload.get_content_type().startswith('application'):
   outfile = open(payload.get_filename(), 'wb')
   body = base64.b64decode(payload.get_payload())
   outfile.write(body)
   outfile.close()
   print("Exported: {}\n".format(outfile.name))
else:
   print(body)
</pre>
<p>After executing the above script, we will get the header information along with various payloads on the console.</p>
<h2>Analyzing MSG Files using Python</h2>
<p>Email messages come in many different formats. MSG is one such kind of format used by Microsoft Outlook and Exchange. Files with MSG extension may contain plain ASCII text for the headers and the main message body as well as hyperlinks and attachments.</p>
<p>In this section, we will learn how to extract information from MSG file using Outlook API. Note that the following Python script will work only on Windows. For this, we need to install third party Python library named <b>pywin32</b> as follows &minus;</p>
<pre class="result notranslate">
pip install pywin32
</pre>
<p>Now, import the following libraries using the commands shown &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser

import os
import win32com.client
import pywintypes
</pre>
<p>Now, let us provide an argument for command-line handler. Here it will accept two arguments one would be the path to MSG file and other would be the desired output folder as follows &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = ArgumentParser(‘Extracting information from MSG file’)
   parser.add_argument("MSG_FILE", help="Path to MSG file")
   parser.add_argument("OUTPUT_DIR", help="Path to output folder")
   args = parser.parse_args()
   out_dir = args.OUTPUT_DIR
   
   if not os.path.exists(out_dir):
      os.makedirs(out_dir)
   main(args.MSG_FILE, args.OUTPUT_DIR)
</pre>
<p>Now, we need to define <b>main()</b> function in which we will call <b>win32com</b> library for setting up <b>Outlook API</b> which further allows access to the <b>MAPI</b> namespace.</p>
<pre class="prettyprint notranslate">
def main(msg_file, output_dir):
   mapi = win32com.client.Dispatch("Outlook.Application").GetNamespace("MAPI")
   msg = mapi.OpenSharedItem(os.path.abspath(args.MSG_FILE))
   
   display_msg_attribs(msg)
   display_msg_recipients(msg)
   
   extract_msg_body(msg, output_dir)
   extract_attachments(msg, output_dir)
</pre>
<p>Now, define different functions which we are using in this script. The code given below shows defining the <b>display_msg_attribs()</b> function that allow us to display various attributes of a message like subject, to , BCC, CC, Size, SenderName, sent, etc.</p>
<pre class="prettyprint notranslate">
def display_msg_attribs(msg):
   attribs = [
      'Application', 'AutoForwarded', 'BCC', 'CC', 'Class',
      'ConversationID', 'ConversationTopic', 'CreationTime',
      'ExpiryTime', 'Importance', 'InternetCodePage', 'IsMarkedAsTask',
      'LastModificationTime', 'Links','ReceivedTime', 'ReminderSet',
      'ReminderTime', 'ReplyRecipientNames', 'Saved', 'Sender',
      'SenderEmailAddress', 'SenderEmailType', 'SenderName', 'Sent',
      'SentOn', 'SentOnBehalfOfName', 'Size', 'Subject',
      'TaskCompletedDate', 'TaskDueDate', 'To', 'UnRead'
   ]
   print("\nMessage Attributes")
   for entry in attribs:
      print("{}: {}".format(entry, getattr(msg, entry, 'N/A')))
</pre>
<p>Now, define the <b>display_msg_recipeints()</b> function that iterates through the messages and displays the recipient details.</p>
<pre class="prettyprint notranslate">
def display_msg_recipients(msg):
   recipient_attrib = ['Address', 'AutoResponse', 'Name', 'Resolved', 'Sendable']
   i = 1
   
   while True:
   try:
      recipient = msg.Recipients(i)
   except pywintypes.com_error:
      break
   print("\nRecipient {}".format(i))
   print("=" * 15)
   
   for entry in recipient_attrib:
      print("{}: {}".format(entry, getattr(recipient, entry, 'N/A')))
   i += 1
</pre>
<p>Next, we define <b>extract_msg_body()</b> function that extracts the body content, HTML as well as Plain text, from the message.</p>
<pre class="prettyprint notranslate">
def extract_msg_body(msg, out_dir):
   html_data = msg.HTMLBody.encode('cp1252')
   outfile = os.path.join(out_dir, os.path.basename(args.MSG_FILE))
   
   open(outfile + ".body.html", 'wb').write(html_data)
   print("Exported: {}".format(outfile + ".body.html"))
   body_data = msg.Body.encode('cp1252')
   
   open(outfile + ".body.txt", 'wb').write(body_data)
   print("Exported: {}".format(outfile + ".body.txt"))
</pre>
<p>Next, we shall define the <b>extract_attachments()</b> function that exports attachment data into desired output directory.</p>
<pre class="prettyprint notranslate">
def extract_attachments(msg, out_dir):
   attachment_attribs = ['DisplayName', 'FileName', 'PathName', 'Position', 'Size']
   i = 1 # Attachments start at 1
   
   while True:
      try:
         attachment = msg.Attachments(i)
   except pywintypes.com_error:
      break
</pre>
<p>Once all the functions are defined, we will print all the attributes to the console with the following line of codes &minus;</p>
<pre class="prettyprint notranslate">
print("\nAttachment {}".format(i))
print("=" * 15)
   
for entry in attachment_attribs:
   print('{}: {}'.format(entry, getattr(attachment, entry,"N/A")))
outfile = os.path.join(os.path.abspath(out_dir),os.path.split(args.MSG_FILE)[-1])
   
if not os.path.exists(outfile):
os.makedirs(outfile)
outfile = os.path.join(outfile, attachment.FileName)
attachment.SaveAsFile(outfile)
   
print("Exported: {}".format(outfile))
i += 1
</pre>
<p>After running the above script, we will get the attributes of message and its attachments in the console window along with several files in the output directory.</p>
<h2>Structuring MBOX files from Google Takeout using Python</h2>
<p>MBOX files are text files with special formatting that split messages stored within. They are often found in association with UNIX systems, Thunderbolt, and Google Takeouts.</p>
<p>In this section, you will see a Python script, where we will be structuring MBOX files got from Google Takeouts. But before that we must know that how we can generate these MBOX files by using our Google account or Gmail account.</p>
<h2>Acquiring Google Account Mailbox into MBX Format</h2>
<p>Acquiring of Google account mailbox implies taking backup of our Gmail account. Backup can be taken for various personal or professional reasons. Note that Google provides backing up of Gmail data. To acquire our Google account mailbox into MBOX format, you need to follow the steps given below &minus;</p>
<ul class="list">
<li><p>Open <b>My account</b> dashboard.</p></li>
<li><p>Go to Personal info &amp; privacy section and select Control your content link.</p></li>
<li><p>You can create a new archive or can manage existing one. If we click, <b>CREATE ARCHIVE</b> link, then we will get some check boxes for each Google product we wish to include.</p></li>
<li><p>After selecting the products, we will get the freedom to choose file type and maximum size for our archive along with the delivery method to select from list.</p></li>
<li><p>Finally, we will get this backup in MBOX format.</p></li>
</ul>
<h3>Python Code</h3>
<p>Now, the MBOX file discussed above can be structured using Python as shown below &minus;</p>
<p>First, need to import Python libraries as follows &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser

import mailbox
import os
import time
import csv
from tqdm import tqdm

import base64
</pre>
<p>All the libraries have been used and explained in earlier scripts, except the <b>mailbox</b> library which is used to parse MBOX files.</p>
<p>Now, provide an argument for command-line handler. Here it will accept two arguments&minus; one would be the path to MBOX file, and the other would be the desired output folder.</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = ArgumentParser('Parsing MBOX files')
   parser.add_argument("MBOX", help="Path to mbox file")
   parser.add_argument(
      "OUTPUT_DIR",help = "Path to output directory to write report ""and exported content")
   args = parser.parse_args()
   main(args.MBOX, args.OUTPUT_DIR)
</pre>
<p>Now, will define <b>main()</b> function and call <b>mbox</b> class of mailbox library with the help of which we can parse a MBOX file by providing its path &minus;</p>
<pre class="prettyprint notranslate">
def main(mbox_file, output_dir):
   print("Reading mbox file")
   mbox = mailbox.mbox(mbox_file, factory=custom_reader)
   print("{} messages to parse".format(len(mbox)))
</pre>
<p>Now, define a reader method for <b>mailbox</b> library as follows &minus;</p>
<pre class="prettyprint notranslate">
def custom_reader(data_stream):
   data = data_stream.read()
   try:
      content = data.decode("ascii")
   except (UnicodeDecodeError, UnicodeEncodeError) as e:
      content = data.decode("cp1252", errors="replace")
   return mailbox.mboxMessage(content)
</pre>
<p>Now, create some variables for further processing as follows &minus;</p>
<pre class="prettyprint notranslate">
parsed_data = []
attachments_dir = os.path.join(output_dir, "attachments")

if not os.path.exists(attachments_dir):
   os.makedirs(attachments_dir)
columns = [
   "Date", "From", "To", "Subject", "X-Gmail-Labels", "Return-Path", "Received", 
   "Content-Type", "Message-ID","X-GM-THRID", "num_attachments_exported", "export_path"]
</pre>
<p>Next, use <b>tqdm</b> to generate a progress bar and to track the iteration process as follows &minus;</p>
<pre class="prettyprint notranslate">
for message in tqdm(mbox):
   msg_data = dict()
   header_data = dict(message._headers)
for hdr in columns:
   msg_data[hdr] = header_data.get(hdr, "N/A")
</pre>
<p>Now, check weather message is having payloads or not. If it is having then we will define <b>write_payload()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
if len(message.get_payload()):
   export_path = write_payload(message, attachments_dir)
   msg_data['num_attachments_exported'] = len(export_path)
   msg_data['export_path'] = ", ".join(export_path)
</pre>
<p>Now, data need to be appended. Then we will call <b>create_report()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
parsed_data.append(msg_data)
create_report(
   parsed_data, os.path.join(output_dir, "mbox_report.csv"), columns)
def write_payload(msg, out_dir):
   pyld = msg.get_payload()
   export_path = []
   
if msg.is_multipart():
   for entry in pyld:
      export_path += write_payload(entry, out_dir)
else:
   content_type = msg.get_content_type()
   if "application/" in content_type.lower():
      content = base64.b64decode(msg.get_payload())
      export_path.append(export_content(msg, out_dir, content))
   elif "image/" in content_type.lower():
      content = base64.b64decode(msg.get_payload())
      export_path.append(export_content(msg, out_dir, content))

   elif "video/" in content_type.lower():
      content = base64.b64decode(msg.get_payload())
      export_path.append(export_content(msg, out_dir, content))
   elif "audio/" in content_type.lower():
      content = base64.b64decode(msg.get_payload())
      export_path.append(export_content(msg, out_dir, content))
   elif "text/csv" in content_type.lower():
      content = base64.b64decode(msg.get_payload())
      export_path.append(export_content(msg, out_dir, content))
   elif "info/" in content_type.lower():
      export_path.append(export_content(msg, out_dir,
      msg.get_payload()))
   elif "text/calendar" in content_type.lower():
      export_path.append(export_content(msg, out_dir,
      msg.get_payload()))
   elif "text/rtf" in content_type.lower():
      export_path.append(export_content(msg, out_dir,
      msg.get_payload()))
   else:
      if "name=" in msg.get('Content-Disposition', "N/A"):
         content = base64.b64decode(msg.get_payload())
      export_path.append(export_content(msg, out_dir, content))
   elif "name=" in msg.get('Content-Type', "N/A"):
      content = base64.b64decode(msg.get_payload())
      export_path.append(export_content(msg, out_dir, content))
return export_path
</pre>
<p>Observe that the above if-else statements are easy to understand. Now, we need to define a method that will extract the filename from the <b>msg</b> object as follows &minus;</p>
<pre class="prettyprint notranslate">
def export_content(msg, out_dir, content_data):
   file_name = get_filename(msg)
   file_ext = "FILE"
   
   if "." in file_name: file_ext = file_name.rsplit(".", 1)[-1]
   file_name = "{}_{:.4f}.{}".format(file_name.rsplit(".", 1)[0], time.time(), file_ext)
   file_name = os.path.join(out_dir, file_name)
</pre>
<p>Now, with the help of following lines of code, you can actually export the file &minus;</p>
<pre class="prettyprint notranslate">
if isinstance(content_data, str):
   open(file_name, 'w').write(content_data)
else:
   open(file_name, 'wb').write(content_data)
return file_name
</pre>
<p>Now, let us define a function to extract filenames from the <b>message</b> to accurately represent the names of these files as follows &minus;</p>
<pre class="prettyprint notranslate">
def get_filename(msg):
   if 'name=' in msg.get("Content-Disposition", "N/A"):
      fname_data = msg["Content-Disposition"].replace("\r\n", " ")
      fname = [x for x in fname_data.split("; ") if 'name=' in x]
      file_name = fname[0].split("=", 1)[-1]
   elif 'name=' in msg.get("Content-Type", "N/A"):
      fname_data = msg["Content-Type"].replace("\r\n", " ")
      fname = [x for x in fname_data.split("; ") if 'name=' in x]
      file_name = fname[0].split("=", 1)[-1]
   else:
      file_name = "NO_FILENAME"
   fchars = [x for x in file_name if x.isalnum() or x.isspace() or x == "."]
   return "".join(fchars)
</pre>
<p>Now, we can write a CSV file by defining the <b>create_report()</b> function as follows &minus;</p>
<pre class="prettyprint notranslate">
def create_report(output_data, output_file, columns):
   with open(output_file, 'w', newline="") as outfile:
      csvfile = csv.DictWriter(outfile, columns)
      csvfile.writeheader()
      csvfile.writerows(output_data)
</pre>
<p>Once you run the script given above, we will get the CSV report and directory full of attachments.</p>
<h1>Important Artifacts In Windows-I</h1>
<p>This chapter will explain various concepts involved in Microsoft Windows forensics and the important artifacts that an investigator can obtain from the investigation process.</p>
<h2>Introduction</h2>
<p>Artifacts are the objects or areas within a computer system that have important information related to the activities performed by the computer user. The type and location of this information depends upon the operating system. During forensic analysis, these artifacts play a very important role in approving or disapproving the investigator’s observation.</p>
<h2>Importance of Windows Artifacts for Forensics</h2>
<p>Windows artifacts assume significance due to the following reasons &minus;</p>
<ul class="list">
<li><p>Around 90% of the traffic in world comes from the computers using Windows as their operating system. That is why for digital forensics examiners Windows artifacts are very essentials.</p></li>
<li><p>The Windows operating system stores different types of evidences related to the user activity on computer system. This is another reason which shows the importance of Windows artifacts for digital forensics.</p></li>
<li><p>Many times the investigator revolves the investigation around old and traditional areas like user crated data. Windows artifacts can lead the investigation towards non-traditional areas like system created data or the artifacts.</p></li>
<li><p>Great abundance of artifacts is provided by Windows which are helpful for investigators as well as for companies and individuals performing informal investigations.</p></li>
<li><p>Increase in cyber-crime in recent years is another reason that Windows artifacts are important.</p></li>
</ul>
<h2>Windows Artifacts and their Python Scripts</h2>
<p>In this section, we are going to discuss about some Windows artifacts and Python scripts to fetch information from them.</p>
<h3>Recycle Bin</h3>
<p>It is one of the important Windows artifacts for forensic investigation. Windows recycle bin contains the files that have been deleted by the user, but not physically removed by the system yet. Even if the user completely removes the file from system, it serves as an important source of investigation. This is because the examiner can extract valuable information, like original file path as well as time that it was sent to Recycle Bin, from the deleted files.</p>
<p>Note that the storage of Recycle Bin evidence depends upon the version of Windows. In the following Python script, we are going to deal with Windows 7 where it creates two files: <b>$R</b> file that contains the actual content of the recycled file and <b>$I</b> file that contains original file name, path, file size when file was deleted.</p>
<p>For Python script we need to install third party modules namely <b>pytsk3, pyewf</b> and <b>unicodecsv</b>. We can use <b>pip</b> to install them. We can follow the following steps to extract information from Recycle Bin &minus;</p>
<ul class="list">
<li><p>First, we need to use recursive method to scan through the <b>$Recycle.bin</b> folder and select all the files starting with <b>$I</b>.</p></li>
<li><p>Next, we will read the contents of the files and parse the available metadata structures.</p></li>
<li><p>Now, we will search for the associated $R file.</p></li>
<li><p>At last, we will write the results into CSV file for review.</p></li>
</ul>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, we need to import the following Python libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser

import datetime
import os
import struct

from utility.pytskutil import TSKUtil
import unicodecsv as csv
</pre>
<p>Next, we need to provide argument for command-line handler. Note that here it will accept three arguments – first is the path to evidence file, second is the type of evidence file and third is the desired output path to the CSV report, as shown below &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('Recycle Bin evidences')
   parser.add_argument('EVIDENCE_FILE', help = "Path to evidence file")
   parser.add_argument('IMAGE_TYPE', help = "Evidence file format",
   choices = ('ewf', 'raw'))
   parser.add_argument('CSV_REPORT', help = "Path to CSV report")
   args = parser.parse_args()
   main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.CSV_REPORT)
</pre>
<p>Now, define the <b>main()</b> function that will handle all the processing. It will search for <b>$I</b> file as follows &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, image_type, report_file):
   tsk_util = TSKUtil(evidence, image_type)
   dollar_i_files = tsk_util.recurse_files("$I", path = '/$Recycle.bin',logic = "startswith")
   
   if dollar_i_files is not None:
      processed_files = process_dollar_i(tsk_util, dollar_i_files)
      write_csv(report_file,['file_path', 'file_size', 'deleted_time','dollar_i_file', 'dollar_r_file', 'is_directory'],processed_files)
   else:
      print("No $I files found")
</pre>
<p>Now, if we found <b>$I</b> file, then it must be sent to <b>process_dollar_i()</b> function which will accept the <b>tsk_util</b> object as well as the list of <b>$I</b> files, as shown below &minus;</p>
<pre class="prettyprint notranslate">
def process_dollar_i(tsk_util, dollar_i_files):
   processed_files = []
   
   for dollar_i in dollar_i_files:
      file_attribs = read_dollar_i(dollar_i[2])
      if file_attribs is None:
         continue
      file_attribs['dollar_i_file'] = os.path.join('/$Recycle.bin', dollar_i[1][1:])
</pre>
<p>Now, search for $R files as follows &minus;</p>
<pre class="prettyprint notranslate">
recycle_file_path = os.path.join('/$Recycle.bin',dollar_i[1].rsplit("/", 1)[0][1:])
dollar_r_files = tsk_util.recurse_files(
   "$R" + dollar_i[0][2:],path = recycle_file_path, logic = "startswith")
   
   if dollar_r_files is None:
      dollar_r_dir = os.path.join(recycle_file_path,"$R" + dollar_i[0][2:])
      dollar_r_dirs = tsk_util.query_directory(dollar_r_dir)
   
   if dollar_r_dirs is None:
      file_attribs['dollar_r_file'] = "Not Found"
      file_attribs['is_directory'] = 'Unknown'
   
   else:
      file_attribs['dollar_r_file'] = dollar_r_dir
      file_attribs['is_directory'] = True
   
   else:
      dollar_r = [os.path.join(recycle_file_path, r[1][1:])for r in dollar_r_files]
      file_attribs['dollar_r_file'] = ";".join(dollar_r)
      file_attribs['is_directory'] = False
      processed_files.append(file_attribs)
   return processed_files  
</pre>
<p>Now, define <b>read_dollar_i()</b> method to read the <b>$I</b> files, in other words, parse the metadata. We will use <b>read_random()</b> method to read the signature’s first eight bytes. This will return none if signature does not match. After that, we will have to read and unpack the values from <b>$I</b> file if that is a valid file.</p>
<pre class="prettyprint notranslate">
def read_dollar_i(file_obj):
   if file_obj.read_random(0, 8) != '\x01\x00\x00\x00\x00\x00\x00\x00':
      return None
   raw_file_size = struct.unpack('&lt;q', file_obj.read_random(8, 8))
   raw_deleted_time = struct.unpack('&lt;q',   file_obj.read_random(16, 8))
   raw_file_path = file_obj.read_random(24, 520)
</pre>
<p>Now, after extracting these files we need to interpret the integers into human-readable values by using <b>sizeof_fmt()</b> function as shown below &minus;</p>
<pre class="prettyprint notranslate">
file_size = sizeof_fmt(raw_file_size[0])
deleted_time = parse_windows_filetime(raw_deleted_time[0])

file_path = raw_file_path.decode("utf16").strip("\x00")
return {'file_size': file_size, 'file_path': file_path,'deleted_time': deleted_time}
</pre>
<p>Now, we need to define <b>sizeof_fmt()</b> function as follows &minus;</p>
<pre class="prettyprint notranslate">
def sizeof_fmt(num, suffix = 'B'):
   for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:
      if abs(num) &lt; 1024.0:
         return "%3.1f%s%s" % (num, unit, suffix)
      num /= 1024.0
   return "%.1f%s%s" % (num, 'Yi', suffix)
</pre>
<p>Now, define a function for interpreted integers into formatted date and time as follows &minus;</p>
<pre class="prettyprint notranslate">
def parse_windows_filetime(date_value):
   microseconds = float(date_value) / 10
   ts = datetime.datetime(1601, 1, 1) + datetime.timedelta(
      microseconds = microseconds)
   return ts.strftime('%Y-%m-%d %H:%M:%S.%f')
</pre>
<p>Now, we will define <b>write_csv()</b> method to write the processed results into a CSV file as follows &minus;</p>
<pre class="prettyprint notranslate">
def write_csv(outfile, fieldnames, data):
   with open(outfile, 'wb') as open_outfile:
      csvfile = csv.DictWriter(open_outfile, fieldnames)
      csvfile.writeheader()
      csvfile.writerows(data)
</pre>
<p>When you run the above script, we will get the data from $I and $R file.</p>
<h3>Sticky Notes</h3>
<p>Windows Sticky Notes replaces the real world habit of writing with pen and paper. These notes used to float on the desktop with different options for colors, fonts etc. In Windows 7 the Sticky Notes file is stored as an OLE file hence in the following Python script we will investigate this OLE file to extract metadata from Sticky Notes.</p>
<p>For this Python script, we need to install third party modules namely <b>olefile, pytsk3, pyewf</b> and unicodecsv. We can use the command <b>pip</b> to install them.</p>
<p>We can follow the steps discussed below for extracting the information from Sticky note file namely <b>StickyNote.sn</b> &minus;</p>
<ul class="list">
<li><p>Firstly, open the evidence file and find all the StickyNote.snt files.</p></li>
<li><p>Then, parse the metadata and content from the OLE stream and write the RTF content to files.</p></li>
<li><p>Lastly, create CSV report of this metadata.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, import the following Python libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser

import unicodecsv as csv
import os
import StringIO

from utility.pytskutil import TSKUtil
import olefile
</pre>
<p>Next, define a global variable which will be used across this script &minus;</p>
<pre class="result notranslate">
REPORT_COLS = ['note_id', 'created', 'modified', 'note_text', 'note_file']
</pre>
<p>Next, we need to provide argument for command-line handler. Note that here it will accept three arguments – first is the path to evidence file, second is the type of evidence file and third is the desired output path as follows &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('Evidence from Sticky Notes')
   parser.add_argument('EVIDENCE_FILE', help="Path to evidence file")
   parser.add_argument('IMAGE_TYPE', help="Evidence file format",choices=('ewf', 'raw'))
   parser.add_argument('REPORT_FOLDER', help="Path to report folder")
   args = parser.parse_args()
   main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.REPORT_FOLDER)
</pre>
<p>Now, we will define <b>main()</b> function which will be similar to the previous script as shown below &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, image_type, report_folder):
   tsk_util = TSKUtil(evidence, image_type)
   note_files = tsk_util.recurse_files('StickyNotes.snt', '/Users','equals')
</pre>
<p>Now, let us iterate through the resulting files. Then we will call <b>parse_snt_file()</b> function to process the file and then we will write RTF file with the <b>write_note_rtf()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
report_details = []
for note_file in note_files:
   user_dir = note_file[1].split("/")[1]
   file_like_obj = create_file_like_obj(note_file[2])
   note_data = parse_snt_file(file_like_obj)
   
   if note_data is None:
      continue
   write_note_rtf(note_data, os.path.join(report_folder, user_dir))
   report_details += prep_note_report(note_data, REPORT_COLS,"/Users" + note_file[1])
   write_csv(os.path.join(report_folder, 'sticky_notes.csv'), REPORT_COLS,report_details)
</pre>
<p>Next, we need to define various functions used in this script.</p>
<p>First of all we will define <b>create_file_like_obj()</b> function for reading the size of the file by taking <b>pytsk</b> file object. Then we will define <b>parse_snt_file()</b> function that will accept the file-like object as its input and is used to read and interpret the sticky note file.</p>
<pre class="prettyprint notranslate">
def parse_snt_file(snt_file):
   
   if not olefile.isOleFile(snt_file):
      print("This is not an OLE file")
      return None
   ole = olefile.OleFileIO(snt_file)
   note = {}
   
   for stream in ole.listdir():
      if stream[0].count("-") == 3:
         if stream[0] not in note:
            note[stream[0]] = {"created": ole.getctime(stream[0]),"modified": ole.getmtime(stream[0])}
         content = None
         if stream[1] == '0':
            content = ole.openstream(stream).read()
         elif stream[1] == '3':
            content = ole.openstream(stream).read().decode("utf-16")
         if content:
            note[stream[0]][stream[1]] = content
	return note
</pre>
<p>Now, create a RTF file by defining <b>write_note_rtf()</b> function as follows</p>
<pre class="prettyprint notranslate">
def write_note_rtf(note_data, report_folder):
   if not os.path.exists(report_folder):
      os.makedirs(report_folder)
   
   for note_id, stream_data in note_data.items():
      fname = os.path.join(report_folder, note_id + ".rtf")
      with open(fname, 'w') as open_file:
         open_file.write(stream_data['0'])
</pre>
<p>Now, we will translate the nested dictionary into a flat list of dictionaries that are more appropriate for a CSV spreadsheet. It will be done by defining <b>prep_note_report()</b> function. Lastly, we will define <b>write_csv()</b> function.</p>
<pre class="prettyprint notranslate">
def prep_note_report(note_data, report_cols, note_file):
   report_details = []
   
   for note_id, stream_data in note_data.items():
      report_details.append({
         "note_id": note_id,
         "created": stream_data['created'],
         "modified": stream_data['modified'],
         "note_text": stream_data['3'].strip("\x00"),
         "note_file": note_file
      })
   return report_details
def write_csv(outfile, fieldnames, data):
   with open(outfile, 'wb') as open_outfile:
      csvfile = csv.DictWriter(open_outfile, fieldnames)
      csvfile.writeheader()
      csvfile.writerows(data)
</pre>
<p>After running the above script, we will get the metadata from Sticky Notes file.</p>
<h3>Registry Files</h3>
<p>Windows registry files contain many important details which are like a treasure trove of information for a forensic analyst. It is a hierarchical database that contains details related to operating system configuration, user activity, software installation etc. In the following Python script we are going to access common baseline information from the <b>SYSTEM</b> and <b>SOFTWARE</b> hives.</p>
<p>For this Python script, we need to install third party modules namely <b>pytsk3, pyewf</b> and <b>registry</b>. We can use <b>pip</b> to install them.</p>
<p>We can follow the steps given below for extracting the information from Windows registry &minus;</p>
<ul class="list">
<li><p>First, find registry hives to process by its name as well as by path.</p></li>
<li><p>Then we to open these files by using StringIO and Registry modules.</p></li>
<li><p>At last we need to process each and every hive and print the parsed values to the console for interpretation.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, import the following Python libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser

import datetime
import StringIO
import struct

from utility.pytskutil import TSKUtil
from Registry import Registry
</pre>
<p>Now, provide argument for the command-line handler. Here it will accept two arguments - first is the path to the evidence file, second is the type of evidence file, as shown below &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('Evidence from Windows Registry')
   parser.add_argument('EVIDENCE_FILE', help = "Path to evidence file")
   parser.add_argument('IMAGE_TYPE', help = "Evidence file format",
   choices = ('ewf', 'raw'))
   args = parser.parse_args()
   main(args.EVIDENCE_FILE, args.IMAGE_TYPE)
</pre>
<p>Now we will define <b>main()</b> function for searching <b>SYSTEM</b> and <b>SOFTWARE</b> hives within <b>/Windows/System32/config</b> folder as follows &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, image_type):
   tsk_util = TSKUtil(evidence, image_type)
   tsk_system_hive = tsk_util.recurse_files('system', '/Windows/system32/config', 'equals')
   tsk_software_hive = tsk_util.recurse_files('software', '/Windows/system32/config', 'equals')
   system_hive = open_file_as_reg(tsk_system_hive[0][2])
   software_hive = open_file_as_reg(tsk_software_hive[0][2])
   process_system_hive(system_hive)
   process_software_hive(software_hive)
</pre>
<p>Now, define the function for opening the registry file. For this purpose, we need to gather the size of file from <b>pytsk</b> metadata as follows &minus;</p>
<pre class="prettyprint notranslate">
def open_file_as_reg(reg_file):
   file_size = reg_file.info.meta.size
   file_content = reg_file.read_random(0, file_size)
   file_like_obj = StringIO.StringIO(file_content)
   return Registry.Registry(file_like_obj)
</pre>
<p>Now, with the help of following method, we can process <b>SYSTEM></b> hive &minus;</p>
<pre class="prettyprint notranslate">
def process_system_hive(hive):
   root = hive.root()
   current_control_set = root.find_key("Select").value("Current").value()
   control_set = root.find_key("ControlSet{:03d}".format(current_control_set))
   raw_shutdown_time = struct.unpack(
      '&lt;Q', control_set.find_key("Control").find_key("Windows").value("ShutdownTime").value())
   
   shutdown_time = parse_windows_filetime(raw_shutdown_time[0])
   print("Last Shutdown Time: {}".format(shutdown_time))
   
   time_zone = control_set.find_key("Control").find_key("TimeZoneInformation")
      .value("TimeZoneKeyName").value()
   
   print("Machine Time Zone: {}".format(time_zone))
   computer_name = control_set.find_key("Control").find_key("ComputerName").find_key("ComputerName")
      .value("ComputerName").value()
   
   print("Machine Name: {}".format(computer_name))
   last_access = control_set.find_key("Control").find_key("FileSystem")
      .value("NtfsDisableLastAccessUpdate").value()
   last_access = "Disabled" if last_access == 1 else "enabled"
   print("Last Access Updates: {}".format(last_access))
</pre>
<p>Now, we need to define a function for interpreted integers into formatted date and time as follows &minus;</p>
<pre class="prettyprint notranslate">
def parse_windows_filetime(date_value):
   microseconds = float(date_value) / 10
   ts = datetime.datetime(1601, 1, 1) + datetime.timedelta(microseconds = microseconds)
   return ts.strftime('%Y-%m-%d %H:%M:%S.%f')

def parse_unix_epoch(date_value):
   ts = datetime.datetime.fromtimestamp(date_value)
   return ts.strftime('%Y-%m-%d %H:%M:%S.%f')
</pre>
<p>Now with the help of following method we can process <b>SOFTWARE</b> hive &minus;</p>
<pre class="prettyprint notranslate">
def process_software_hive(hive):
   root = hive.root()
   nt_curr_ver = root.find_key("Microsoft").find_key("Windows NT")
      .find_key("CurrentVersion")
   
   print("Product name: {}".format(nt_curr_ver.value("ProductName").value()))
   print("CSD Version: {}".format(nt_curr_ver.value("CSDVersion").value()))
   print("Current Build: {}".format(nt_curr_ver.value("CurrentBuild").value()))
   print("Registered Owner: {}".format(nt_curr_ver.value("RegisteredOwner").value()))
   print("Registered Org: 
      {}".format(nt_curr_ver.value("RegisteredOrganization").value()))
   
   raw_install_date = nt_curr_ver.value("InstallDate").value()
   install_date = parse_unix_epoch(raw_install_date)
   print("Installation Date: {}".format(install_date))
</pre>
<p>After running the above script, we will get the metadata stored in Windows Registry files.</p>
<h1>Important Artifacts In Windows-II</h1>
<p>This chapter talks about some more important artifacts in Windows and their extraction method using Python.</p>
<h2>User Activities</h2>
<p>Windows having <b>NTUSER.DAT</b> file for storing various user activities. Every user profile is having hive like <b>NTUSER.DAT</b>, which stores the information and configurations related to that user specifically. Hence, it is highly useful for the purpose of investigation by forensic analysts.</p>
<p>The following Python script will parse some of the keys of <b>NTUSER.DAT</b> for exploring the actions of a user on the system. Before proceeding further, for Python script, we need to install third party modules namely <b>Registry, pytsk3</b>, pyewf and <b>Jinja2</b>. We can use pip to install them.</p>
<p>We can follow the following steps to extract information from <b>NTUSER.DAT</b> file &minus;</p>
<ul class="list">
<li><p>First, search for all <b>NTUSER.DAT</b> files in the system.</p></li>
<li><p>Then parse the <b>WordWheelQuery, TypePath and RunMRU</b> key for each <b>NTUSER.DAT</b> file.</p></li>
<li><p>At last we will write these artifacts, already processed, to an HTML report by using <b>Jinja2</b> fmodule.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First of all, we need to import the following Python modules &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser

import os
import StringIO
import struct

from utility.pytskutil import TSKUtil
from Registry import Registry
import jinja2
</pre>
<p>Now, provide argument for command-line handler. Here it will accept three arguments - first is the path to evidence file, second is the type of evidence file and third is the desired output path to the HTML report, as shown below &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('Information from user activities')
   parser.add_argument('EVIDENCE_FILE',help = "Path to evidence file")
   parser.add_argument('IMAGE_TYPE',help = "Evidence file format",choices = ('ewf', 'raw'))
   parser.add_argument('REPORT',help = "Path to report file")
   args = parser.parse_args()
   main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.REPORT)
</pre>
<p>Now, let us define <b>main()</b> function for searching all <b>NTUSER.DAT</b> files, as shown &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, image_type, report):
   tsk_util = TSKUtil(evidence, image_type)
   tsk_ntuser_hives = tsk_util.recurse_files('ntuser.dat','/Users', 'equals')
   
   nt_rec = {
      'wordwheel': {'data': [], 'title': 'WordWheel Query'},
      'typed_path': {'data': [], 'title': 'Typed Paths'},
      'run_mru': {'data': [], 'title': 'Run MRU'}
   }
</pre>
<p>Now, we will try to find the key in <b>NTUSER.DAT</b> file and once you find it, define the user processing functions as shown below &minus;</p>
<pre class="prettyprint notranslate">
for ntuser in tsk_ntuser_hives:
   uname = ntuser[1].split("/")

open_ntuser = open_file_as_reg(ntuser[2])
try:
   explorer_key = open_ntuser.root().find_key("Software").find_key("Microsoft")
      .find_key("Windows").find_key("CurrentVersion").find_key("Explorer")
   except Registry.RegistryKeyNotFoundException:
      continue
   nt_rec['wordwheel']['data'] += parse_wordwheel(explorer_key, uname)
   nt_rec['typed_path']['data'] += parse_typed_paths(explorer_key, uname)
   nt_rec['run_mru']['data'] += parse_run_mru(explorer_key, uname)
   nt_rec['wordwheel']['headers'] = \ nt_rec['wordwheel']['data'][0].keys()
   nt_rec['typed_path']['headers'] = \ nt_rec['typed_path']['data'][0].keys()
   nt_rec['run_mru']['headers'] = \ nt_rec['run_mru']['data'][0].keys()
</pre>
<p>Now, pass the dictionary object and its path to <b>write_html()</b> method as follows &minus;</p>
<pre class="result notranslate">
write_html(report, nt_rec)
</pre>
<p>Now, define a method, that takes <b>pytsk</b> file handle and read it into the Registry class via the <b>StringIO</b> class.</p>
<pre class="prettyprint notranslate">
def open_file_as_reg(reg_file):
   file_size = reg_file.info.meta.size
   file_content = reg_file.read_random(0, file_size)
   file_like_obj = StringIO.StringIO(file_content)
   return Registry.Registry(file_like_obj)
</pre>
<p>Now, we will define the function that will parse and handles <b>WordWheelQuery</b> key from <b>NTUSER.DAT</b> file as follows &minus;</p>
<pre class="prettyprint notranslate">
def parse_wordwheel(explorer_key, username):
   try:
      wwq = explorer_key.find_key("WordWheelQuery")
   except Registry.RegistryKeyNotFoundException:
      return []
   mru_list = wwq.value("MRUListEx").value()
   mru_order = []
   
   for i in xrange(0, len(mru_list), 2):
      order_val = struct.unpack('h', mru_list[i:i + 2])[0]
   if order_val in mru_order and order_val in (0, -1):
      break
   else:
      mru_order.append(order_val)
   search_list = []
   
   for count, val in enumerate(mru_order):
      ts = "N/A"
      if count == 0:
         ts = wwq.timestamp()
      search_list.append({
         'timestamp': ts,
         'username': username,
         'order': count,
         'value_name': str(val),
         'search': wwq.value(str(val)).value().decode("UTF-16").strip("\x00")
})
   return search_list  
</pre>
<p>Now, we will define the function that will parse and handles <b>TypedPaths</b> key from <b>NTUSER.DAT</b> file as follows &minus;</p>
<pre class="prettyprint notranslate">
def parse_typed_paths(explorer_key, username):
   try:
      typed_paths = explorer_key.find_key("TypedPaths")
   except Registry.RegistryKeyNotFoundException:
      return []
   typed_path_details = []
   
   for val in typed_paths.values():
      typed_path_details.append({
         "username": username,
         "value_name": val.name(),
         "path": val.value()
      })
   return typed_path_details
</pre>
<p>Now, we will define the function that will parse and handles <b>RunMRU</b> key from <b>NTUSER.DAT</b> file as follows &minus;</p>
<pre class="prettyprint notranslate">
def parse_run_mru(explorer_key, username):
   try:
      run_mru = explorer_key.find_key("RunMRU")
   except Registry.RegistryKeyNotFoundException:
      return []
   
   if len(run_mru.values()) == 0:
      return []
   mru_list = run_mru.value("MRUList").value()
   mru_order = []
   
   for i in mru_list:
      mru_order.append(i)
   mru_details = []
   
   for count, val in enumerate(mru_order):
      ts = "N/A"
      if count == 0:
         ts = run_mru.timestamp()
      mru_details.append({
         "username": username,
         "timestamp": ts,
         "order": count,
         "value_name": val,
         "run_statement": run_mru.value(val).value()
      })
   return mru_details
</pre>
<p>Now, the following function will handle the creation of HTML report &minus;</p>
<pre class="prettyprint notranslate">
def write_html(outfile, data_dict):
   cwd = os.path.dirname(os.path.abspath(__file__))
   env = jinja2.Environment(loader=jinja2.FileSystemLoader(cwd))
   template = env.get_template("user_activity.html")
   rendering = template.render(nt_data=data_dict)
   
   with open(outfile, 'w') as open_outfile:
      open_outfile.write(rendering)
</pre>
<p>At last we can write HTML document for report. After running the above script, we will get the information from NTUSER.DAT file in HTML document format.</p>
<h2>LINK files</h2>
<p>Shortcuts files are created when a user or the operating system creates shortcut files for the files which are frequently used, double clicked or accessed from system drives such as attached storage. Such kinds of shortcut files are called link files. By accessing these link files, an investigator can find the activity of window such as the time and location from where these files have been accessed.</p>
<p>Let us discuss the Python script that we can use to get the information from these Windows LINK files.</p>
<p>For Python script, install third party modules namely <b>pylnk, pytsk3, pyewf</b>. We can follow the following steps to extract information from <b>lnk</b> files</p>
<ul class="list">
<li><p>First, search for <b>lnk</b> files within the system.</p></li>
<li><p>Then, extract the information from that file by iterating through them.</p></li>
<li><p>Now, at last we need to this information to a CSV report.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, import the following Python libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser

import csv
import StringIO

from utility.pytskutil import TSKUtil
import pylnk
</pre>
<p>Now, provide the argument for command-line handler. Here it will accept three arguments – first is the path to evidence file, second is the type of evidence file and third is the desired output path to the CSV report, as shown below &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = argparse.ArgumentParser('Parsing LNK files')
   parser.add_argument('EVIDENCE_FILE', help = "Path to evidence file")
   parser.add_argument('IMAGE_TYPE', help = "Evidence file format",choices = ('ewf', 'raw'))
   parser.add_argument('CSV_REPORT', help = "Path to CSV report")
   args = parser.parse_args()
   main(args.EVIDENCE_FILE, args.IMAGE_TYPE, args.CSV_REPORT)
</pre>
<p>Now, interpret the evidence file by creating an object of <b>TSKUtil</b> and iterate through the file system to find files ending with <b>lnk</b>. It can be done by defining <b>main()</b> function as follows &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, image_type, report):
   tsk_util = TSKUtil(evidence, image_type)
   lnk_files = tsk_util.recurse_files("lnk", path="/", logic="endswith")
   
   if lnk_files is None:
      print("No lnk files found")
      exit(0)
   columns = [
      'command_line_arguments', 'description', 'drive_serial_number',
      'drive_type', 'file_access_time', 'file_attribute_flags',
      'file_creation_time', 'file_modification_time', 'file_size',
      'environmental_variables_location', 'volume_label',
      'machine_identifier', 'local_path', 'network_path',
      'relative_path', 'working_directory'
   ]
</pre>
<p>Now with the help of following code, we will iterate through <b>lnk</b> files by creating a function as follows &minus;</p>
<pre class="prettyprint notranslate">
parsed_lnks = []

for entry in lnk_files:
   lnk = open_file_as_lnk(entry[2])
   lnk_data = {'lnk_path': entry[1], 'lnk_name': entry[0]}
   
   for col in columns:
      lnk_data[col] = getattr(lnk, col, "N/A")
   lnk.close()
   parsed_lnks.append(lnk_data)
write_csv(report, columns + ['lnk_path', 'lnk_name'], parsed_lnks)
</pre>
<p>Now we need to define two functions, one will open the <b>pytsk</b> file object and other will be used for writing CSV report as shown below &minus;</p>
<pre class="prettyprint notranslate">
def open_file_as_lnk(lnk_file):
   file_size = lnk_file.info.meta.size
   file_content = lnk_file.read_random(0, file_size)
   file_like_obj = StringIO.StringIO(file_content)
   lnk = pylnk.file()
   lnk.open_file_object(file_like_obj)
   return lnk
def write_csv(outfile, fieldnames, data):
   with open(outfile, 'wb') as open_outfile:
      csvfile = csv.DictWriter(open_outfile, fieldnames)
      csvfile.writeheader()
      csvfile.writerows(data)
</pre>
<p>After running the above script, we will get the information from the discovered <b>lnk</b> files in a CSV report &minus;</p>
<h2>Prefetch Files</h2>
<p>Whenever an application is running for the first time from a specific location, Windows creates <b>prefetch files</b>. These are used to speed up the application startup process. The extension for these files is <b>.PF</b> and these are stored in the <b>”\Root\Windows\Prefetch”</b> folder.</p>
<p>Digital forensic experts can reveal the evidence of program execution from a specified location along with the details of the user. Prefetch files are useful artifacts for the examiner because their entry remains even after the program has been deleted or un-installed.</p>
<p>Let us discuss the Python script that will fetch information from Windows prefetch files as given below &minus;</p>
<p>For Python script, install third party modules namely <b>pylnk, pytsk3</b> and <b>unicodecsv</b>. Recall that we have already worked with these libraries in the Python scripts that we have discussed in the previous chapters.</p>
<p>We have to follow steps given below to extract information from <b>prefetch</b> files &minus;</p>
<ul class="list">
<li><p>First, scan for <b>.pf</b> extension files or the prefetch files.</p></li>
<li><p>Now, perform the signature verification to eliminate false positives.</p></li>
<li><p>Next, parse the Windows prefetch file format. This differs with the Windows version. For example, for Windows XP it is 17, for Windows Vista and Windows 7 it is 23, 26 for Windows 8.1 and 30 for Windows 10.</p></li>
<li><p>Lastly, we will write the parsed result in a CSV file.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, import the following Python libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import argparse
from datetime import datetime, timedelta

import os
import pytsk3
import pyewf
import struct
import sys
import unicodecsv as csv
from utility.pytskutil import TSKUtil
</pre>
<p>Now, provide an argument for command-line handler. Here it will accept two arguments, first would be the path to evidence file and second would be the type of evidence file. It also accepts an optional argument for specifying the path to scan for prefetch files &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == "__main__":
   parser = argparse.ArgumentParser('Parsing Prefetch files')
   parser.add_argument("EVIDENCE_FILE", help = "Evidence file path")
   parser.add_argument("TYPE", help = "Type of Evidence",choices = ("raw", "ewf"))
   parser.add_argument("OUTPUT_CSV", help = "Path to write output csv")
   parser.add_argument("-d", help = "Prefetch directory to scan",default = "/WINDOWS/PREFETCH")
   args = parser.parse_args()
   
   if os.path.exists(args.EVIDENCE_FILE) and \
      os.path.isfile(args.EVIDENCE_FILE):
   main(args.EVIDENCE_FILE, args.TYPE, args.OUTPUT_CSV, args.d)
else:
   print("[-] Supplied input file {} does not exist or is not a ""file".format(args.EVIDENCE_FILE))
   sys.exit(1)
</pre>
<p>Now, interpret the evidence file by creating an object of <b>TSKUtil</b> and iterate through the file system to find files ending with <b>.pf</b>. It can be done by defining <b>main()</b> function as follows &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, image_type, output_csv, path):
   tsk_util = TSKUtil(evidence, image_type)
   prefetch_dir = tsk_util.query_directory(path)
   prefetch_files = None
   
   if prefetch_dir is not None:
      prefetch_files = tsk_util.recurse_files(".pf", path=path, logic="endswith")
   
   if prefetch_files is None:
      print("[-] No .pf files found")
      sys.exit(2)
   print("[+] Identified {} potential prefetch files".format(len(prefetch_files)))
   prefetch_data = []
   
   for hit in prefetch_files:
      prefetch_file = hit[2]
      pf_version = check_signature(prefetch_file)
</pre>
<p>Now, define a method that will do the validation of signatures as shown below &minus;</p>
<pre class="prettyprint notranslate">
def check_signature(prefetch_file):
   version, signature = struct.unpack("^&lt;2i", prefetch_file.read_random(0, 8))
   
   if signature == 1094927187:
      return version
   else:
      return None
   
   if pf_version is None:
      continue
   pf_name = hit[0]
   
   if pf_version == 17:
      parsed_data = parse_pf_17(prefetch_file, pf_name)
      parsed_data.append(os.path.join(path, hit[1].lstrip("//")))
      prefetch_data.append(parsed_data)
</pre>
<p>Now, start processing Windows prefetch files. Here we are taking the example of Windows XP prefetch files &minus;</p>
<pre class="prettyprint notranslate">
def parse_pf_17(prefetch_file, pf_name):
   create = convert_unix(prefetch_file.info.meta.crtime)
   modify = convert_unix(prefetch_file.info.meta.mtime)
def convert_unix(ts):
   if int(ts) == 0:
      return ""
   return datetime.utcfromtimestamp(ts)
def convert_filetime(ts):
   if int(ts) == 0:
      return ""
   return datetime(1601, 1, 1) + timedelta(microseconds=ts / 10)
</pre>
<p>Now, extract the data embedded within the prefetched files by using struct as follows &minus;</p>
<pre class="prettyprint notranslate">
pf_size, name, vol_info, vol_entries, vol_size, filetime, \
   count = struct.unpack("&lt;i60s32x3iq16xi",prefetch_file.read_random(12, 136))
name = name.decode("utf-16", "ignore").strip("/x00").split("/x00")[0]

vol_name_offset, vol_name_length, vol_create, \
   vol_serial = struct.unpack("&lt;2iqi",prefetch_file.read_random(vol_info, 20))
   vol_serial = hex(vol_serial).lstrip("0x")
   vol_serial = vol_serial[:4] + "-" + vol_serial[4:]
   vol_name = struct.unpack(
      "&lt;{}s".format(2 * vol_name_length),
      prefetch_file.read_random(vol_info + vol_name_offset,vol_name_length * 2))[0]

vol_name = vol_name.decode("utf-16", "ignore").strip("/x00").split("/x00")[0]
return [
   pf_name, name, pf_size, create,
   modify, convert_filetime(filetime), count, vol_name,
   convert_filetime(vol_create), vol_serial ]
</pre>
<p>As we have provided the prefetch version for Windows XP but what if it will encounter prefetch versions for other Windows. Then it must have to display an error message as follows &minus;</p>
<pre class="prettyprint notranslate">
elif pf_version == 23:
   print("[-] Windows Vista / 7 PF file {} -- unsupported".format(pf_name))
   continue
elif pf_version == 26:
   print("[-] Windows 8 PF file {} -- unsupported".format(pf_name))
   continue
elif pf_version == 30:
   print("[-] Windows 10 PF file {} -- unsupported".format(pf_name))
continue

else:
   print("[-] Signature mismatch - Name: {}\nPath: {}".format(hit[0], hit[1]))
continue
write_output(prefetch_data, output_csv)
</pre>
<p>Now, define the method for writing result into CSV report as follows &minus;</p>
<pre class="prettyprint notranslate">
def write_output(data, output_csv):
   print("[+] Writing csv report")
   with open(output_csv, "wb") as outfile:
      writer = csv.writer(outfile)
      writer.writerow([
         "File Name", "Prefetch Name", "File Size (bytes)",
         "File Create Date (UTC)", "File Modify Date (UTC)",
         "Prefetch Last Execution Date (UTC)",
         "Prefetch Execution Count", "Volume", "Volume Create Date",
         "Volume Serial", "File Path" ])
      writer.writerows(data)
</pre>
<p>After running the above script, we will get the information from prefetch files of Windows XP version into a spreadsheet.</p>
<h1>Important Artifacts In Windows-III</h1>
<p>This chapter will explain about further artifacts that an investigator can obtain during forensic analysis on Windows.</p>
<h2>Event Logs</h2>
<p>Windows event log files, as name –suggests, are special files that stores significant events like when user logs on the computer, when program encounter an error, about system changes, RDP access, application specific events etc. Cyber investigators are always interested in event log information because it provides lots of useful historical information about the access of system. In the following Python script we are going to process both legacy and current Windows event log formats.</p>
<p>For Python script, we need to install third party modules namely <b>pytsk3, pyewf, unicodecsv, pyevt and pyevt</b>x. We can follow the steps given below to extract information from event logs &minus;</p>
<ul class="list">
<li><p>First, search for all the event logs that match the input argument.</p></li>
<li><p>Then, perform file signature verification.</p></li>
<li><p>Now, process each event log found with the appropriate library.</p></li>
<li><p>Lastly, write the output to spreadsheet.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, import the following Python libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import argparse
import unicodecsv as csv
import os
import pytsk3
import pyewf
import pyevt
import pyevtx
import sys
from utility.pytskutil import TSKUtil
</pre>
<p>Now, provide the arguments for command-line handler. Note that here it will accept three arguments – first is the path to evidence file, second is the type of evidence file and third is the name of the event log to process.</p>
<pre class="prettyprint notranslate">
if __name__ == "__main__":
   parser = argparse.ArgumentParser('Information from Event Logs')
   parser.add_argument("EVIDENCE_FILE", help = "Evidence file path")
   parser.add_argument("TYPE", help = "Type of Evidence",choices = ("raw", "ewf"))
   parser.add_argument(
      "LOG_NAME",help = "Event Log Name (SecEvent.Evt, SysEvent.Evt, ""etc.)")
   
   parser.add_argument(
      "-d", help = "Event log directory to scan",default = "/WINDOWS/SYSTEM32/WINEVT")
   
   parser.add_argument(
      "-f", help = "Enable fuzzy search for either evt or"" evtx extension", action = "store_true")
   args = parser.parse_args()
   
   if os.path.exists(args.EVIDENCE_FILE) and \ os.path.isfile(args.EVIDENCE_FILE):
      main(args.EVIDENCE_FILE, args.TYPE, args.LOG_NAME, args.d, args.f)
   else:
      print("[-] Supplied input file {} does not exist or is not a ""file".format(args.EVIDENCE_FILE))
   sys.exit(1)
</pre>
<p>Now, interact with event logs to query the existence of the user supplied path by creating our <b>TSKUtil</b> object. It can be done with the help of <b>main()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, image_type, log, win_event, fuzzy):
   tsk_util = TSKUtil(evidence, image_type)
   event_dir = tsk_util.query_directory(win_event)
   
   if event_dir is not None:
      if fuzzy is True:
         event_log = tsk_util.recurse_files(log, path=win_event)
   else:
      event_log = tsk_util.recurse_files(log, path=win_event, logic="equal")
   
   if event_log is not None:
      event_data = []
      for hit in event_log:
         event_file = hit[2]
         temp_evt = write_file(event_file)
</pre>
<p>Now, we need to perform signature verification followed by defining a method that will write the entire content to the current directory &minus;</p>
<pre class="prettyprint notranslate">
def write_file(event_file):
   with open(event_file.info.name.name, "w") as outfile:
      outfile.write(event_file.read_random(0, event_file.info.meta.size))
   return event_file.info.name.name
      if pyevt.check_file_signature(temp_evt):
         evt_log = pyevt.open(temp_evt)
         print("[+] Identified {} records in {}".format(
            evt_log.number_of_records, temp_evt))
         
         for i, record in enumerate(evt_log.records):
            strings = ""
            for s in record.strings:
               if s is not None:
                  strings += s + "\n"
            event_data.append([
               i, hit[0], record.computer_name,
               record.user_security_identifier,
               record.creation_time, record.written_time,
               record.event_category, record.source_name,
               record.event_identifier, record.event_type,
               strings, "",
               os.path.join(win_event, hit[1].lstrip("//"))
            ])
      elif pyevtx.check_file_signature(temp_evt):
         evtx_log = pyevtx.open(temp_evt)
         print("[+] Identified {} records in {}".format(
            evtx_log.number_of_records, temp_evt))
         for i, record in enumerate(evtx_log.records):
            strings = ""
            for s in record.strings:
			   if s is not None:
               strings += s + "\n"
         event_data.append([
            i, hit[0], record.computer_name,
            record.user_security_identifier, "",
            record.written_time, record.event_level,
            record.source_name, record.event_identifier,
            "", strings, record.xml_string,
            os.path.join(win_event, hit[1].lstrip("//"))
      ])
      else:
         print("[-] {} not a valid event log. Removing temp" file...".format(temp_evt))
         os.remove(temp_evt)
      continue
      write_output(event_data)
   else:
      print("[-] {} Event log not found in {} directory".format(log, win_event))
      sys.exit(3)
else:
   print("[-] Win XP Event Log Directory {} not found".format(win_event))
   sys.exit(2
</pre>
<p>Lastly, define a method for writing the output to spreadsheet as follows &minus;</p>
<pre class="prettyprint notranslate">
def write_output(data):
   output_name = "parsed_event_logs.csv"
   print("[+] Writing {} to current working directory: {}".format(
      output_name, os.getcwd()))
   
   with open(output_name, "wb") as outfile:
      writer = csv.writer(outfile)
      writer.writerow([
         "Index", "File name", "Computer Name", "SID",
         "Event Create Date", "Event Written Date",
         "Event Category/Level", "Event Source", "Event ID",
         "Event Type", "Data", "XML Data", "File Path"
      ])
      writer.writerows(data)
</pre>
<p>Once you successfully run the above script, we will get the information of events log in spreadsheet.</p>
<h2>Internet History</h2>
<p>Internet history is very much useful for forensic analysts; as most cyber-crimes happen over the internet only. Let us see how to extract internet history from the Internet Explorer, as we discussing about Windows forensics, and Internet Explorer comes by default with Windows.</p>
<p>On Internet Explorer, the internet history is saved in <b>index.dat</b> file. Let us look into a Python script, which will extract the information from <b>index.dat</b> file.</p>
<p>We can follow the steps given below to extract information from <b>index.dat</b> files &minus;</p>
<ul class="list">
<li><p>First, search for <b>index.dat</b> files within the system.</p></li>
<li><p>Then, extract the information from that file by iterating through them.</p></li>
<li><p>Now, write all this information to a CSV report.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, import the following Python libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import argparse

from datetime import datetime, timedelta
import os
import pytsk3
import pyewf
import pymsiecf
import sys
import unicodecsv as csv

from utility.pytskutil import TSKUtil
</pre>
<p>Now, provide arguments for command-line handler. Note that here it will accept two arguments – first would be the path to evidence file and second would be the type of evidence file &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == "__main__":
parser = argparse.ArgumentParser('getting information from internet history')
   parser.add_argument("EVIDENCE_FILE", help = "Evidence file path")
   parser.add_argument("TYPE", help = "Type of Evidence",choices = ("raw", "ewf"))
   parser.add_argument("-d", help = "Index.dat directory to scan",default = "/USERS")
   args = parser.parse_args()
   
   if os.path.exists(args.EVIDENCE_FILE) and os.path.isfile(args.EVIDENCE_FILE):
      main(args.EVIDENCE_FILE, args.TYPE, args.d)
   else:
      print("[-] Supplied input file {} does not exist or is not a ""file".format(args.EVIDENCE_FILE))
      sys.exit(1)
</pre>
<p>Now, interpret the evidence file by creating an object of <b>TSKUtil</b> and iterate through the file system to find index.dat files. It can be done by defining the <b>main()</b> function as follows &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, image_type, path):
   tsk_util = TSKUtil(evidence, image_type)
   index_dir = tsk_util.query_directory(path)
   
   if index_dir is not None:
      index_files = tsk_util.recurse_files("index.dat", path = path,logic = "equal")
      
      if index_files is not None:
         print("[+] Identified {} potential index.dat files".format(len(index_files)))
         index_data = []
         
         for hit in index_files:
            index_file = hit[2]
            temp_index = write_file(index_file)
</pre>
<p>Now, define a function with the help of which we can copy the information of index.dat file to the current working directory and later on they can be processed by a third party module &minus;</p>
<pre class="prettyprint notranslate">
def write_file(index_file):
   with open(index_file.info.name.name, "w") as outfile:
   outfile.write(index_file.read_random(0, index_file.info.meta.size))
return index_file.info.name.name
</pre>
<p>Now, use the following code to perform the signature validation with the help of the built-in function namely <b>check_file_signature()</b> &minus;</p>
<pre class="prettyprint notranslate">
if pymsiecf.check_file_signature(temp_index):
   index_dat = pymsiecf.open(temp_index)
   print("[+] Identified {} records in {}".format(
   index_dat.number_of_items, temp_index))

   for i, record in enumerate(index_dat.items):
   try:
      data = record.data
   if data is not None:
      data = data.rstrip("\x00")
   except AttributeError:
   
   if isinstance(record, pymsiecf.redirected):
      index_data.append([
         i, temp_index, "", "", "", "", "",record.location, "", "", record.offset,os.path.join(path, hit[1].lstrip("//"))])
   
   elif isinstance(record, pymsiecf.leak):
      index_data.append([
         i, temp_index, record.filename, "","", "", "", "", "", "", record.offset,os.path.join(path, hit[1].lstrip("//"))])
   continue
   
   index_data.append([
      i, temp_index, record.filename,
      record.type, record.primary_time,
      record.secondary_time,
      record.last_checked_time, record.location,
      record.number_of_hits, data, record.offset,
      os.path.join(path, hit[1].lstrip("//"))
   ])
   else:
      print("[-] {} not a valid index.dat file. Removing "
      "temp file..".format(temp_index))
      os.remove("index.dat")
      continue
      os.remove("index.dat")
      write_output(index_data)
   else:
      print("[-] Index.dat files not found in {} directory".format(path))
   sys.exit(3)
   else:
      print("[-] Directory {} not found".format(win_event))
   sys.exit(2)
</pre>
<p>Now, define a method that will print the output in CSV file, as shown below &minus;</p>
<pre class="prettyprint notranslate">
def write_output(data):
   output_name = "Internet_Indexdat_Summary_Report.csv"
   print("[+] Writing {} with {} parsed index.dat files to current "
   "working directory: {}".format(output_name, len(data),os.getcwd()))
   
   with open(output_name, "wb") as outfile:
      writer = csv.writer(outfile)
      writer.writerow(["Index", "File Name", "Record Name",
      "Record Type", "Primary Date", "Secondary Date",
      "Last Checked Date", "Location", "No. of Hits",
      "Record Data", "Record Offset", "File Path"])
      writer.writerows(data)
</pre>
<p>After running above script we will get the information from index.dat file in CSV file.</p>
<h2>Volume Shadow Copies</h2>
<p>A shadow copy is the technology included in Windows for taking backup copies or snapshots of computer files manually or automatically. It is also called volume snapshot service or volume shadow service(VSS).</p>
<p>With the help of these VSS files, forensic experts can have some historical information about how the system changed over time and what files existed on the computer. Shadow copy technology requires the file system to be NTFS for creating and storing shadow copies.</p>
<p>In this section, we are going to see a Python script, which helps in accessing any volume of shadow copies present in the forensic image.</p>
<p>For Python script we need to install third party modules namely <b>pytsk3, pyewf, unicodecsv, pyvshadow</b> and <b>vss</b>. We can follow the steps given below to extract information from VSS files</p>
<ul class="list">
<li><p>First, access the volume of raw image and identify all the NTFS partitions.</p></li>
<li><p>Then, extract the information from that shadow copies by iterating through them.</p></li>
<li><p>Now, at last we need to create a file listing of data within the snapshots.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, import the following Python libraries &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
import argparse
from datetime import datetime, timedelta

import os
import pytsk3
import pyewf
import pyvshadow
import sys
import unicodecsv as csv

from utility import vss
from utility.pytskutil import TSKUtil
from utility import pytskutil
</pre>
<p>Now, provide arguments for command-line handler. Here it will accept two arguments – first is the path to evidence file and second is the output file.</p>
<pre class="prettyprint notranslate">
if __name__ == "__main__":
   parser = argparse.ArgumentParser('Parsing Shadow Copies')
   parser.add_argument("EVIDENCE_FILE", help = "Evidence file path")
   parser.add_argument("OUTPUT_CSV", help = "Output CSV with VSS file listing")
   args = parser.parse_args()
</pre>
<p>Now, validate the input file path’s existence and also separate the directory from output file.</p>
<pre class="prettyprint notranslate">
directory = os.path.dirname(args.OUTPUT_CSV)
if not os.path.exists(directory) and directory != "":
   os.makedirs(directory)
if os.path.exists(args.EVIDENCE_FILE) and \ os.path.isfile(args.EVIDENCE_FILE):
   main(args.EVIDENCE_FILE, args.OUTPUT_CSV)
else:
   print("[-] Supplied input file {} does not exist or is not a "
   "file".format(args.EVIDENCE_FILE))
   
   sys.exit(1)
</pre>
<p>Now, interact with evidence file’s volume by creating the <b>TSKUtil</b> object. It can be done with the help of <b>main()</b> method as follows &minus;</p>
<pre class="prettyprint notranslate">
def main(evidence, output):
   tsk_util = TSKUtil(evidence, "raw")
   img_vol = tsk_util.return_vol()

if img_vol is not None:
   for part in img_vol:
      if tsk_util.detect_ntfs(img_vol, part):
         print("Exploring NTFS Partition for VSS")
         explore_vss(evidence, part.start * img_vol.info.block_size,output)
      else:
         print("[-] Must be a physical preservation to be compatible ""with this script")
         sys.exit(2)
</pre>
<p>Now, define a method for exploring the parsed volume shadow file as follows &minus;</p>
<pre class="prettyprint notranslate">
def explore_vss(evidence, part_offset, output):
   vss_volume = pyvshadow.volume()
   vss_handle = vss.VShadowVolume(evidence, part_offset)
   vss_count = vss.GetVssStoreCount(evidence, part_offset)
   
   if vss_count > 0:
      vss_volume.open_file_object(vss_handle)
      vss_data = []
      
      for x in range(vss_count):
         print("Gathering data for VSC {} of {}".format(x, vss_count))
         vss_store = vss_volume.get_store(x)
         image = vss.VShadowImgInfo(vss_store)
         vss_data.append(pytskutil.openVSSFS(image, x))
write_csv(vss_data, output)
</pre>
<p>Lastly, define the method for writing the result in spreadsheet as follows &minus;</p>
<pre class="prettyprint notranslate">
def write_csv(data, output):
   if data == []:
      print("[-] No output results to write")
      sys.exit(3)
   print("[+] Writing output to {}".format(output))
   if os.path.exists(output):
      append = True
with open(output, "ab") as csvfile:
      csv_writer = csv.writer(csvfile)
      headers = ["VSS", "File", "File Ext", "File Type", "Create Date",
         "Modify Date", "Change Date", "Size", "File Path"]
      if not append:
         csv_writer.writerow(headers)
      for result_list in data:
         csv_writer.writerows(result_list)
</pre>
<p>Once you successfully run this Python script, we will get the information residing in VSS into a spreadsheet.</p>
<h1>Investigation Of Log Based Artifacts</h1>
<p>Till now, we have seen how to obtain artifacts in Windows using Python. In this chapter, let us learn about investigation of log based artifacts using Python.</p>
<h2>Introduction</h2>
<p>Log-based artifacts are the treasure trove of information that can be very useful for a digital forensic expert. Though we have various monitoring software for collecting the information, the main issue for parsing useful information from them is that we need lot of data.</p>
<h2>Various Log-based Artifacts and Investigating in Python</h2>
<p>In this section, let us discuss various log based artifacts and their investigation in Python &minus;</p>
<h2>Timestamps</h2>
<p>Timestamp conveys the data and time of the activity in the log. It is one of the important elements of any log file. Note that these data and time values can come in various formats.</p>
<p>The Python script shown below will take the raw date-time as input and provides a formatted timestamp as its output.</p>
<p>For this script, we need to follow the following steps &minus;</p>
<ul class="list">
<li><p>First, set up the arguments that will take the raw data value along with source of data and the data type.</p></li>
<li><p>Now, provide a class for providing common interface for data across different date formats.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, import the following Python modules &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from datetime import datetime as dt
from datetime import timedelta
</pre>
<p>Now as usual we need to provide argument for command-line handler. Here it will accept three arguments, first would be the date value to be processed, second would be the source of that date value and third would be its type &minus;</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = ArgumentParser('Timestamp Log-based artifact')
   parser.add_argument("date_value", help="Raw date value to parse")
   parser.add_argument(
      "source", help = "Source format of date",choices = ParseDate.get_supported_formats())
   parser.add_argument(
      "type", help = "Data type of input value",choices = ('number', 'hex'), default = 'int')
   
   args = parser.parse_args()
   date_parser = ParseDate(args.date_value, args.source, args.type)
   date_parser.run()
   print(date_parser.timestamp)
</pre>
<p>Now, we need to define a class which will accept the arguments for date value, date source, and the value type &minus;</p>
<pre class="prettyprint notranslate">
class ParseDate(object):
   def __init__(self, date_value, source, data_type):
      self.date_value = date_value
      self.source = source
      self.data_type = data_type
      self.timestamp = None
</pre>
<p>Now we will define a method that will act like a controller just like the main() method &minus;</p>
<pre class="prettyprint notranslate">
def run(self):
   if self.source == 'unix-epoch':
      self.parse_unix_epoch()
   elif self.source == 'unix-epoch-ms':
      self.parse_unix_epoch(True)
   elif self.source == 'windows-filetime':
      self.parse_windows_filetime()
@classmethod
def get_supported_formats(cls):
   return ['unix-epoch', 'unix-epoch-ms', 'windows-filetime']
</pre>
<p>Now, we need to define two methods which will process Unix epoch time and FILETIME respectively &minus;</p>
<pre class="prettyprint notranslate">
def parse_unix_epoch(self, milliseconds=False):
   if self.data_type == 'hex':
      conv_value = int(self.date_value)
      if milliseconds:
         conv_value = conv_value / 1000.0
   elif self.data_type == 'number':
      conv_value = float(self.date_value)
      if milliseconds:
         conv_value = conv_value / 1000.0
   else:
      print("Unsupported data type '{}' provided".format(self.data_type))
      sys.exit('1')
   ts = dt.fromtimestamp(conv_value)
   self.timestamp = ts.strftime('%Y-%m-%d %H:%M:%S.%f')
def parse_windows_filetime(self):
   if self.data_type == 'hex':
      microseconds = int(self.date_value, 16) / 10.0
   elif self.data_type == 'number':
      microseconds = float(self.date_value) / 10
   else:
      print("Unsupported data type '{}'   provided".format(self.data_type))
      sys.exit('1')
   ts = dt(1601, 1, 1) + timedelta(microseconds=microseconds)
   self.timestamp = ts.strftime('%Y-%m-%d %H:%M:%S.%f')
</pre>
<p>After running the above script, by providing a timestamp we can get the converted value in easy-to-read format.</p>
<h2>Web Server Logs</h2>
<p>From the point of view of digital forensic expert, web server logs are another important artifact because they can get useful user statistics along with information about the user and geographical locations. Following is the Python script that will create a spreadsheet, after processing the web server logs, for easy analysis of the information.</p>
<p>First of all we need to import the following Python modules &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser, FileType

import re
import shlex
import logging
import sys
import csv

logger = logging.getLogger(__file__)
</pre>
<p>Now, we need to define the patterns that will be parsed from the logs &minus;</p>
<pre class="prettyprint notranslate">
iis_log_format = [
   ("date", re.compile(r"\d{4}-\d{2}-\d{2}")),
   ("time", re.compile(r"\d\d:\d\d:\d\d")),
   ("s-ip", re.compile(
      r"((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\.|$)){4}")),
   ("cs-method", re.compile(
      r"(GET)|(POST)|(PUT)|(DELETE)|(OPTIONS)|(HEAD)|(CONNECT)")),
   ("cs-uri-stem", re.compile(r"([A-Za-z0-1/\.-]*)")),
   ("cs-uri-query", re.compile(r"([A-Za-z0-1/\.-]*)")),
   ("s-port", re.compile(r"\d*")),
   ("cs-username", re.compile(r"([A-Za-z0-1/\.-]*)")),
   ("c-ip", re.compile(
      r"((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\.|$)){4}")),
   ("cs(User-Agent)", re.compile(r".*")),
   ("sc-status", re.compile(r"\d*")),
   ("sc-substatus", re.compile(r"\d*")),
   ("sc-win32-status", re.compile(r"\d*")),
   ("time-taken", re.compile(r"\d*"))]
</pre>
<p>Now, provide an argument for command-line handler. Here it will accept two arguments, first would be the IIS log to be processed, second would be the desired CSV file path.</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = ArgumentParser('Parsing Server Based Logs')
   parser.add_argument('iis_log', help = "Path to IIS Log",type = FileType('r'))
   parser.add_argument('csv_report', help = "Path to CSV report")
   parser.add_argument('-l', help = "Path to processing log",default=__name__ + '.log')
   args = parser.parse_args()
   logger.setLevel(logging.DEBUG)
   msg_fmt = logging.Formatter(
      "%(asctime)-15s %(funcName)-10s ""%(levelname)-8s %(message)s")
   
   strhndl = logging.StreamHandler(sys.stdout)
   strhndl.setFormatter(fmt = msg_fmt)
   fhndl = logging.FileHandler(args.log, mode = 'a')
   fhndl.setFormatter(fmt = msg_fmt)
   
   logger.addHandler(strhndl)
   logger.addHandler(fhndl)
   logger.info("Starting IIS Parsing ")
   logger.debug("Supplied arguments: {}".format(", ".join(sys.argv[1:])))
   logger.debug("System " + sys.platform)
   logger.debug("Version " + sys.version)
   main(args.iis_log, args.csv_report, logger)
   iologger.info("IIS Parsing Complete")
</pre>
<p>Now we need to define main() method that will handle the script for bulk log information &minus;</p>
<pre class="prettyprint notranslate">
def main(iis_log, report_file, logger):
   parsed_logs = []

for raw_line in iis_log:
   line = raw_line.strip()
   log_entry = {}

if line.startswith("#") or len(line) == 0:
   continue

if '\"' in line:
   line_iter = shlex.shlex(line_iter)
else:
   line_iter = line.split(" ")
   for count, split_entry in enumerate(line_iter):
      col_name, col_pattern = iis_log_format[count]

      if col_pattern.match(split_entry):
         log_entry[col_name] = split_entry
else:
   logger.error("Unknown column pattern discovered. "
      "Line preserved in full below")
      logger.error("Unparsed Line: {}".format(line))
      parsed_logs.append(log_entry)
      
      logger.info("Parsed {} lines".format(len(parsed_logs)))
      cols = [x[0] for x in iis_log_format]
      
      logger.info("Creating report file: {}".format(report_file))
      write_csv(report_file, cols, parsed_logs)
      logger.info("Report created")
</pre>
<p>Lastly, we need to define a method that will write the output to spreadsheet &minus;</p>
<pre class="prettyprint notranslate">
def write_csv(outfile, fieldnames, data):
   with open(outfile, 'w', newline="") as open_outfile:
      csvfile = csv.DictWriter(open_outfile, fieldnames)
      csvfile.writeheader()
      csvfile.writerows(data)
</pre>
<p>After running the above script we will get the web server based logs in a spreadsheet.</p>
<h2>Scanning Important Files using YARA</h2>
<p>YARA(Yet Another Recursive Algorithm) is a pattern matching utility designed for malware identification and incident response. We will use YARA for scanning the files. In the following Python script, we will use YARA.</p>
<p>We can install YARA with the help of following command &minus;</p>
<pre class="result notranslate">
pip install YARA
</pre>
<p>We can follow the steps given below for using YARA rules to scan files &minus;</p>
<ul class="list">
<li><p>First, set up and compile YARA rules</p></li>
<li><p>Then, scan a single file and then iterate through the directories to process individual files.</p></li>
<li><p>Lastly, we will export the result to CSV.</p></li>
</ul>
<h3>Python Code</h3>
<p>Let us see how to use Python code for this purpose &minus;</p>
<p>First, we need to import the following Python modules &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import print_function
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter

import os
import csv
import yara
</pre>
<p>Next, provide argument for command-line handler. Note that here it will accept two arguments – first is the path to YARA rules, second is the file to be scanned.</p>
<pre class="prettyprint notranslate">
if __name__ == '__main__':
   parser = ArgumentParser('Scanning files by YARA')
   parser.add_argument(
      'yara_rules',help = "Path to Yara rule to scan with. May be file or folder path.")
   parser.add_argument('path_to_scan',help = "Path to file or folder to scan")
   parser.add_argument('--output',help = "Path to output a CSV report of scan results")
   args = parser.parse_args()
   main(args.yara_rules, args.path_to_scan, args.output)
</pre>
<p>Now we will define the main() function that will accept the path to the yara rules and file to be scanned &minus;</p>
<pre class="prettyprint notranslate">
def main(yara_rules, path_to_scan, output):
   if os.path.isdir(yara_rules):
      yrules = yara.compile(yara_rules)
   else:
      yrules = yara.compile(filepath=yara_rules)
   if os.path.isdir(path_to_scan):
      match_info = process_directory(yrules, path_to_scan)
   else:
      match_info = process_file(yrules, path_to_scan)
   columns = ['rule_name', 'hit_value', 'hit_offset', 'file_name',
   'rule_string', 'rule_tag']
   
   if output is None:
      write_stdout(columns, match_info)
   else:
      write_csv(output, columns, match_info)
</pre>
<p>Now, define a method that will iterate through the directory and passes the result to another method for further processing &minus;</p>
<pre class="prettyprint notranslate">
def process_directory(yrules, folder_path):
   match_info = []
   for root, _, files in os.walk(folder_path):
      for entry in files:
         file_entry = os.path.join(root, entry)
         match_info += process_file(yrules, file_entry)
   return match_info
</pre>
<p>Next, define two functions. Note that first we will use <b>match()</b> method to <b>yrules</b> object and another will report that match information to the console if the user does not specify any output file. Observe the code shown below &minus;</p>
<pre class="prettyprint notranslate">
def process_file(yrules, file_path):
   match = yrules.match(file_path)
   match_info = []
   
   for rule_set in match:
      for hit in rule_set.strings:
         match_info.append({
            'file_name': file_path,
            'rule_name': rule_set.rule,
            'rule_tag': ",".join(rule_set.tags),
            'hit_offset': hit[0],
            'rule_string': hit[1],
            'hit_value': hit[2]
         })
   return match_info
def write_stdout(columns, match_info):
   for entry in match_info:
      for col in columns:
         print("{}: {}".format(col, entry[col]))
   print("=" * 30)
</pre>
<p>Lastly, we will define a method that will write the output to CSV file, as shown below &minus;</p>
<pre class="prettyprint notranslate">
def write_csv(outfile, fieldnames, data):
   with open(outfile, 'w', newline="") as open_outfile:
      csvfile = csv.DictWriter(open_outfile, fieldnames)
      csvfile.writeheader()
      csvfile.writerows(data)
</pre>
<p>Once you run the above script successfully, we can provide appropriate arguments at the command-line and can generate a CSV report.</p>
<div class="mui-container-fluid button-borders show">
<div class="pre-btn">
<a href="/python_digital_forensics/python_digital_forensics_investigation_of_log_based_artifacts.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/python_digital_forensics/python_digital_forensics_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="google-bottom-ads">
<div>Advertisements</div>
<script><!--
var width = 580;
var height = 400;
var format = "580x400_as";
if( window.innerWidth < 468 ){
   width = 300;
   height = 250;
   format = "300x250_as";
}
google_ad_client = "pub-7133395778201029";
google_ad_width = width;
google_ad_height = height;
google_ad_format = format;
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
<div class="space-bottom"></div>
</div>
</div>
<!-- Tutorial Content Ends Here -->
<!-- Right Column Starts Here -->
<div class="mui-col-md-2 google-right-ads">
<div class="space-top"></div>
<div class="google-right-ad" style="margin: 0px auto !important;margin-top:5px;">
<script><!--
google_ad_client = "pub-2537027957187252";
google_ad_width = 300;
google_ad_height = 250;
google_ad_format = "300x250_as";
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9012177"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9012177")})</script>
</div>
<div class="space-bottom"></div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9013289"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9013289")})</script>
</div>
<div class="space-bottom" style="margin-bottom:15px;"></div>
</div>
<!-- Right Column Ends Here -->
</div>
</div>
<div class="clear"></div>
<footer id="footer">
<div class="mui--text-center">
<div class="mui--text-caption mui--text-light">
<a href="/index.htm" class="logo"><img class="img-responsive" src="/images/logo-black.png" alt="Tutorials Point" title="Tutorials Point"></a>
</div>
<ul class="mui-list--inline mui--text-body2 mui--text-light">
<li><a href="/about/index.htm"><i class="fal fa-globe"></i> About us</a></li>
<li><a href="/about/about_terms_of_use.htm"><i class="fal fa-asterisk"></i> Terms of use</a></li>
<li><a href="/about/about_privacy.htm#cookies"> <i class="fal fa-shield-check"></i> Cookies Policy</a></li>
<li><a href="/about/faq.htm"><i class="fal fa-question-circle"></i> FAQ's</a></li>
<li><a href="/about/about_helping.htm"><i class="fal fa-hands-helping"></i> Helping</a></li>
<li><a href="/about/contact_us.htm"><i class="fal fa-map-marker-alt"></i> Contact</a></li>
</ul>
<div class="mui--text-caption mui--text-light bottom-copyright-text">&copy; Copyright 2019. All Rights Reserved.</div>
</div>
<div id="privacy-banner">
  <div>
    <p>
      We use cookies to provide and improve our services. By using our site, you consent to our Cookies Policy.
      <a id="banner-accept" href="#">Accept</a>
      <a id="banner-learn" href="/about/about_cookies.htm" target="_blank">Learn more</a>
    </p>
  </div>
</div>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-232293-17"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-232293-6');
</script>
</footer>
</body>
</html>
