<!DOCTYPE html>
<html lang="en-US">
<head>
<title>TensorFlow - Quick Guide - Tutorialspoint</title>
<meta charset="utf-8">
<meta name="description" content="TensorFlow - Quick Guide - TensorFlow is a software library or framework, designed by the Google team to implement machine learning and deep learning concepts in the easiest manner. It co"/>
<meta name="keywords" content="C, C++, Python, Java, HTML, CSS, JavaScript, SQL, PHP, jQuery, XML, DOM, Bootstrap, Tutorials, Articles, Programming, training, learning, quiz, preferences, examples, code"/>
<link rel="canonical" href="https://www.tutorialspoint.com/tensorflow/tensorflow_quick_guide.htm" />
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<meta name="viewport" content="width=device-width,initial-scale=1.0,user-scalable=yes">
<script src="/theme/js/script-min-v2.js?v=3"></script>
<link rel="stylesheet" href="/theme/css/style-min-v2.css?v=6">
<script src="//services.bilsyndication.com/adv1/?d=901" defer="" async=""></script>
<script> var vitag = vitag || {};</script>
<script> vitag.outStreamConfig = { enablePC: false, enableMobile: false };</script>  
<style>
.right-menu .mui-btn {
    background-color:#e35a2b;
}
a.demo {
    background:#e35a2b;
}
li.heading {
    background:#e35a2b;
}
.course-box{background:#e35a2b}
.home-intro-sub p{color:#e35a2b}
</style>
</head>
<body>
<header id="header">
<!-- Top sub-menu Starts Here -->
<div class="mui-appbar mui-container-fulid top-menu">
<div class="mui-container">
<div class="top-menu-item home">
<a href="https://www.tutorialspoint.com/index.htm" target="_blank" title="TutorialsPoint - Home"><i class="fal fa-home"></i> <span>Home</span></a>
</div>
<div class="top-menu-item qa">
<a href="https://www.tutorialspoint.com/about/about_careers.htm" target="_blank" title="Job @ Tutorials Point"><i class="fa fa-suitcase"></i> <span>Jobs</span></a>
</div>
<div class="top-menu-item tools">
<a href="https://www.tutorialspoint.com/online_dev_tools.htm" target="_blank" title="Tools - Online Development and Testing Tools"><i class="fal fa-cogs"></i> <span>Tools</span></a>
</div>
<div class="top-menu-item coding-ground">
<a href="https://www.tutorialspoint.com/codingground.htm" target="_blank" title="Coding Ground - Free Online IDE and Terminal"><i class="fal fa-code"></i> <span>Coding Ground </span></a> 
</div>
<div class="top-menu-item current-affairs">
<a href="https://www.tutorialspoint.com/current_affairs.htm" target="_blank" title="Daily Current Affairs"><i class="fal fa-layer-plus"></i> <span>Current Affairs</span></a>
</div>
<div class="top-menu-item upsc-notes">
<a href="https://www.tutorialspoint.com/upsc_ias_exams.htm" target="_blank" title="UPSC IAS Exams Notes - TutorialsPoint"><i class="fal fa-user-tie"></i> <span>UPSC Notes</span></a>
</div>      
<div class="top-menu-item online-tutoris">
<a href="https://www.tutorialspoint.com/tutor_connect/index.php" target="_blank" title="Top Online Tutors - Tutor Connect"><i class="fal fa-user"></i> <span>Online Tutors</span></a>
</div>
<div class="top-menu-item whiteboard">
<a href="https://www.tutorialspoint.com/whiteboard.htm" target="_blank" title="Free Online Whiteboard"><i class="fal fa-chalkboard"></i> <span>Whiteboard</span></a>
</div>
<div class="top-menu-item net-meeting">
<a href="https://www.tutorialspoint.com/netmeeting.php" target="_blank" title="A free tool for online video conferencing"><i class="fal fa-chalkboard-teacher"></i> <span>Net Meeting</span></a> 
</div>
<div class="top-menu-item articles">
<a href="https://www.tutorix.com" target="_blank" title="Tutorx - The Best Learning App" rel="nofollow"><i class="fal fa-video"></i> <span>Tutorix</span></a> 
</div>        
<div class="social-menu-item">
<a href="https://www.facebook.com/tutorialspointindia" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Facebook"><i class="fab fa-facebook-f"></i></a> 
<a href="https://www.twitter.com/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Twitter"><i class="fab fa-twitter"></i></a>
<a href="https://www.linkedin.com/company/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Linkedin"><i class="fab fa-linkedin-in"></i></a>
<a href="https://www.youtube.com/channel/UCVLbzhxVTiTLiVKeGV7WEBg" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint YouTube"><i class="fab fa-youtube"></i></a>
</div>        
</div>
</div>
<!-- Top sub-menu Ends Here -->
<!-- Top main-menu Starts Here -->
<div class="mui-appbar mui-container-fulid mui--appbar-line-height mui--z1" id="logo-menu">
<div class="mui-container">
<div class="left-menu">
<a href="https://www.tutorialspoint.com/index.htm" title="Tutorialspoint">
<img class="tp-logo" alt="tutorialspoint" src="/tensorflow/images/logo.png">
</a>
<div class="mui-dropdown">
<a class="mui-btn mui-btn--primary categories" data-mui-toggle="dropdown"><i class="fa fa-th-large"></i> 
<span>Categories <span class="mui-caret"></span></span></a>            
<ul class="mui-dropdown__menu cat-menu">
<li>
<ul>
<li><a href="/academic_tutorials.htm"><i class="fa fa-caret-right"></i> Academic Tutorials</a></li>
<li><a href="/big_data_tutorials.htm"><i class="fa fa-caret-right"></i> Big Data &amp; Analytics </a></li>
<li><a href="/computer_programming_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Programming </a></li>
<li><a href="/computer_science_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Science </a></li>
<li><a href="/database_tutorials.htm"><i class="fa fa-caret-right"></i> Databases </a></li>
<li><a href="/devops_tutorials.htm"><i class="fa fa-caret-right"></i> DevOps </a></li>
<li><a href="/digital_marketing_tutorials.htm"><i class="fa fa-caret-right"></i> Digital Marketing </a></li>
<li><a href="/engineering_tutorials.htm"><i class="fa fa-caret-right"></i> Engineering Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> Exams Syllabus </a></li>
<li><a href="/famous_monuments.htm"><i class="fa fa-caret-right"></i> Famous Monuments </a></li>
<li><a href="/gate_exams_tutorials.htm"><i class="fa fa-caret-right"></i> GATE Exams Tutorials</a></li>
<li><a href="/latest_technologies.htm"><i class="fa fa-caret-right"></i> Latest Technologies </a></li>
<li><a href="/machine_learning_tutorials.htm"><i class="fa fa-caret-right"></i> Machine Learning </a></li>
<li><a href="/mainframe_tutorials.htm"><i class="fa fa-caret-right"></i> Mainframe Development </a></li>
<li><a href="/management_tutorials.htm"><i class="fa fa-caret-right"></i> Management Tutorials </a></li>
<li><a href="/maths_tutorials.htm"><i class="fa fa-caret-right"></i> Mathematics Tutorials</a></li>
<li><a href="/microsoft_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Microsoft Technologies </a></li>
<li><a href="/misc_tutorials.htm"><i class="fa fa-caret-right"></i> Misc tutorials </a></li>
<li><a href="/mobile_development_tutorials.htm"><i class="fa fa-caret-right"></i> Mobile Development </a></li>
<li><a href="/java_technology_tutorials.htm"><i class="fa fa-caret-right"></i> Java Technologies </a></li>
<li><a href="/python_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Python Technologies </a></li>
<li><a href="/sap_tutorials.htm"><i class="fa fa-caret-right"></i> SAP Tutorials </a></li>
<li><a href="/scripting_lnaguage_tutorials.htm"><i class="fa fa-caret-right"></i>Programming Scripts </a></li>
<li><a href="/selected_reading.htm"><i class="fa fa-caret-right"></i> Selected Reading </a></li>
<li><a href="/software_quality_tutorials.htm"><i class="fa fa-caret-right"></i> Software Quality </a></li>
<li><a href="/soft_skill_tutorials.htm"><i class="fa fa-caret-right"></i> Soft Skills </a></li>
<li><a href="/telecom_tutorials.htm"><i class="fa fa-caret-right"></i> Telecom Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> UPSC IAS Exams </a></li>
<li><a href="/web_development_tutorials.htm"><i class="fa fa-caret-right"></i> Web Development </a></li>
<li><a href="/sports_tutorials.htm"><i class="fa fa-caret-right"></i> Sports Tutorials </a></li>
<li><a href="/xml_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> XML Technologies </a></li>
<li><a href="/multi_language_tutorials.htm"><i class="fa fa-caret-right"></i> Multi-Language Tutorials</a></li>
<li><a href="/questions_and_answers.htm"><i class="fa fa-caret-right"></i> Interview Questions</a></li>
</ul>
</li>
</ul>
<div class="clear"></div>
</div> 
</div>
<div class="right-menu">
<div class="toc-toggle">
<a href="javascript:void(0);"><i class="fa fa-bars"></i></a>
</div>
<div class="mobile-search-btn">
<a href="https://www.tutorialspoint.com/search.htm"><i class="fal fa-search"></i></a>
</div>
<div class="search-box">
<form method="get" class="" name="searchform" action="https://www.google.com/search" target="_blank" novalidate="">
<input type="hidden" name="sitesearch" value="www.tutorialspoint.com" class="user-valid valid">
<input class="header-search-box" type="text" id="search-string" name="q" placeholder="Search your favorite tutorials..." onfocus="if (this.value == 'Search your favorite tutorials...') {this.value = '';}" onblur="if (this.value == '') {this.value = 'Search your favorite tutorials...';}" autocomplete="off">
<button><i class="fal fa-search"></i></button>
</form>
</div>
<div class="menu-btn library-btn">
<a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a>
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a> 
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/questions/index.php"><i class="fa fa-location-arrow"></i> <span>Q/A</span></a>
</div>
<div class="menu-btn ebooks-btn">
<a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a>
</div>
<div class="mui-dropdown">
<button class="mui-btn mui-btn--primary" data-mui-toggle="dropdown">
<span class="mui-caret"></span>
</button>
<ul class="mui-dropdown__menu">
<li><a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a></li>
<li><a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a></li>
<li><a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a></li>
</ul>
</div>
</div>
</div>
</div>
<!-- Top main-menu Ends Here -->
</header>
<div class="mui-container-fluid content">
<div class="mui-container">
<!-- Tutorial ToC Starts Here -->
<div class="mui-col-md-3 tutorial-toc">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
"HTML-CSS": {
 linebreaks: { automatic: true, width: "container" }          
}              
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>
<div class="mini-logo">
<img src="/tensorflow/images/tensorflow-mini-logo.jpg" alt="TensorFlow Tutorial" />
</div>
<ul class="toc chapters">
<li class="heading">TensorFlow Tutorial</li>
<li><a href="/tensorflow/index.htm">TensorFlow - Home</a></li>
<li><a href="/tensorflow/tensorflow_introduction.htm">TensorFlow - Introduction</a></li>
<li><a href="/tensorflow/tensorflow_installation.htm">TensorFlow - Installation</a></li>
<li><a href="/tensorflow/tensorflow_understanding_artificial_intelligence.htm"> Understanding Artificial Intelligence</a></li>
<li><a href="/tensorflow/tensorflow_mathematical_foundations.htm">Mathematical Foundations</a></li>
<li><a href="/tensorflow/tensorflow_machine_learning_deep_learning.htm">Machine Learning &amp; Deep Learning</a></li>
<li><a href="/tensorflow/tensorflow_basics.htm">TensorFlow - Basics</a></li>
<li><a href="/tensorflow/tensorflow_convolutional_neural_networks.htm">Convolutional Neural Networks</a></li>
<li><a href="/tensorflow/tensorflow_recurrent_neural_networks.htm">Recurrent Neural Networks</a></li>
<li><a href="/tensorflow/tensorflow_tensorboard_visualization.htm">TensorBoard Visualization</a></li>
<li><a href="/tensorflow/tensorflow_word_embedding.htm">TensorFlow - Word Embedding</a></li>
<li><a href="/tensorflow/tensorflow_single_layer_perceptron.htm">Single Layer Perceptron</a></li>
<li><a href="/tensorflow/tensorflow_linear_regression.htm">TensorFlow - Linear Regression</a></li>
<li><a href="/tensorflow/tensorflow_tflearn_its_installation.htm">TFLearn and its installation</a></li>
<li><a href="/tensorflow/tensorflow_cnn_and_rnn_difference.htm">CNN and RNN Difference</a></li>
<li><a href="/tensorflow/tensorflow_keras.htm">TensorFlow - Keras</a></li>
<li><a href="/tensorflow/tensorflow_distributed_computing.htm">TensorFlow - Distributed Computing</a></li>
<li><a href="/tensorflow/tensorflow_exporting.htm">TensorFlow - Exporting</a></li>
<li><a href="/tensorflow/tensorflow_multi_layer_perceptron_learning.htm">Multi-Layer Perceptron Learning</a></li>
<li><a href="/tensorflow/tensorflow_hidden_layers_of_perceptron.htm">Hidden Layers of Perceptron</a></li>
<li><a href="/tensorflow/tensorflow_optimizers.htm">TensorFlow - Optimizers</a></li>
<li><a href="/tensorflow/tensorflow_xor_implementation.htm">TensorFlow - XOR Implementation</a></li>
<li><a href="/tensorflow/tensorflow_gradient_descent_optimization.htm">Gradient Descent Optimization</a></li>
<li><a href="/tensorflow/tensorflow_forming_graphs.htm">TensorFlow - Forming Graphs</a></li>
<li><a href="/tensorflow/image_recognition_using_tensorflow.htm">Image Recognition using TensorFlow</a></li>
<li><a href="/tensorflow/tensorflow_recommendations_for_neural_network_training.htm">Recommendations for Neural Network Training</a></li>
</ul>
<ul class="toc chapters">
<li class="heading">TensorFlow Useful Resources</li>
<li><a href="/tensorflow/tensorflow_quick_guide.htm">TensorFlow - Quick Guide</a></li>
<li><a href="/tensorflow/tensorflow_useful_resources.htm">TensorFlow - Useful Resources</a></li>
<li><a href="/tensorflow/tensorflow_discussion.htm">TensorFlow - Discussion</a></li>
</ul>
<ul class="toc reading">
<li class="sreading">Selected Reading</li>
<li><a target="_top" href="/upsc_ias_exams.htm">UPSC IAS Exams Notes</a></li>
<li><a target="_top" href="/developers_best_practices/index.htm">Developer's Best Practices</a></li>
<li><a target="_top" href="/questions_and_answers.htm">Questions and Answers</a></li>
<li><a target="_top" href="/effective_resume_writing.htm">Effective Resume Writing</a></li>
<li><a target="_top" href="/hr_interview_questions/index.htm">HR Interview Questions</a></li>
<li><a target="_top" href="/computer_glossary.htm">Computer Glossary</a></li>
<li><a target="_top" href="/computer_whoiswho.htm">Who is Who</a></li>
</ul>
</div>
<!-- Tutorial ToC Ends Here -->
<!-- Tutorial Content Starts Here -->
<div class="mui-col-md-6 tutorial-content">
<h1>TensorFlow - Quick Guide</h1>
<hr />
<div class="top-ad-heading">Advertisements</div>
<div style="text-align: center;">
<script><!--
google_ad_client = "pub-7133395778201029";
var width = document.getElementsByClassName("tutorial-content")[0].clientWidth - 40;
google_ad_width = width;
google_ad_height = 150;
google_ad_format = width + "x150_as";
google_ad_type = "image";
google_ad_channel = "";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="mui-container-fluid button-borders">
<div class="pre-btn">
<a href="/tensorflow/tensorflow_recommendations_for_neural_network_training.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/tensorflow/tensorflow_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="clearer"></div>
<h1>TensorFlow - Introduction</h1>
<p>TensorFlow is a software library or framework, designed by the Google team to implement machine learning and deep learning concepts in the easiest manner. It combines the computational algebra of optimization techniques for easy calculation of many mathematical expressions.</p>
<p>The official website of TensorFlow is mentioned below &minus;</p>
<p><a target="_blank" rel="nofollow" href="https://www.tensorflow.org">www.tensorflow.org</a></p>
<img src="/tensorflow/images/tensorflow_homepage.jpg" alt="TensorFlow Home Page" />
<p>Let us now consider the following important features of TensorFlow &minus;</p>
<ul class="list">
<li><p>It includes a feature of that defines, optimizes and calculates mathematical expressions easily with the help of multi-dimensional arrays called tensors.</p></li>
<li><p>It includes a programming support of deep neural networks and machine learning techniques.</p></li>
<li><p>It includes a high scalable feature of computation with various data sets.</p></li>
<li><p>TensorFlow uses GPU computing, automating management. It also includes a unique feature of optimization of same memory and the data used.</p></li>
</ul>
<h2>Why is TensorFlow So Popular?</h2>
<p>TensorFlow is well-documented and includes plenty of machine learning libraries. It offers a few important functionalities and methods for the same.</p>
<p>TensorFlow is also called a “Google” product. It includes a variety of machine learning and deep learning algorithms. TensorFlow can train and run deep neural networks for handwritten digit classification, image recognition, word embedding and creation of various sequence models.</p>
<h1>TensorFlow - Installation</h1>
<p>To install TensorFlow, it is important to have “Python” installed in your system. Python version 3.4+ is considered the best to start with TensorFlow installation.</p>
<p>Consider the following steps to install TensorFlow in Windows operating system.</p>
<p><b>Step 1</b> &minus; Verify the python version being installed.</p>
<img src="/tensorflow/images/python_version_installed.jpg" alt="Python Version Installed" />
<p><b>Step 2</b> &minus; A user can pick up any mechanism to install TensorFlow in the system. We recommend “pip” and “Anaconda”. Pip is a command used for executing and installing modules in Python.</p>
<p>Before we install TensorFlow, we need to install Anaconda framework in our system.</p>
<img src="/tensorflow/images/install_anaconda.jpg" alt="Install Anaconda" />
<p>After successful installation, check in command prompt through “conda” command. The execution of command is displayed below &minus;</p>
<img src="/tensorflow/images/conda_command_execution.jpg" alt="Conda Command Execution" />
<p><b>Step 3</b> &minus; Execute the following command to initialize the installation of TensorFlow &minus;</p>
<pre class="result notranslate">
conda create --name tensorflow python = 3.5
</pre>
<p></p>
<img src="/tensorflow/images/installation_of_tensorflow.jpg" alt="Installation of TensorFlow" />
<p>It downloads the necessary packages needed for TensorFlow setup.</p>
<p><b>Step 4</b> &minus; After successful environmental setup, it is important to activate TensorFlow module.</p>
<pre class="result notranslate">
activate tensorflow
</pre>
<p></p>
<img src="/tensorflow/images/environmental_setup.jpg" alt="Environmental Setup" />
<p><b>Step 5</b> &minus; Use pip to install “Tensorflow” in the system. The command used for installation is mentioned as below &minus;</p>
<pre class="result notranslate">
pip install tensorflow
</pre>
<p>And,</p>
<pre class="result notranslate">
pip install tensorflow-gpu
</pre>
<p></p>
<img src="/tensorflow/images/pip_to_install.jpg" alt="Pip To Install" />
<p></p>
<img src="/tensorflow/images/pip_to_install_tensorflow.jpg" alt="Pip To Install TensorFlow" />
<p>After successful installation, it is important to know the sample program execution of TensorFlow.</p>
<p>Following example helps us understand the basic program creation “Hello World” in TensorFlow.</p>
<img src="/tensorflow/images/hello_world_example.jpg" alt="Hello World Example" />
<p>The code for first program implementation is mentioned below &minus;</p>
<pre class="result notranslate">
&gt;&gt; activate tensorflow
&gt;&gt; python (activating python shell)
&gt;&gt; import tensorflow as tf
&gt;&gt; hello = tf.constant(‘Hello, Tensorflow!’)
&gt;&gt; sess = tf.Session()
&gt;&gt; print(sess.run(hello))
</pre>
<h1>Understanding Artificial Intelligence</h1>
<p>Artificial Intelligence includes the simulation process of human intelligence by machines and special computer systems. The examples of artificial intelligence include learning, reasoning and self-correction. Applications of AI include speech recognition, expert systems, and image recognition and machine vision.</p>
<p>Machine learning is the branch of artificial intelligence, which deals with systems and algorithms that can learn any new data and data patterns.</p>
<p>Let us focus on the Venn diagram mentioned below for understanding machine learning and deep learning concepts.</p>
<img src="/tensorflow/images/venn_diagram.jpg" alt="Venn diagram" />
<p>Machine learning includes a section of machine learning and deep learning is a part of machine learning. The ability of program which follows machine learning concepts is to improve its performance of observed data. The main motive of data transformation is to improve its knowledge in order to achieve better results in the future, provide output closer to the desired output for that particular system. Machine learning includes “pattern recognition” which includes the ability to recognize the patterns in data.</p>
<p>The patterns should be trained to show the output in desirable manner.</p>
<p>Machine learning can be trained in two different ways &minus;</p>
<ul class="list">
<li>Supervised training</li>
<li>Unsupervised training</li>
</ul>
<h2>Supervised Learning</h2>
<p>Supervised learning or supervised training includes a procedure where the training set is given as input to the system wherein, each example is labeled with a desired output value. The training in this type is performed using minimization of a particular loss function, which represents the output error with respect to the desired output system.</p>
<p>After completion of training, the accuracy of each model is measured with respect to disjoint examples from training set, also called the validation set.</p>
<img src="/tensorflow/images/supervised_learning.jpg" alt="Supervised Learning" />
<p>The best example to illustrate “Supervised learning” is with a bunch of photos given with information included in them. Here, the user can train a model to recognize new photos.</p>
<h2>Unsupervised Learning</h2>
<p>In unsupervised learning or unsupervised training, include training examples, which are not labeled by the system to which class they belong. The system looks for the data, which share common characteristics, and changes them based on internal knowledge features.This type of learning algorithms are basically used in clustering problems.</p>
<p>The best example to illustrate “Unsupervised learning” is with a bunch of photos with no information included and user trains model with classification and clustering. This type of training algorithm works with assumptions as no information is given.</p>
<img src="/tensorflow/images/unsupervised_learning.jpg" alt="Unsupervised Learning" />
<h1>TensorFlow - Mathematical Foundations</h1>
<p>It is important to understand mathematical concepts needed for TensorFlow before creating the basic application in TensorFlow. Mathematics is considered as the heart of any machine learning algorithm. It is with the help of core concepts of Mathematics, a solution for specific machine learning algorithm is defined.</p>
<h2>Vector</h2>
<p>An array of numbers, which is either continuous or discrete, is defined as a vector. Machine learning algorithms deal with fixed length vectors for better output generation.</p>
<p>Machine learning algorithms deal with multidimensional data so vectors play a crucial role.</p>
<img src="/tensorflow/images/vector.jpg" alt="Vector" />
<p>The pictorial representation of vector model is as shown below &minus;</p>
<img src="/tensorflow/images/vector_model.jpg" alt="Vector Model" />
<h3>Scalar</h3>
<p>Scalar can be defined as one-dimensional vector. Scalars are those, which include only magnitude and no direction. With scalars, we are only concerned with the magnitude.</p>
<p>Examples of scalar include weight and height parameters of children.</p>
<h3>Matrix</h3>
<p>Matrix can be defined as multi-dimensional arrays, which are arranged in the format of rows and columns. The size of matrix is defined by row length and column length. Following figure shows the representation of any specified matrix.</p>
<img src="/tensorflow/images/multi_dimensional_arrays.jpg" alt="Multi Dimensional Arrays" />
<p>Consider the matrix with “m” rows and “n” columns as mentioned above, the matrix representation will be specified as “m*n matrix” which defined the length of matrix as well.</p>
<h2>Mathematical Computations</h2>
<p>In this section, we will learn about the different Mathematical Computations in TensorFlow.</p>
<h3>Addition of matrices</h3>
<p>Addition of two or more matrices is possible if the matrices are of the same dimension. The addition implies addition of each element as per the given position.</p>
<p>Consider the following example to understand how addition of matrices works &minus;</p>
<p>$$Example:A=\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}B=\begin{bmatrix}5 & 6 \\7 & 8 \end{bmatrix}\:then\:A+B=\begin{bmatrix}1+5 & 2+6 \\3+7 & 4+8 \end{bmatrix}=\begin{bmatrix}6 & 8 \\10 & 12 \end{bmatrix}$$</p>
<h3>Subtraction of matrices</h3>
<p>The subtraction of matrices operates in similar fashion like the addition of two matrices. The user can subtract two matrices provided the dimensions are equal.</p>
<p>$$Example:A-\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}B-\begin{bmatrix}5 & 6 \\7 & 8 \end{bmatrix}\:then\:A-B-\begin{bmatrix}1-5 & 2-6 \\3-7 & 4-8 \end{bmatrix}-\begin{bmatrix}-4 & -4 \\-4 & -4 \end{bmatrix}$$</p>
<h3>Multiplication of matrices</h3>
<p>For two matrices A m*n and B p*q to be multipliable, <b>n</b> should be equal to <b>p</b>. The resulting matrix is &minus;</p>
<p>C m*q</p>
<p>$$A=\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}B=\begin{bmatrix}5 & 6 \\7 & 8 \end{bmatrix}$$</p>
<p>$$c_{11}=\begin{bmatrix}1 & 2 \end{bmatrix}\begin{bmatrix}5 \\7 \end{bmatrix}=1\times5+2\times7=19\:c_{12}=\begin{bmatrix}1 & 2 \end{bmatrix}\begin{bmatrix}6 \\8 \end{bmatrix}=1\times6+2\times8=22$$</p>
<p>$$c_{21}=\begin{bmatrix}3 & 4 \end{bmatrix}\begin{bmatrix}5 \\7 \end{bmatrix}=3\times5+4\times7=43\:c_{22}=\begin{bmatrix}3 & 4 \end{bmatrix}\begin{bmatrix}6 \\8 \end{bmatrix}=3\times6+4\times8=50$$</p>
<p>$$C=\begin{bmatrix}c_{11} & c_{12} \\c_{21} & c_{22} \end{bmatrix}=\begin{bmatrix}19 & 22 \\43 & 50 \end{bmatrix}$$</p>
<h3>Transpose of matrix</h3>
<p>The transpose of a matrix A, m*n is generally represented by AT (transpose) n*m and is obtained by transposing the column vectors as row vectors.</p>
<p>$$Example:A=\begin{bmatrix}1 & 2 \\3 & 4 \end{bmatrix}\:then\:A^{T}\begin{bmatrix}1 & 3 \\2 & 4 \end{bmatrix}$$</p>
<h3>Dot product of vectors</h3>
<p>Any vector of dimension n can be represented as a matrix v = R^n*1.</p>
<p>$$v_{1}=\begin{bmatrix}v_{11} \\v_{12} \\\cdot\\\cdot\\\cdot\\v_{1n}\end{bmatrix}v_{2}=\begin{bmatrix}v_{21} \\v_{22} \\\cdot\\\cdot\\\cdot\\v_{2n}\end{bmatrix}$$</p>
<p>The dot product of two vectors is the sum of the product of corresponding components &minus; Components along the same dimension and can be expressed as</p>
<p>$$v_{1}\cdot v_{2}=v_1^Tv_{2}=v_2^Tv_{1}=v_{11}v_{21}+v_{12}v_{22}+\cdot\cdot+v_{1n}v_{2n}=\displaystyle\sum\limits_{k=1}^n v_{1k}v_{2k}$$</p>
<p>The example of dot product of vectors is mentioned below &minus;</p>
<p>$$Example:v_{1}=\begin{bmatrix}1 \\2 \\3\end{bmatrix}v_{2}=\begin{bmatrix}3 \\5 \\-1\end{bmatrix}v_{1}\cdot v_{2}=v_1^Tv_{2}=1\times3+2\times5-3\times1=10$$</p>
<h1>Machine Learning and Deep Learning</h1>
<p>Artificial Intelligence is one of the most popular trends of recent times. Machine learning and deep learning constitute artificial intelligence. The Venn diagram shown below explains the relationship of machine learning and deep learning &minus;</p>
<img src="/tensorflow/images/venn_diagram.jpg" alt="Venn Diagram" />
<h2>Machine Learning</h2>
<p>Machine learning is the art of science of getting computers to act as per the algorithms designed and programmed. Many researchers think machine learning is the best way to make progress towards human-level AI. Machine learning includes the following types of patterns</p>
<ul class="list">
<li>Supervised learning pattern</li>
<li>Unsupervised learning pattern</li>
</ul>
<h2>Deep Learning</h2>
<p>Deep learning is a subfield of machine learning where concerned algorithms are inspired by the structure and function of the brain called artificial neural networks.</p>
<p>All the value today of deep learning is through supervised learning or learning from labelled data and algorithms.</p>
<p>Each algorithm in deep learning goes through the same process. It includes a hierarchy of nonlinear transformation of input that can be used to generate a statistical model as output.</p>
<p>Consider the following steps that define the Machine Learning process</p>
<ul class="list">
<li>Identifies relevant data sets and prepares them for analysis.</li>
<li>Chooses the type of algorithm to use</li>
<li>Builds an analytical model based on the algorithm used.</li>
<li>Trains the model on test data sets, revising it as needed.</li>
<li>Runs the model to generate test scores.</li>
</ul>
<h2>Difference between Machine Learning and Deep learning</h2>
<p>In this section, we will learn about the difference between Machine Learning and Deep Learning.</p>
<h3>Amount of data</h3>
<p>Machine learning works with large amounts of data. It is useful for small amounts of data too. Deep learning on the other hand works efficiently if the amount of data increases rapidly. The following diagram shows the working of machine learning and deep learning with the amount of data &minus;</p>
<img src="/tensorflow/images/amount_of_data.jpg" alt="Amount of Data" />
<h3>Hardware Dependencies</h3>
<p>Deep learning algorithms are designed to heavily depend on high-end machines unlike the traditional machine learning algorithms. Deep learning algorithms perform a number of matrix multiplication operations, which require a large amount of hardware support.</p>
<h3>Feature Engineering</h3>
<p>Feature engineering is the process of putting domain knowledge into specified features to reduce the complexity of data and make patterns that are visible to learning algorithms it works.</p>
<p>Example &minus; Traditional machine learning patterns focus on pixels and other attributes needed for feature engineering process. Deep learning algorithms focus on high-level features from data. It reduces the task of developing new feature extractor of every new problem.</p>
<h3>Problem Solving Approach</h3>
<p>The traditional machine learning algorithms follow a standard procedure to solve the problem. It breaks the problem into parts, solve each one of them and combine them to get the required result. Deep learning focusses in solving the problem from end to end instead of breaking them into divisions.</p>
<h3>Execution Time</h3>
<p>Execution time is the amount of time required to train an algorithm. Deep learning requires a lot of time to train as it includes a lot of parameters which takes a longer time than usual. Machine learning algorithm comparatively requires less execution time.</p>
<h3>Interpretability</h3>
<p>Interpretability is the major factor for comparison of machine learning and deep learning algorithms. The main reason is that deep learning is still given a second thought before its usage in industry.</p>
<h2>Applications of Machine Learning and Deep Learning</h2>
<p>In this section, we will learn about the different applications of Machine Learning and Deep Learning.</p>
<ul class="list">
<li><p>Computer vision which is used for facial recognition and attendance mark through fingerprints or vehicle identification through number plate.</p></li>
<li><p>Information Retrieval from search engines like text search for image search.</p></li>
<li><p>Automated email marketing with specified target identification.</p></li>
<li><p>Medical diagnosis of cancer tumors or anomaly identification of any chronic disease.</p></li>
<li><p>Natural language processing for applications like photo tagging. The best example to explain this scenario is used in Facebook.</p></li>
<li><p>Online Advertising.</p></li>
</ul>
<h3>Future Trends</h3>
<ul class="list">
<li><p>With the increasing trend of using data science and machine learning in the industry, it will become important for each organization to inculcate machine learning in their businesses.</p></li>
<li><p>Deep learning is gaining more importance than machine learning. Deep learning is proving to be one of the best techniques in state-of-art performance.</p></li>
<li><p>Machine learning and deep learning will prove beneficial in research and academics field.</p></li>
</ul>
<h3>Conclusion</h3>
<p>In this article, we had an overview of machine learning and deep learning with illustrations and differences also focusing on future trends. Many of AI applications utilize machine learning algorithms primarily to drive self-service, increase agent productivity and workflows more reliable. Machine learning and deep learning algorithms include an exciting prospect for many businesses and industry leaders.</p>
<h1>TensorFlow - Basics</h1>
<p>In this chapter, we will learn about the basics of TensorFlow. We will begin by understanding the data structure of tensor.</p>
<h2>Tensor Data Structure</h2>
<p>Tensors are used as the basic data structures in TensorFlow language. Tensors represent the connecting edges in any flow diagram called the Data Flow Graph. Tensors are defined as multidimensional array or list.</p>
<p>Tensors are identified by the following three parameters &minus;</p>
<h3>Rank</h3>
<p>Unit of dimensionality described within tensor is called rank. It identifies the number of dimensions of the tensor. A rank of a tensor can be described as the order or n-dimensions of a tensor defined.</p>
<h3>Shape</h3>
<p>The number of rows and columns together define the shape of Tensor.</p>
<h3>Type</h3>
<p>Type describes the data type assigned to Tensor’s elements.</p>
<p>A user needs to consider the following activities for building a Tensor &minus;</p>
<ul class="list">
<li>Build an n-dimensional array</li>
<li>Convert the n-dimensional array.</li>
</ul>
<img src="/tensorflow/images/tensor_data_structure.jpg" alt="Tensor Data Structure" />
<h2>Various Dimensions of TensorFlow</h2>
<p>TensorFlow includes various dimensions. The dimensions are described in brief below &minus;</p>
<h3>One dimensional Tensor</h3>
<p>One dimensional tensor is a normal array structure which includes one set of values of the same data type.</p>
<p><b>Declaration</b></p>
<pre class="result notranslate">
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; tensor_1d = np.array([1.3, 1, 4.0, 23.99])
&gt;&gt;&gt; print tensor_1d
</pre>
<p>The implementation with the output is shown in the screenshot below &minus;</p>
<img src="/tensorflow/images/one_dimensional_tensor.jpg" alt="One Dimensional Tensor" />
<p>The indexing of elements is same as Python lists. The first element starts with index of 0; to print the values through index, all you need to do is mention the index number.</p>
<pre class="result notranslate">
&gt;&gt;&gt; print tensor_1d[0]
1.3
&gt;&gt;&gt; print tensor_1d[2]
4.0
</pre>
<p></p>
<img src="/tensorflow/images/declaration.jpg" alt="Declaration" />
<h2>Two dimensional Tensors</h2>
<p>Sequence of arrays are used for creating “two dimensional tensors”.</p>
<p>The creation of two-dimensional tensors is described below &minus;</p>
<img src="/tensorflow/images/two_dimensional_tensors.jpg" alt="Two Dimensional Tensors" />
<p>Following is the complete syntax for creating two dimensional arrays &minus;</p>
<pre class="result notranslate">
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; tensor_2d = np.array([(1,2,3,4),(4,5,6,7),(8,9,10,11),(12,13,14,15)])
&gt;&gt;&gt; print(tensor_2d)
[[ 1 2 3 4]
[ 4 5 6 7]
[ 8 9 10 11]
[12 13 14 15]]
&gt;&gt;&gt;
</pre>
<p>The specific elements of two dimensional tensors can be tracked with the help of row number and column number specified as index numbers.</p>
<pre class="result notranslate">
&gt;&gt;&gt; tensor_2d[3][2]
14
</pre>
<p></p>
<img src="/tensorflow/images/two_dimensional_tensors_tracked.jpg" alt="Two Dimensional Tensors Tracked" />
<h2>Tensor Handling and Manipulations</h2>
<p>In this section, we will learn about Tensor Handling and Manipulations.</p>
<p>To begin with, let us consider the following code &minus;</p>
<pre class="prettyprint notranslate">
import tensorflow as tf
import numpy as np

matrix1 = np.array([(2,2,2),(2,2,2),(2,2,2)],dtype = 'int32')
matrix2 = np.array([(1,1,1),(1,1,1),(1,1,1)],dtype = 'int32')

print (matrix1)
print (matrix2)

matrix1 = tf.constant(matrix1)
matrix2 = tf.constant(matrix2)
matrix_product = tf.matmul(matrix1, matrix2)
matrix_sum = tf.add(matrix1,matrix2)
matrix_3 = np.array([(2,7,2),(1,4,2),(9,0,2)],dtype = 'float32')
print (matrix_3)

matrix_det = tf.matrix_determinant(matrix_3)
with tf.Session() as sess:
   result1 = sess.run(matrix_product)
   result2 = sess.run(matrix_sum)
   result3 = sess.run(matrix_det)

print (result1)
print (result2)
print (result3)
</pre>
<p><b>Output</b></p>
<p>The above code will generate the following output &minus;</p>
<img src="/tensorflow/images/tensor_handling_and_manipulations.jpg" alt="Tensor Handling and Manipulations" />
<h3>Explanation</h3>
<p>We have created multidimensional arrays in the above source code. Now, it is important to understand that we created graph and sessions, which manage the Tensors and generate the appropriate output. With the help of graph, we have the output specifying the mathematical calculations between Tensors.</p>
<h1>TensorFlow - Convolutional Neural Networks</h1>
<p>After understanding machine-learning concepts, we can now shift our focus to deep learning concepts. Deep learning is a division of machine learning and is considered as a crucial step taken by researchers in recent decades. The examples of deep learning implementation include applications like image recognition and speech recognition.</p>
<p>Following are the two important types of deep neural networks &minus;</p>
<ul class="list">
<li>Convolutional Neural Networks</li>
<li>Recurrent Neural Networks</li>
</ul>
<p>In this chapter, we will focus on the CNN, Convolutional Neural Networks.</p>
<h2>Convolutional Neural Networks</h2>
<p>Convolutional Neural networks are designed to process data through multiple layers of arrays. This type of neural networks is used in applications like image recognition or face recognition. The primary difference between CNN and any other ordinary neural network is that CNN takes input as a two-dimensional array and operates directly on the images rather than focusing on feature extraction which other neural networks focus on.</p>
<p>The dominant approach of CNN includes solutions for problems of recognition. Top companies like Google and Facebook have invested in research and development towards recognition projects to get activities done with greater speed.</p>
<p>A convolutional neural network uses three basic ideas &minus;</p>
<ul class="list">
<li>Local respective fields</li>
<li>Convolution</li>
<li>Pooling</li>
</ul>
<p>Let us understand these ideas in detail.</p>
<p>CNN utilizes spatial correlations that exist within the input data. Each concurrent layer of a neural network connects some input neurons. This specific region is called local receptive field. Local receptive field focusses on the hidden neurons. The hidden neurons process the input data inside the mentioned field not realizing the changes outside the specific boundary.</p>
<p>Following is a diagram representation of generating local respective fields &minus;</p>
<img src="/tensorflow/images/convolutional_neural_networks.jpg" alt="Convolutional Neural Networks" />
<p>If we observe the above representation, each connection learns a weight of the hidden neuron with an associated connection with movement from one layer to another. Here, individual neurons perform a shift from time to time. This process is called “convolution”.</p>
<p>The mapping of connections from the input layer to the hidden feature map is defined as “shared weights” and bias included is called “shared bias”.</p>
<p>CNN or convolutional neural networks use pooling layers, which are the layers, positioned immediately after CNN declaration. It takes the input from the user as a feature map that comes out of convolutional networks and prepares a condensed feature map. Pooling layers helps in creating layers with neurons of previous layers.</p>
<h2>TensorFlow Implementation of CNN</h2>
<p>In this section, we will learn about the TensorFlow implementation of CNN. The steps,which require the execution and proper dimension of the entire network, are as shown below &minus;</p>
<p><b>Step 1</b> &minus; Include the necessary modules for TensorFlow and the data set modules, which are needed to compute the CNN model.</p>
<pre class="result notranslate">
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
</pre>
<p><b>Step 2</b> &minus; Declare a function called <b>run_cnn()</b>, which includes various parameters and optimization variables with declaration of data placeholders. These optimization variables will declare the training pattern.</p>
<pre class="result notranslate">
def run_cnn():
   mnist = input_data.read_data_sets("MNIST_data/", one_hot = True)
   learning_rate = 0.0001
   epochs = 10
   batch_size = 50
</pre>
<p><b>Step 3</b> &minus; In this step, we will declare the training data placeholders with input parameters - for 28 x 28 pixels = 784. This is the flattened image data that is drawn from <b>mnist.train.nextbatch()</b>.</p>
<p>We can reshape the tensor according to our requirements. The first value (-1) tells function to dynamically shape that dimension based on the amount of data passed to it. The two middle dimensions are set to the image size (i.e. 28 x 28).</p>
<pre class="result notranslate">
x = tf.placeholder(tf.float32, [None, 784])
x_shaped = tf.reshape(x, [-1, 28, 28, 1])
y = tf.placeholder(tf.float32, [None, 10])
</pre>
<p><b>Step 4</b> &minus; Now it is important to create some convolutional layers &minus;</p>
<pre class="result notranslate">
layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name = 'layer1')
layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name = 'layer2')
</pre>
<p><b>Step 5</b> &minus; Let us flatten the output ready for the fully connected output stage - after two layers of stride 2 pooling with the dimensions of 28 x 28, to dimension of 14 x 14 or minimum 7 x 7 x,y co-ordinates, but with 64 output channels. To create the fully connected with "dense" layer, the new shape needs to be [-1, 7 x 7 x 64]. We can set up some weights and bias values for this layer, then activate with ReLU.</p>
<pre class="prettyprint notranslate">
flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])

wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev = 0.03), name = 'wd1')
bd1 = tf.Variable(tf.truncated_normal([1000], stddev = 0.01), name = 'bd1')

dense_layer1 = tf.matmul(flattened, wd1) + bd1
dense_layer1 = tf.nn.relu(dense_layer1)
</pre>
<p><b>Step 6</b> &minus; Another layer with specific softmax activations with the required optimizer defines the accuracy assessment, which makes the setup of initialization operator.</p>
<pre class="prettyprint notranslate">
wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev = 0.03), name = 'wd2')
bd2 = tf.Variable(tf.truncated_normal([10], stddev = 0.01), name = 'bd2')

dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2
y_ = tf.nn.softmax(dense_layer2)

cross_entropy = tf.reduce_mean(
   tf.nn.softmax_cross_entropy_with_logits(logits = dense_layer2, labels = y))

optimiser = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cross_entropy)

correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

init_op = tf.global_variables_initializer()
</pre>
<p><b>Step 7</b> &minus; We should set up recording variables. This adds up a summary to store the accuracy of data.</p>
<pre class="prettyprint notranslate">
tf.summary.scalar('accuracy', accuracy)
   merged = tf.summary.merge_all()
   writer = tf.summary.FileWriter('E:\TensorFlowProject')
   
   with tf.Session() as sess:
      sess.run(init_op)
      total_batch = int(len(mnist.train.labels) / batch_size)
      
      for epoch in range(epochs):
         avg_cost = 0
      for i in range(total_batch):
         batch_x, batch_y = mnist.train.next_batch(batch_size = batch_size)
            _, c = sess.run([optimiser, cross_entropy], feed_dict = {
            x:batch_x, y: batch_y})
            avg_cost += c / total_batch
         test_acc = sess.run(accuracy, feed_dict = {x: mnist.test.images, y:
            mnist.test.labels})
            summary = sess.run(merged, feed_dict = {x: mnist.test.images, y:
            mnist.test.labels})
         writer.add_summary(summary, epoch)

   print("\nTraining complete!")
   writer.add_graph(sess.graph)
   print(sess.run(accuracy, feed_dict = {x: mnist.test.images, y:
      mnist.test.labels}))

def create_new_conv_layer(
   input_data, num_input_channels, num_filters,filter_shape, pool_shape, name):

   conv_filt_shape = [
      filter_shape[0], filter_shape[1], num_input_channels, num_filters]

   weights = tf.Variable(
      tf.truncated_normal(conv_filt_shape, stddev = 0.03), name = name+'_W')
   bias = tf.Variable(tf.truncated_normal([num_filters]), name = name+'_b')

#Out layer defines the output
   out_layer =
      tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding = 'SAME')

   out_layer += bias
   out_layer = tf.nn.relu(out_layer)
   ksize = [1, pool_shape[0], pool_shape[1], 1]
   strides = [1, 2, 2, 1]
   out_layer = tf.nn.max_pool(
      out_layer, ksize = ksize, strides = strides, padding = 'SAME')

   return out_layer

if __name__ == "__main__":
run_cnn()
</pre>
<p>Following is the output generated by the above code &minus;</p>
<pre class="result notranslate">
See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

2018-09-19 17:22:58.802268: I
T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140]
Your CPU supports instructions that this TensorFlow binary was not compiled to
use: AVX2

2018-09-19 17:25:41.522845: W
T:\src\github\tensorflow\tensorflow\core\framework\allocator.cc:101] Allocation
of 1003520000 exceeds 10% of system memory.

2018-09-19 17:25:44.630941: W
T:\src\github\tensorflow\tensorflow\core\framework\allocator.cc:101] Allocation
of 501760000 exceeds 10% of system memory.

Epoch: 1 cost = 0.676 test accuracy: 0.940

2018-09-19 17:26:51.987554: W
T:\src\github\tensorflow\tensorflow\core\framework\allocator.cc:101] Allocation
of 1003520000 exceeds 10% of system memory.
</pre>
<h1>TensorFlow - Recurrent Neural Networks</h1>
<p>Recurrent neural networks is a type of deep learning-oriented algorithm, which follows a sequential approach. In neural networks, we always assume that each input and output is independent of all other layers. These type of neural networks are called recurrent because they perform mathematical computations in sequential manner.</p>
<p>Consider the following steps to train a recurrent neural network &minus;</p>
<p><b>Step 1</b> &minus; Input a specific example from dataset.</p>
<p><b>Step 2</b> &minus; Network will take an example and compute some calculations using randomly initialized variables.</p>
<p><b>Step 3</b> &minus; A predicted result is then computed.</p>
<p><b>Step 4</b> &minus; The comparison of actual result generated with the expected value will produce an error.</p>
<p><b>Step 5</b> &minus; To trace the error, it is propagated through same path where the variables are also adjusted.</p>
<p><b>Step 6</b> &minus; The steps from 1 to 5 are repeated until we are confident that the variables declared to get the output are defined properly.</p>
<p><b>Step 7</b> &minus; A systematic prediction is made by applying these variables to get new unseen input.</p>
<p>The schematic approach of representing recurrent neural networks is described below &minus;</p>
<img src="/tensorflow/images/recurrent_neural_networks.jpg" alt="Recurrent Neural Networks" />
<h2>Recurrent Neural Network Implementation with TensorFlow</h2>
<p>In this section, we will learn how to implement recurrent neural network with TensorFlow.</p>
<p><b>Step 1</b> &minus; TensorFlow includes various libraries for specific implementation of the recurrent neural network module.</p>
<pre class="prettyprint notranslate">
#Import necessary modules
from __future__ import print_function

import tensorflow as tf
from tensorflow.contrib import rnn
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot = True)
</pre>
<p>As mentioned above, the libraries help in defining the input data, which forms the primary part of recurrent neural network implementation.</p>
<p><b>Step 2</b> &minus; Our primary motive is to classify the images using a recurrent neural network, where we consider every image row as a sequence of pixels. MNIST image shape is specifically defined as 28*28 px. Now we will handle 28 sequences of 28 steps for each sample that is mentioned. We will define the input parameters to get the sequential pattern done.</p>
<pre class="prettyprint notranslate">
n_input = 28 # MNIST data input with img shape 28*28
n_steps = 28
n_hidden = 128
n_classes = 10

# tf Graph input
x = tf.placeholder("float", [None, n_steps, n_input])
y = tf.placeholder("float", [None, n_classes]
weights = {
   'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))
}
biases = {
   'out': tf.Variable(tf.random_normal([n_classes]))
}
</pre>
<p><b>Step 3</b> &minus; Compute the results using a defined function in RNN to get the best results. Here, each data shape is compared with current input shape and the results are computed to maintain the accuracy rate.</p>
<pre class="prettyprint notranslate">
def RNN(x, weights, biases):
   x = tf.unstack(x, n_steps, 1)

   # Define a lstm cell with tensorflow
   lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)

   # Get lstm cell output
   outputs, states = rnn.static_rnn(lstm_cell, x, dtype = tf.float32)

   # Linear activation, using rnn inner loop last output
   return tf.matmul(outputs[-1], weights['out']) + biases['out']

pred = RNN(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))
optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.global_variables_initializer()
</pre>
<p><b>Step 4</b> &minus; In this step, we will launch the graph to get the computational results. This also helps in calculating the accuracy for test results.</p>
<pre class="prettyprint notranslate">
with tf.Session() as sess:
   sess.run(init)
   step = 1
   # Keep training until reach max iterations
   
   while step * batch_size &lt; training_iters:
      batch_x, batch_y = mnist.train.next_batch(batch_size)
      batch_x = batch_x.reshape((batch_size, n_steps, n_input))
      sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
      
      if step % display_step == 0:
         # Calculate batch accuracy
         acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})
         
         # Calculate batch loss
         loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})
         
         print("Iter " + str(step*batch_size) + ", Minibatch Loss= " + \
            "{:.6f}".format(loss) + ", Training Accuracy= " + \
            "{:.5f}".format(acc))
      step += 1
   print("Optimization Finished!")
      test_len = 128
   test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))
   
   test_label = mnist.test.labels[:test_len]
   print("Testing Accuracy:", \
      sess.run(accuracy, feed_dict={x: test_data, y: test_label}))
</pre>
<p>The screenshots below show the output generated &minus;</p>
<img src="/tensorflow/images/recurrent_neural_networks_implementation_output.jpg" alt="Recurrent Neural Networks Implementation Output" />
<p></p>
<img src="/tensorflow/images/recurrent_neural_networks_implementation_output_transflow.jpg" alt="Recurrent Neural Networks Implementation Output TransFlow" />
<h1>TensorFlow - TensorBoard Visualization</h1>
<p>TensorFlow includes a visualization tool, which is called the TensorBoard. It is used for analyzing Data Flow Graph and also used to understand machine-learning models. The important feature of TensorBoard includes a view of different types of statistics about the parameters and details of any graph in vertical alignment.</p>
<p>Deep neural network includes up to 36,000 nodes. TensorBoard helps in collapsing these nodes in high-level blocks and highlighting the identical structures. This allows better analysis of graph focusing on the primary sections of the computation graph. The TensorBoard visualization is said to be very interactive where a user can pan, zoom and expand the nodes to display the details.</p>
<p>The following schematic diagram representation shows the complete working of TensorBoard visualization &minus;</p>
<img src="/tensorflow/images/tensorboard_visualization.jpg" alt="TensorBoard visualization" />
<p>The algorithms collapse nodes into high-level blocks and highlight the specific groups with identical structures, which separate high-degree nodes. The TensorBoard thus created is useful and is treated equally important for tuning a machine learning model. This visualization tool is designed for the configuration log file with summary information and details that need to be displayed.</p>
<p>Let us focus on the demo example of TensorBoard visualization with the help of the following code &minus;</p>
<pre class="prettyprint notranslate">
import tensorflow as tf 

# Constants creation for TensorBoard visualization 
a = tf.constant(10,name = "a") 
b = tf.constant(90,name = "b") 
y = tf.Variable(a+b*2,name = 'y') 
model = tf.initialize_all_variables() #Creation of model 

with tf.Session() as session: 
   merged = tf.merge_all_summaries() 
   writer = tf.train.SummaryWriter("/tmp/tensorflowlogs",session.graph) 
   session.run(model) 
   print(session.run(y))
</pre>
<p>The following table shows the various symbols of TensorBoard visualization used for the node representation &minus;</p>
<img src="/tensorflow/images/node_representation.jpg" alt="Node Representation" />
<h1>TensorFlow - Word Embedding</h1>
<p>Word embedding is the concept of mapping from discrete objects such as words to vectors and real numbers. It is important for input for machine learning. The concept includes standard functions, which effectively transform discrete input objects to useful vectors.</p>
<p>The sample illustration of input of word embedding is as shown below &minus;</p>
<pre class="result notranslate">
blue: (0.01359, 0.00075997, 0.24608, ..., -0.2524, 1.0048, 0.06259)
blues: (0.01396, 0.11887, -0.48963, ..., 0.033483, -0.10007, 0.1158)
orange: (-0.24776, -0.12359, 0.20986, ..., 0.079717, 0.23865, -0.014213)
oranges: (-0.35609, 0.21854, 0.080944, ..., -0.35413, 0.38511, -0.070976)
</pre>
<h2>Word2vec</h2>
<p>Word2vec is the most common approach used for unsupervised word embedding technique. It trains the model in such a way that a given input word predicts the word’s context by using skip-grams.</p>
<p>TensorFlow enables many ways to implement this kind of model with increasing levels of sophistication and optimization and using multithreading concepts and higher-level abstractions.</p>
<pre class="prettyprint notranslate">
import os 
import math 
import numpy as np 
import tensorflow as tf 

from tensorflow.contrib.tensorboard.plugins import projector 
batch_size = 64 
embedding_dimension = 5 
negative_samples = 8 
LOG_DIR = "logs/word2vec_intro" 

digit_to_word_map = {
   1: "One", 
   2: "Two", 
   3: "Three", 
   4: "Four", 
   5: "Five", 
   6: "Six", 
   7: "Seven", 
   8: "Eight", 
   9: "Nine"} 
sentences = [] 

# Create two kinds of sentences - sequences of odd and even digits. 
   for i in range(10000): 
   rand_odd_ints = np.random.choice(range(1, 10, 2), 3) 
      sentences.append(" ".join([digit_to_word_map[r] for r in rand_odd_ints])) 
   rand_even_ints = np.random.choice(range(2, 10, 2), 3) 
      sentences.append(" ".join([digit_to_word_map[r] for r in rand_even_ints])) 
   
# Map words to indices
word2index_map = {} 
index = 0 

for sent in sentences: 
   for word in sent.lower().split(): 
   
   if word not in word2index_map: 
      word2index_map[word] = index 
      index += 1 
index2word_map = {index: word for word, index in word2index_map.items()} 

vocabulary_size = len(index2word_map) 

# Generate skip-gram pairs 
skip_gram_pairs = [] 

for sent in sentences: 
   tokenized_sent = sent.lower().split() 
   
   for i in range(1, len(tokenized_sent)-1):        
      word_context_pair = [[word2index_map[tokenized_sent[i-1]], 
         word2index_map[tokenized_sent[i+1]]], word2index_map[tokenized_sent[i]]] 
      
      skip_gram_pairs.append([word_context_pair[1], word_context_pair[0][0]]) 
      skip_gram_pairs.append([word_context_pair[1], word_context_pair[0][1]]) 

def get_skipgram_batch(batch_size): 
   instance_indices = list(range(len(skip_gram_pairs))) 
      np.random.shuffle(instance_indices)
   batch = instance_indices[:batch_size] 
   x = [skip_gram_pairs[i][0] for i in batch] 
   y = [[skip_gram_pairs[i][1]] for i in batch] 
   return x, y 
   
# batch example 
x_batch, y_batch = get_skipgram_batch(8) 
x_batch 
y_batch 
[index2word_map[word] for word in x_batch] [index2word_map[word[0]] for word in y_batch] 

# Input data, labels train_inputs = tf.placeholder(tf.int32, shape = [batch_size]) 
   train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1]) 

# Embedding lookup table currently only implemented in CPU with 
   tf.name_scope("embeddings"): 
   embeddings = tf.Variable(    
      tf.random_uniform([vocabulary_size, embedding_dimension], -1.0, 1.0), 
         name = 'embedding') 
   # This is essentialy a lookup table 
   embed = tf.nn.embedding_lookup(embeddings, train_inputs) 
   
# Create variables for the NCE loss
nce_weights = tf.Variable(     
   tf.truncated_normal([vocabulary_size, embedding_dimension], stddev = 1.0 / 
      math.sqrt(embedding_dimension))) 
   
nce_biases = tf.Variable(tf.zeros([vocabulary_size])) 

loss = tf.reduce_mean(     
   tf.nn.nce_loss(weights = nce_weights, biases = nce_biases, inputs = embed, 
   labels = train_labels,num_sampled = negative_samples, 
   num_classes = vocabulary_size)) tf.summary.scalar("NCE_loss", loss) 
   
# Learning rate decay 
global_step = tf.Variable(0, trainable = False) 
   learningRate = tf.train.exponential_decay(learning_rate = 0.1, 
   global_step = global_step, decay_steps = 1000, decay_rate = 0.95, staircase = True) 

train_step = tf.train.GradientDescentOptimizer(learningRate).minimize(loss) 
   merged = tf.summary.merge_all() 
with tf.Session() as sess: 
   train_writer = tf.summary.FileWriter(LOG_DIR,    
      graph = tf.get_default_graph()) 
   saver = tf.train.Saver() 
   
   with open(os.path.join(LOG_DIR, 'metadata.tsv'), "w") as metadata: 
      metadata.write('Name\tClass\n') for k, v in index2word_map.items(): 
      metadata.write('%s\t%d\n' % (v, k)) 
   
   config = projector.ProjectorConfig() 
   embedding = config.embeddings.add() embedding.tensor_name = embeddings.name 
   
   # Link this tensor to its metadata file (e.g. labels). 
   embedding.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv') 
      projector.visualize_embeddings(train_writer, config) 
   
   tf.global_variables_initializer().run() 
   
   for step in range(1000): 
      x_batch, y_batch = get_skipgram_batch(batch_size) summary, _ = sess.run(
         [merged, train_step], feed_dict = {train_inputs: x_batch, train_labels: y_batch})
      train_writer.add_summary(summary, step)
      
      if step % 100 == 0:
         saver.save(sess, os.path.join(LOG_DIR, "w2v_model.ckpt"), step)
         loss_value = sess.run(loss, feed_dict = {
            train_inputs: x_batch, train_labels: y_batch})
         print("Loss at %d: %.5f" % (step, loss_value))

   # Normalize embeddings before using
   norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims = True))
   normalized_embeddings = embeddings /
      norm normalized_embeddings_matrix = sess.run(normalized_embeddings)
   
ref_word = normalized_embeddings_matrix[word2index_map["one"]]

cosine_dists = np.dot(normalized_embeddings_matrix, ref_word)
ff = np.argsort(cosine_dists)[::-1][1:10] for f in ff: print(index2word_map[f])
print(cosine_dists[f])
</pre>
<h3>Output</h3>
<p>The above code generates the following output &minus;</p>
<img src="/tensorflow/images/word2vec.jpg" alt="Word2vec" />
<h1>TensorFlow - Single Layer Perceptron</h1>
<p>For understanding single layer perceptron, it is important to understand Artificial Neural Networks (ANN). Artificial neural networks is the information processing system the mechanism of which is inspired with the functionality of biological neural circuits. An artificial neural network possesses many processing units connected to each other. Following is the schematic representation of artificial neural network &minus;</p>
<img src="/tensorflow/images/schematic_representation.jpg" alt="Schematic Representation" />
<p>The diagram shows that the hidden units communicate with the external layer. While the input and output units communicate only through the hidden layer of the network.</p>
<p>The pattern of connection with nodes, the total number of layers and level of nodes between inputs and outputs with the number of neurons per layer define the architecture of a neural network.</p>
<p>There are two types of architecture. These types focus on the functionality artificial neural networks as follows &minus;</p>
<ul class="list">
<li>Single Layer Perceptron</li>
<li>Multi-Layer Perceptron</li>
</ul>
<h2>Single Layer Perceptron</h2>
<p>Single layer perceptron is the first proposed neural model created. The content of the local memory of the neuron consists of a vector of weights. The computation of a single layer perceptron is performed over the calculation of sum of the input vector each with the value multiplied by corresponding element of vector of the weights. The value which is displayed in the output will be the input of an activation function.</p>
<img src="/tensorflow/images/single_layer_perceptron.jpg" alt="Single Layer Perceptron" />
<p>Let us focus on the implementation of single layer perceptron for an image classification problem using TensorFlow. The best example to illustrate the single layer perceptron is through representation of “Logistic Regression”.</p>
<img src="/tensorflow/images/logistic_regression.jpg" alt="Logistic Regression" />
<p>Now, let us consider the following basic steps of training logistic regression &minus;</p>
<ul class="list">
<li><p>The weights are initialized with random values at the beginning of the training.</p></li>
<li><p>For each element of the training set, the error is calculated with the difference between desired output and the actual output. The error calculated is used to adjust the weights.</p></li>
<li><p>The process is repeated until the error made on the entire training set is not less than the specified threshold, until the maximum number of iterations is reached.</p></li>
</ul>
<p>The complete code for evaluation of logistic regression is mentioned below &minus;</p>
<pre class="prettyprint notranslate">
# Import MINST data 
from tensorflow.examples.tutorials.mnist import input_data 
mnist = input_data.read_data_sets("/tmp/data/", one_hot = True) 

import tensorflow as tf 
import matplotlib.pyplot as plt 

# Parameters 
learning_rate = 0.01 
training_epochs = 25 
batch_size = 100 
display_step = 1 

# tf Graph Input 
x = tf.placeholder("float", [None, 784]) # mnist data image of shape 28*28 = 784 
y = tf.placeholder("float", [None, 10]) # 0-9 digits recognition => 10 classes 

# Create model 
# Set model weights 
W = tf.Variable(tf.zeros([784, 10])) 
b = tf.Variable(tf.zeros([10])) 

# Construct model 
activation = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax 

# Minimize error using cross entropy 
cross_entropy = y*tf.log(activation) 
cost = tf.reduce_mean\ (-tf.reduce_sum\ (cross_entropy,reduction_indices = 1)) 

optimizer = tf.train.\ GradientDescentOptimizer(learning_rate).minimize(cost) 

#Plot settings 
avg_set = [] 
epoch_set = [] 

# Initializing the variables init = tf.initialize_all_variables()
# Launch the graph 
with tf.Session() as sess:
   sess.run(init)
   
   # Training cycle
   for epoch in range(training_epochs):
      avg_cost = 0.
      total_batch = int(mnist.train.num_examples/batch_size)
      
      # Loop over all batches
      for i in range(total_batch):
         batch_xs, batch_ys = \ mnist.train.next_batch(batch_size)
         # Fit training using batch data sess.run(optimizer, \ feed_dict = {
            x: batch_xs, y: batch_ys}) 
         # Compute average loss avg_cost += sess.run(cost, \ feed_dict = {
            x: batch_xs, \ y: batch_ys})/total_batch
      # Display logs per epoch step
      if epoch % display_step == 0:
         print ("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))
            avg_set.append(avg_cost) epoch_set.append(epoch+1)
   print ("Training phase finished")
    
   plt.plot(epoch_set,avg_set, 'o', label = 'Logistic Regression Training phase') 
   plt.ylabel('cost') 
   plt.xlabel('epoch') 
   plt.legend() 
   plt.show() 
    
   # Test model 
   correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1)) 
   
   # Calculate accuracy 
   accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) print 
      ("Model accuracy:", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))
</pre>
<h3>Output</h3>
<p>The above code generates the following output &minus;</p>
<img src="/tensorflow/images/evaluation_of_logistic_regression.jpg" alt="Evaluation of Logistic Regression" />
<p>The logistic regression is considered as a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal or independent variables.</p>
<img src="/tensorflow/images/independent_variables.jpg" alt="Independent Variables" />
<h1>TensorFlow - Linear Regression</h1>
<p>In this chapter, we will focus on the basic example of linear regression implementation using TensorFlow. Logistic regression or linear regression is a supervised machine learning approach for the classification of order discrete categories. Our goal in this chapter is to build a model by which a user can predict the relationship between predictor variables and one or more independent variables.</p>
<p>The relationship between these two variables is cons &minus;idered linear. If y is the dependent variable and x is considered as the independent variable, then the linear regression relationship of two variables will look like the following equation &minus;</p>
<pre class="result notranslate">
Y = Ax+b
</pre>
<p>We will design an algorithm for linear regression. This will allow us to understand the following two important concepts &minus;</p>
<ul class="list">
<li>Cost Function</li>
<li>Gradient descent algorithms</li>
</ul>
<p>The schematic representation of linear regression is mentioned below &minus;</p>
<img src="/tensorflow/images/schematic_representation_linear_regression.jpg" alt="Schematic Representation Linear Regression" />
<p>The graphical view of the equation of linear regression is mentioned below &minus;</p>
<img src="/tensorflow/images/graphical_schematic_representation.jpg" alt="Graphical Schematic Representation" />
<h2>Steps to design an algorithm for linear regression</h2>
<p>We will now learn about the steps that help in designing an algorithm for linear regression.</p>
<h3>Step 1</h3>
<p>It is important to import the necessary modules for plotting the linear regression module. We start importing the Python library NumPy and Matplotlib.</p>
<pre class="result notranslate">
import numpy as np 
import matplotlib.pyplot as plt
</pre>
<h3>Step 2</h3>
<p>Define the number of coefficients necessary for logistic regression.</p>
<pre class="result notranslate">
number_of_points = 500 
x_point = [] 
y_point = [] 
a = 0.22 
b = 0.78
</pre>
<h3>Step 3</h3>
<p>Iterate the variables for generating 300 random points around the regression equation &minus;</p>
<p>Y = 0.22x+0.78</p>
<pre class="result notranslate">
for i in range(number_of_points): 
   x = np.random.normal(0.0,0.5) 
   y = a*x + b +np.random.normal(0.0,0.1) x_point.append([x]) 
   y_point.append([y])
</pre>
<h3>Step 4</h3>
<p>View the generated points using Matplotlib.</p>
<pre class="result notranslate">
fplt.plot(x_point,y_point, 'o', label = 'Input Data') plt.legend() plt.show()
</pre>
<p>The complete code for logistic regression is as follows &minus;</p>
<pre class="prettyprint notranslate">
import numpy as np 
import matplotlib.pyplot as plt 

number_of_points = 500 
x_point = [] 
y_point = [] 
a = 0.22 
b = 0.78 

for i in range(number_of_points): 
   x = np.random.normal(0.0,0.5) 
   y = a*x + b +np.random.normal(0.0,0.1) x_point.append([x]) 
   y_point.append([y]) 
   
plt.plot(x_point,y_point, 'o', label = 'Input Data') plt.legend() 
plt.show()
</pre>
<p>The number of points which is taken as input is considered as input data.</p>
<img src="/tensorflow/images/code_for_logistic_regression.jpg" alt="Code For Logistic Regression" />
<h1>TensorFlow - TFLearn And Its Installation</h1>
<p>TFLearn can be defined as a modular and transparent deep learning aspect used in TensorFlow framework. The main motive of TFLearn is to provide a higher level API to TensorFlow for facilitating and showing up new experiments.</p>
<p>Consider the following important features of TFLearn &minus;</p>
<ul class="list">
<li><p>TFLearn is easy to use and understand.</p></li>
<li><p>It includes easy concepts to build highly modular network layers, optimizers and various metrics embedded within them.</p></li>
<li><p>It includes full transparency with TensorFlow work system.</p></li>
<li><p>It includes powerful helper functions to train the built in tensors which accept multiple inputs, outputs and optimizers.</p></li>
<li><p>It includes easy and beautiful graph visualization.</p></li>
<li><p>The graph visualization includes various details of weights, gradients and activations.</p></li>
</ul>
<p>Install TFLearn by executing the following command &minus;</p>
<pre class="result notranslate">
pip install tflearn
</pre>
<p>Upon execution of the above code, the following output will be generated &minus;</p>
<img src="/tensorflow/images/install_tflearn.jpg" alt="Install TFLearn" />
<p>The following illustration shows the implementation of TFLearn with Random Forest classifier &minus;</p>
<pre class="prettyprint notranslate">
from __future__ import division, print_function, absolute_import

#TFLearn module implementation
import tflearn
from tflearn.estimators import RandomForestClassifier

# Data loading and pre-processing with respect to dataset
import tflearn.datasets.mnist as mnist
X, Y, testX, testY = mnist.load_data(one_hot = False)

m = RandomForestClassifier(n_estimators = 100, max_nodes = 1000)
m.fit(X, Y, batch_size = 10000, display_step = 10)

print("Compute the accuracy on train data:")
print(m.evaluate(X, Y, tflearn.accuracy_op))

print("Compute the accuracy on test set:")
print(m.evaluate(testX, testY, tflearn.accuracy_op))

print("Digits for test images id 0 to 5:")
print(m.predict(testX[:5]))

print("True digits:")
print(testY[:5])
</pre>
<h1>TensorFlow - CNN And RNN Difference</h1>
<p>In this chapter, we will focus on the difference between CNN and RNN &minus;</p>
<table class="table table-bordered">
<tr>
<th style="width:50%;text-align:center;">CNN</th>
<th style="text-align:center;">RNN</th>
</tr>
<tr>
<td>It is suitable for spatial data such as images.</td>
<td>RNN is suitable for temporal data, also called sequential data.</td>
</tr>
<tr>
<td>CNN is considered to be more powerful than RNN.</td>
<td>RNN includes less feature compatibility when compared to CNN.</td>
</tr>
<tr>
<td>This network takes fixed size inputs and generates fixed size outputs.</td>
<td>RNN can handle arbitrary input/output lengths.</td>
</tr>
<tr>
<td>CNN is a type of feed-forward artificial neural network with variations of multilayer perceptrons designed to use minimal amounts of preprocessing.</td>
<td>RNN unlike feed forward neural networks - can use their internal memory to process arbitrary sequences of inputs.</td>
</tr>
<tr>
<td>CNNs use connectivity pattern between the neurons. This is inspired by the organization of the animal visual cortex, whose individual neurons are arranged in such a way that they respond to overlapping regions tiling the visual field.</td>
<td>Recurrent neural networks use time-series information - what a user spoke last will impact what he/she will speak next.</td>
</tr>
<tr>
<td>CNNs are ideal for images and video processing.</td>
<td>RNNs are ideal for text and speech analysis.</td>
</tr>
</table>
<p>Following illustration shows the schematic representation of CNN and RNN &minus;</p>
<img src="/tensorflow/images/schematic_representation_of_cnn_and_rnn.jpg" alt="Schematic Representation Of CNN And RNN" />
<h1>TensorFlow - Keras</h1>
<p>Keras is compact, easy to learn, high-level Python library run on top of TensorFlow framework. It is made with focus of understanding deep learning techniques, such as creating layers for neural networks maintaining the concepts of shapes and mathematical details. The creation of freamework can be of the following two types &minus;</p>
<ul class="list">
<li>Sequential API</li>
<li>Functional API</li>
</ul>
<p>Consider the following eight steps to create deep learning model in Keras &minus;</p>
<ul class="list">
<li>Loading the data</li>
<li>Preprocess the loaded data</li>
<li>Definition of model</li>
<li>Compiling the model</li>
<li>Fit the specified model</li>
<li>Evaluate it</li>
<li>Make the required predictions</li>
<li>Save the model</li>
</ul>
<p>We will use the Jupyter Notebook for execution and display of output as shown below &minus;</p>
<p><b>Step 1</b> &minus; Loading the data and preprocessing the loaded data is implemented first to execute the deep learning model.</p>
<pre class="prettyprint notranslate">
import warnings
warnings.filterwarnings('ignore')

import numpy as np
np.random.seed(123) # for reproducibility

from keras.models import Sequential
from keras.layers import Flatten, MaxPool2D, Conv2D, Dense, Reshape, Dropout
from keras.utils import np_utils
Using TensorFlow backend.
from keras.datasets import mnist

# Load pre-shuffled MNIST data into train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
Y_train = np_utils.to_categorical(y_train, 10)
Y_test = np_utils.to_categorical(y_test, 10)
</pre>
<p>This step can be defined as “Import libraries and Modules” which means all the libraries and modules are imported as an initial step.</p>
<p><b>Step 2</b> &minus; In this step, we will define the model architecture &minus;</p>
<pre class="prettyprint notranslate">
model = Sequential()
model.add(Conv2D(32, 3, 3, activation = 'relu', input_shape = (28,28,1)))
model.add(Conv2D(32, 3, 3, activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation = 'softmax'))
</pre>
<p><b>Step 3</b> &minus; Let us now compile the specified model &minus;</p>
<pre class="result notranslate">
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
</pre>
<p><b>Step 4</b> &minus; We will now fit the model using training data &minus;</p>
<pre class="result notranslate">
model.fit(X_train, Y_train, batch_size = 32, epochs = 10, verbose = 1)
</pre>
<p>The output of iterations created is as follows &minus;</p>
<pre class="result notranslate">
Epoch 1/10 60000/60000 [==============================] - 65s - 
loss: 0.2124 - 
acc: 0.9345 
Epoch 2/10 60000/60000 [==============================] - 62s - 
loss: 0.0893 - 
acc: 0.9740 
Epoch 3/10 60000/60000 [==============================] - 58s - 
loss: 0.0665 - 
acc: 0.9802 
Epoch 4/10 60000/60000 [==============================] - 62s - 
loss: 0.0571 - 
acc: 0.9830 
Epoch 5/10 60000/60000 [==============================] - 62s - 
loss: 0.0474 - 
acc: 0.9855 
Epoch 6/10 60000/60000 [==============================] - 59s -
loss: 0.0416 - 
acc: 0.9871 
Epoch 7/10 60000/60000 [==============================] - 61s - 
loss: 0.0380 - 
acc: 0.9877 
Epoch 8/10 60000/60000 [==============================] - 63s - 
loss: 0.0333 - 
acc: 0.9895 
Epoch 9/10 60000/60000 [==============================] - 64s - 
loss: 0.0325 - 
acc: 0.9898 
Epoch 10/10 60000/60000 [==============================] - 60s - 
loss: 0.0284 - 
acc: 0.9910
</pre>
<h1>TensorFlow - Distributed Computing</h1>
<p>This chapter will focus on how to get started with distributed TensorFlow. The aim is to help developers understand the basic distributed TF concepts that are reoccurring, such as TF servers. We will use the Jupyter Notebook for evaluating distributed TensorFlow. The implementation of distributed computing with TensorFlow is mentioned below &minus;</p>
<p><b>Step 1</b> &minus; Import the necessary modules mandatory for distributed computing &minus;</p>
<pre class="result notranslate">
import tensorflow as tf
</pre>
<p><b>Step 2</b> &minus; Create a TensorFlow cluster with one node. Let this node be responsible for a job that that has name "worker" and that will operate one take at localhost:2222.</p>
<pre class="result notranslate">
cluster_spec = tf.train.ClusterSpec({'worker' : ['localhost:2222']})
server = tf.train.Server(cluster_spec)
server.target
</pre>
<p>The above scripts generate the following output &minus;</p>
<pre class="result notranslate">
'grpc://localhost:2222'
The server is currently running.
</pre>
<p><b>Step 3</b> &minus; The server configuration with respective session can be calculated by executing the following command &minus;</p>
<pre class="result notranslate">
server.server_def
</pre>
<p>The above command generates the following output &minus;</p>
<pre class="prettyprint notranslate">
cluster {
   job {
      name: "worker"
      tasks {
         value: "localhost:2222"
      }
   }
}
job_name: "worker"
protocol: "grpc"
</pre>
<p><b>Step 4</b> &minus; Launch a TensorFlow session with the execution engine being the server. Use TensorFlow to create a local server and use <b>lsof</b> to find out the location of the server.</p>
<pre class="result notranslate">
sess = tf.Session(target = server.target)
server = tf.train.Server.create_local_server()
</pre>
<p><b>Step 5</b> &minus; View devices available in this session and close the respective session.</p>
<pre class="prettyprint notranslate">
devices = sess.list_devices()
for d in devices:
   print(d.name)
sess.close()
</pre>
<p>The above command generates the following output &minus;</p>
<pre class="result notranslate">
/job:worker/replica:0/task:0/device:CPU:0
</pre>
<h1>TensorFlow - Exporting</h1>
<p>Here, we will focus on MetaGraph formation in TensorFlow. This will help us understand export module in TensorFlow. The MetaGraph contains the basic information, which is required to train, perform evaluation, or run inference on a previously trained graph.</p>
<p>Following is the code snippet for the same &minus;</p>
<pre class="prettyprint notranslate">
def export_meta_graph(filename = None, collection_list = None, as_text = False): 
   """this code writes `MetaGraphDef` to save_path/filename. 
   
   Arguments: 
   filename: Optional meta_graph filename including the path. collection_list: 
      List of string keys to collect. as_text: If `True`, 
      writes the meta_graph as an ASCII proto. 
   
   Returns: 
   A `MetaGraphDef` proto. """
</pre>
<p>One of the typical usage model for the same is mentioned below &minus;</p>
<pre class="prettyprint notranslate">
# Build the model ... 
with tf.Session() as sess: 
   # Use the model ... 
# Export the model to /tmp/my-model.meta. 
meta_graph_def = tf.train.export_meta_graph(filename = '/tmp/my-model.meta')
</pre>
<h1>TensorFlow - Multi-Layer Perceptron Learning</h1>
<p>Multi-Layer perceptron defines the most complicated architecture of artificial neural networks. It is substantially formed from multiple layers of perceptron.</p>
<p>The diagrammatic representation of multi-layer perceptron learning is as shown below &minus;</p>
<img src="/tensorflow/images/multi_layer_perceptron.jpg" alt="Multi Layer Perceptron" />
<p>MLP networks are usually used for supervised learning format. A typical learning algorithm for MLP networks is also called back propagation’s algorithm.</p>
<p>Now, we will focus on the implementation with MLP for an image classification problem.</p>
<pre class="prettyprint notranslate">
# Import MINST data 
from tensorflow.examples.tutorials.mnist import input_data 
mnist = input_data.read_data_sets("/tmp/data/", one_hot = True) 

import tensorflow as tf 
import matplotlib.pyplot as plt 

# Parameters 
learning_rate = 0.001 
training_epochs = 20 
batch_size = 100 
display_step = 1 

# Network Parameters 
n_hidden_1 = 256 

# 1st layer num features
n_hidden_2 = 256 # 2nd layer num features 
n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 
# MNIST total classes (0-9 digits) 

# tf Graph input 
x = tf.placeholder("float", [None, n_input]) 
y = tf.placeholder("float", [None, n_classes]) 

# weights layer 1 
h = tf.Variable(tf.random_normal([n_input, n_hidden_1])) # bias layer 1 
bias_layer_1 = tf.Variable(tf.random_normal([n_hidden_1])) 
# layer 1 layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, h), bias_layer_1)) 

# weights layer 2 
w = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])) 

# bias layer 2 
bias_layer_2 = tf.Variable(tf.random_normal([n_hidden_2])) 

# layer 2 
layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w), bias_layer_2)) 

# weights output layer 
output = tf.Variable(tf.random_normal([n_hidden_2, n_classes])) 

# biar output layer 
bias_output = tf.Variable(tf.random_normal([n_classes])) # output layer 
output_layer = tf.matmul(layer_2, output) + bias_output

# cost function 
cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
   logits = output_layer, labels = y)) 

#cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(output_layer, y)) 
# optimizer 
optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost) 

# optimizer = tf.train.GradientDescentOptimizer(
   learning_rate = learning_rate).minimize(cost) 

# Plot settings 
avg_set = [] 
epoch_set = [] 

# Initializing the variables 
init = tf.global_variables_initializer() 

# Launch the graph 
with tf.Session() as sess: 
   sess.run(init) 
   
   # Training cycle
   for epoch in range(training_epochs): 
      avg_cost = 0. 
      total_batch = int(mnist.train.num_examples / batch_size) 
      
      # Loop over all batches 
      for i in range(total_batch): 
         batch_xs, batch_ys = mnist.train.next_batch(batch_size) 
         # Fit training using batch data sess.run(optimizer, feed_dict = {
            x: batch_xs, y: batch_ys}) 
         # Compute average loss 
         avg_cost += sess.run(cost, feed_dict = {x: batch_xs, y: batch_ys}) / total_batch
      # Display logs per epoch step 
      if epoch % display_step == 0: 
         print 
         Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost)
      avg_set.append(avg_cost) 
      epoch_set.append(epoch + 1)
   print 
   "Training phase finished" 
   
   plt.plot(epoch_set, avg_set, 'o', label = 'MLP Training phase') 
   plt.ylabel('cost') 
   plt.xlabel('epoch') 
   plt.legend() 
   plt.show() 
   
   # Test model 
   correct_prediction = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1)) 
   
   # Calculate accuracy 
   accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) 
   print 
   "Model Accuracy:", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})
</pre>
<p>The above line of code generates the following output &minus;</p>
<img src="/tensorflow/images/implementation_with_mlp.jpg" alt="Implementation with MLP" />
<h1>TensorFlow - Hidden Layers of Perceptron</h1>
<p>In this chapter, we will be focus on the network we will have to learn from known set of points called x and f(x). A single hidden layer will build this simple network.</p>
<p>The code for the explanation of hidden layers of perceptron is as shown below &minus;</p>
<pre class="prettyprint notranslate">
#Importing the necessary modules 
import tensorflow as tf 
import numpy as np 
import math, random 
import matplotlib.pyplot as plt 

np.random.seed(1000) 
function_to_learn = lambda x: np.cos(x) + 0.1*np.random.randn(*x.shape) 
layer_1_neurons = 10 
NUM_points = 1000 

#Training the parameters 
batch_size = 100 
NUM_EPOCHS = 1500 

all_x = np.float32(np.random.uniform(-2*math.pi, 2*math.pi, (1, NUM_points))).T 
   np.random.shuffle(all_x) 

train_size = int(900) 
#Training the first 700 points in the given set x_training = all_x[:train_size] 
y_training = function_to_learn(x_training)

#Training the last 300 points in the given set x_validation = all_x[train_size:] 
y_validation = function_to_learn(x_validation) 

plt.figure(1) 
plt.scatter(x_training, y_training, c = 'blue', label = 'train') 
plt.scatter(x_validation, y_validation, c = 'pink', label = 'validation') 
plt.legend() 
plt.show()

X = tf.placeholder(tf.float32, [None, 1], name = "X")
Y = tf.placeholder(tf.float32, [None, 1], name = "Y")

#first layer 
#Number of neurons = 10 
w_h = tf.Variable(
   tf.random_uniform([1, layer_1_neurons],\ minval = -1, maxval = 1, dtype = tf.float32)) 
b_h = tf.Variable(tf.zeros([1, layer_1_neurons], dtype = tf.float32)) 
h = tf.nn.sigmoid(tf.matmul(X, w_h) + b_h)

#output layer 
#Number of neurons = 10 
w_o = tf.Variable(
   tf.random_uniform([layer_1_neurons, 1],\ minval = -1, maxval = 1, dtype = tf.float32)) 
b_o = tf.Variable(tf.zeros([1, 1], dtype = tf.float32)) 

#build the model 
model = tf.matmul(h, w_o) + b_o 

#minimize the cost function (model - Y) 
train_op = tf.train.AdamOptimizer().minimize(tf.nn.l2_loss(model - Y)) 

#Start the Learning phase 
sess = tf.Session() sess.run(tf.initialize_all_variables()) 

errors = [] 
for i in range(NUM_EPOCHS): 
   for start, end in zip(range(0, len(x_training), batch_size),\ 
      range(batch_size, len(x_training), batch_size)): 
      sess.run(train_op, feed_dict = {X: x_training[start:end],\ Y: y_training[start:end]})
   cost = sess.run(tf.nn.l2_loss(model - y_validation),\ feed_dict = {X:x_validation}) 
   errors.append(cost) 
   
   if i%100 == 0: 
      print("epoch %d, cost = %g" % (i, cost)) 
      
plt.plot(errors,label='MLP Function Approximation') plt.xlabel('epochs') 
plt.ylabel('cost') 
plt.legend() 
plt.show()
</pre>
<h3>Output</h3>
<p>Following is the representation of function layer approximation &minus;</p>
<img src="/tensorflow/images/function_layer_approximation.jpg" alt="Function Layer Approximation" />
<p>Here two data are represented in shape of W. The two data are: train and validation which are represented in distinct colors as visible in legend section.</p>
<img src="/tensorflow/images/distinct_colors.jpg" alt="Distinct Colors" />
<p></p>
<img src="/tensorflow/images/mlp_function_approximation.jpg" alt="MLP Function Approximation" />
<h1>TensorFlow - Optimizers</h1>
<p>Optimizers are the extended class, which include added information to train a specific model. The optimizer class is initialized with given parameters but it is important to remember that no Tensor is needed. The optimizers are used for improving speed and performance for training a specific model.</p>
<p>The basic optimizer of TensorFlow is &minus;</p>
<pre class="result notranslate">
tf.train.Optimizer
</pre>
<p>This class is defined in the specified path of tensorflow/python/training/optimizer.py.</p>
<p>Following are some optimizers in Tensorflow &minus;</p>
<ul class="list">
<li>Stochastic Gradient descent</li>
<li>Stochastic Gradient descent with gradient clipping</li>
<li>Momentum</li>
<li>Nesterov momentum</li>
<li>Adagrad</li>
<li>Adadelta</li>
<li>RMSProp</li>
<li>Adam</li>
<li>Adamax</li>
<li>SMORMS3</li>
</ul>
<p>We will focus on the Stochastic Gradient descent. The illustration for creating optimizer for the same is mentioned below &minus;</p>
<pre class="prettyprint notranslate">
def sgd(cost, params, lr = np.float32(0.01)):
   g_params = tf.gradients(cost, params)
   updates = []
   
   for param, g_param in zip(params, g_params):
      updates.append(param.assign(param - lr*g_param))
   return updates
</pre>
<p>The basic parameters are defined within the specific function. In our subsequent chapter, we will focus on Gradient Descent Optimization with implementation of optimizers.</p>
<h1>TensorFlow - XOR Implementation</h1>
<p>In this chapter, we will learn about the XOR implementation using TensorFlow. Before starting with XOR implementation in TensorFlow, let us see the XOR table values. This will help us understand encryption and decryption process.</p>
<table style="width:60%; margin:auto;" class="table table-bordered ts">
<tr>
<td>A</td>
<td>B</td>
<td>A <b>XOR</b> B</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</table>
<p>XOR Cipher encryption method is basically used to encrypt data which is hard to crack with brute force method, i.e., by generating random encryption keys which match the appropriate key.</p>
<p>The concept of implementation with XOR Cipher is to define a XOR encryption key and then perform XOR operation of the characters in the specified string with this key, which a user tries to encrypt. Now we will focus on XOR implementation using TensorFlow, which is mentioned below &minus;</p>
<pre class="prettyprint notranslate">
#Declaring necessary modules
import tensorflow as tf
import numpy as np
"""
A simple numpy implementation of a XOR gate to understand the backpropagation
algorithm
"""

x = tf.placeholder(tf.float64,shape = [4,2],name = "x")
#declaring a place holder for input x
y = tf.placeholder(tf.float64,shape = [4,1],name = "y")
#declaring a place holder for desired output y

m = np.shape(x)[0]#number of training examples
n = np.shape(x)[1]#number of features
hidden_s = 2 #number of nodes in the hidden layer
l_r = 1#learning rate initialization

theta1 = tf.cast(tf.Variable(tf.random_normal([3,hidden_s]),name = "theta1"),tf.float64)
theta2 = tf.cast(tf.Variable(tf.random_normal([hidden_s+1,1]),name = "theta2"),tf.float64)

#conducting forward propagation
a1 = tf.concat([np.c_[np.ones(x.shape[0])],x],1)
#the weights of the first layer are multiplied by the input of the first layer

z1 = tf.matmul(a1,theta1)
#the input of the second layer is the output of the first layer, passed through the 
   activation function and column of biases is added

a2 = tf.concat([np.c_[np.ones(x.shape[0])],tf.sigmoid(z1)],1)
#the input of the second layer is multiplied by the weights

z3 = tf.matmul(a2,theta2)
#the output is passed through the activation function to obtain the final probability

h3 = tf.sigmoid(z3)
cost_func = -tf.reduce_sum(y*tf.log(h3)+(1-y)*tf.log(1-h3),axis = 1)

#built in tensorflow optimizer that conducts gradient descent using specified 
   learning rate to obtain theta values

optimiser = tf.train.GradientDescentOptimizer(learning_rate = l_r).minimize(cost_func)

#setting required X and Y values to perform XOR operation
X = [[0,0],[0,1],[1,0],[1,1]]
Y = [[0],[1],[1],[0]]

#initializing all variables, creating a session and running a tensorflow session
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

#running gradient descent for each iteration and printing the hypothesis 
   obtained using the updated theta values
for i in range(100000):
   sess.run(optimiser, feed_dict = {x:X,y:Y})#setting place holder values using feed_dict
   if i%100==0:
      print("Epoch:",i)
      print("Hyp:",sess.run(h3,feed_dict = {x:X,y:Y}))
</pre>
<p>The above line of code generates an output as shown in the screenshot below &minus;</p>
<img src="/tensorflow/images/xor_implementation_using_tensorflow.jpg" alt="XOR implementation using TensorFlow" />
<h1>TensorFlow - Gradient Descent Optimization</h1>
<p>Gradient descent optimization is considered to be an important concept in data science.</p>
<p>Consider the steps shown below to understand the implementation of gradient descent optimization &minus;</p>
<h2>Step 1</h2>
<p>Include necessary modules and declaration of x and y variables through which we are going to define the gradient descent optimization.</p>
<pre class="prettyprint notranslate">
import tensorflow as tf

x = tf.Variable(2, name = 'x', dtype = tf.float32)
log_x = tf.log(x)
log_x_squared = tf.square(log_x)

optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(log_x_squared)
</pre>
<h2>Step 2</h2>
<p>Initialize the necessary variables and call the optimizers for defining and calling it with respective function.</p>
<pre class="prettyprint notranslate">
init = tf.initialize_all_variables()

def optimize():
   with tf.Session() as session:
      session.run(init)
      print("starting at", "x:", session.run(x), "log(x)^2:", session.run(log_x_squared))
      
      for step in range(10):
         session.run(train)
         print("step", step, "x:", session.run(x), "log(x)^2:", session.run(log_x_squared))
optimize()
</pre>
<p>The above line of code generates an output as shown in the screenshot below &minus;</p>
<img src="/tensorflow/images/initialize_variables.jpg" alt="Initialize Variables" />
<p>We can see that the necessary epochs and iterations are calculated as shown in the output.</p>
<h1>TensorFlow - Forming Graphs</h1>
<p>A partial differential equation (PDE) is a differential equation, which involves partial derivatives with unknown function of several independent variables. With reference to partial differential equations, we will focus on creating new graphs.</p>
<p>Let us assume there is a pond with dimension 500*500 square &minus;</p>
<p><b>N = 500</b></p>
<p>Now, we will compute partial differential equation and form the respective graph using it. Consider the steps given below for computing graph.</p>
<p><b>Step 1</b> &minus; Import libraries for simulation.</p>
<pre class="result notranslate">
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
</pre>
<p><b>Step 2</b> &minus; Include functions for transformation of a 2D array into a convolution kernel and simplified 2D convolution operation.</p>
<pre class="prettyprint notranslate">
def make_kernel(a):
   a = np.asarray(a)
   a = a.reshape(list(a.shape) + [1,1])
   return tf.constant(a, dtype=1)

def simple_conv(x, k):
   """A simplified 2D convolution operation"""
   x = tf.expand_dims(tf.expand_dims(x, 0), -1)
   y = tf.nn.depthwise_conv2d(x, k, [1, 1, 1, 1], padding = 'SAME')
   return y[0, :, :, 0]

def laplace(x):
   """Compute the 2D laplacian of an array"""
   laplace_k = make_kernel([[0.5, 1.0, 0.5], [1.0, -6., 1.0], [0.5, 1.0, 0.5]])
   return simple_conv(x, laplace_k)
   
sess = tf.InteractiveSession()
</pre>
<p><b>Step 3</b> &minus; Include the number of iterations and compute the graph to display the records accordingly.</p>
<pre class="prettyprint notranslate">
N = 500

# Initial Conditions -- some rain drops hit a pond

# Set everything to zero
u_init = np.zeros([N, N], dtype = np.float32)
ut_init = np.zeros([N, N], dtype = np.float32)

# Some rain drops hit a pond at random points
for n in range(100):
   a,b = np.random.randint(0, N, 2)
   u_init[a,b] = np.random.uniform()

plt.imshow(u_init)
plt.show()

# Parameters:
# eps -- time resolution
# damping -- wave damping
eps = tf.placeholder(tf.float32, shape = ())
damping = tf.placeholder(tf.float32, shape = ())

# Create variables for simulation state
U = tf.Variable(u_init)
Ut = tf.Variable(ut_init)

# Discretized PDE update rules
U_ = U + eps * Ut
Ut_ = Ut + eps * (laplace(U) - damping * Ut)

# Operation to update the state
step = tf.group(U.assign(U_), Ut.assign(Ut_))

# Initialize state to initial conditions
tf.initialize_all_variables().run()

# Run 1000 steps of PDE
for i in range(1000):
   # Step simulation
   step.run({eps: 0.03, damping: 0.04})
   
   # Visualize every 50 steps
   if i % 500 == 0:
      plt.imshow(U.eval())
      plt.show()
</pre>
<p>The graphs are plotted as shown below &minus;</p>
<img src="/tensorflow/images/forming_graphs.jpg" alt="Forming Graphs" />
<p></p>
<img src="/tensorflow/images/graphs_plotted.jpg" alt="Graphs Plotted " />
<h1>Image Recognition using TensorFlow</h1>
<p>TensorFlow includes a special feature of image recognition and these images are stored in a specific folder. With relatively same images, it will be easy to implement this logic for security purposes.</p>
<p>The folder structure of image recognition code implementation is as shown below &minus;</p>
<img src="/tensorflow/images/image_recognition.jpg" alt="Image Recognition" />
<p>The dataset_image includes the related images, which need to be loaded. We will focus on image recognition with our logo defined in it. The images are loaded with “load_data.py” script, which helps in keeping a note on various image recognition modules within them.</p>
<pre class="prettyprint notranslate">
import pickle
from sklearn.model_selection import train_test_split
from scipy import misc

import numpy as np
import os

label = os.listdir("dataset_image")
label = label[1:]
dataset = []

for image_label in label:
   images = os.listdir("dataset_image/"+image_label)
   
   for image in images:
      img = misc.imread("dataset_image/"+image_label+"/"+image)
      img = misc.imresize(img, (64, 64))
      dataset.append((img,image_label))
X = []
Y = []

for input,image_label in dataset:
   X.append(input)
   Y.append(label.index(image_label))

X = np.array(X)
Y = np.array(Y)

X_train,y_train, = X,Y

data_set = (X_train,y_train)

save_label = open("int_to_word_out.pickle","wb")
pickle.dump(label, save_label)
save_label.close()
</pre>
<p>The training of images helps in storing the recognizable patterns within specified folder.</p>
<pre class="prettyprint notranslate">
import numpy
import matplotlib.pyplot as plt

from keras.layers import Dropout
from keras.layers import Flatten
from keras.constraints import maxnorm
from keras.optimizers import SGD
from keras.layers import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
from keras import backend as K

import load_data
from keras.models import Sequential
from keras.layers import Dense

import keras
K.set_image_dim_ordering('tf')

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)

# load data
(X_train,y_train) = load_data.data_set

# normalize inputs from 0-255 to 0.0-1.0
X_train = X_train.astype('float32')

#X_test = X_test.astype('float32')
X_train = X_train / 255.0

#X_test = X_test / 255.0
# one hot encode outputs
y_train = np_utils.to_categorical(y_train)

#y_test = np_utils.to_categorical(y_test)
num_classes = y_train.shape[1]

# Create the model
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), padding = 'same', 
   activation = 'relu', kernel_constraint = maxnorm(3)))

model.add(Dropout(0.2))
model.add(Conv2D(32, (3, 3), activation = 'relu', padding = 'same', 
   kernel_constraint = maxnorm(3)))

model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Flatten())
model.add(Dense(512, activation = 'relu', kernel_constraint = maxnorm(3)))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation = 'softmax'))

# Compile model
epochs = 10
lrate = 0.01
decay = lrate/epochs
sgd = SGD(lr = lrate, momentum = 0.9, decay = decay, nesterov = False)
model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])
print(model.summary())

#callbacks = [keras.callbacks.EarlyStopping(
   monitor = 'val_loss', min_delta = 0, patience = 0, verbose = 0, mode = 'auto')]
callbacks = [keras.callbacks.TensorBoard(log_dir='./logs', 
   histogram_freq = 0, batch_size = 32, write_graph = True, write_grads = False, 
   write_images = True, embeddings_freq = 0, embeddings_layer_names = None, 
   embeddings_metadata = None)]

# Fit the model

model.fit(X_train, y_train, epochs = epochs, 
   batch_size = 32,shuffle = True,callbacks = callbacks)

# Final evaluation of the model
scores = model.evaluate(X_train, y_train, verbose = 0)
print("Accuracy: %.2f%%" % (scores[1]*100))

# serialize model to JSONx
model_json = model.to_json()
with open("model_face.json", "w") as json_file:
   json_file.write(model_json)

# serialize weights to HDF5
model.save_weights("model_face.h5")
print("Saved model to disk")
</pre>
<p>The above line of code generates an output as shown below &minus;</p>
<img src="/tensorflow/images/recognizable_patterns.jpg" alt="Recognizable Patterns" />
<p></p>
<img src="/tensorflow/images/recognizable_patterns_output.jpg" alt="Recognizable Patterns Output" />
<h1>Recommendations for Neural Network Training</h1>
<p>In this chapter, we will understand the various aspects of neural network training which can be implemented using TensorFlow framework.</p>
<p>Following are the ten recommendations, which can be evaluated &minus;</p>
<h2>Back Propagation</h2>
<p>Back propagation is a simple method to compute partial derivatives, which includes the basic form of composition best suitable for neural nets.</p>
<img src="/tensorflow/images/back_propagation.jpg" alt="Back Propagation" />
<h2>Stochastic Gradient Descent</h2>
<p>In stochastic gradient descent, a <b>batch</b> is the total number of examples, which a user uses to calculate the gradient in a single iteration. So far, it is assumed that the batch has been the entire data set. The best illustration is working at Google scale; data sets often contain billions or even hundreds of billions of examples.</p>
<img src="/tensorflow/images/stochastic_gradient_descent.jpg" alt="Stochastic Gradient Descent" />
<h2>Learning Rate Decay</h2>
<img src="/tensorflow/images/learning_rate_decay.jpg" alt="Learning Rate Decay" />
<p>Adapting the learning rate is one of the most important features of gradient descent optimization. This is crucial to TensorFlow implementation.</p>
<h2>Dropout</h2>
<p>Deep neural nets with a large number of parameters form powerful machine learning systems. However, over fitting is a serious problem in such networks.</p>
<img src="/tensorflow/images/dropout.jpg" alt="Dropout" />
<h2>Max Pooling</h2>
<p>Max pooling is a sample-based discretization process. The object is to down-sample an input representation, which reduces the dimensionality with the required assumptions.</p>
<img src="/tensorflow/images/max_pooling.jpg" alt="Max Pooling" />
<h2>Long Short Term Memory (LSTM)</h2>
<p>LSTM controls the decision on what inputs should be taken within the specified neuron. It includes the control on deciding what should be computed and what output should be generated.</p>
<img src="/tensorflow/images/long_short_term_memory.jpg" alt="Long Short Term Memory" />
<div class="mui-container-fluid button-borders show">
<div class="pre-btn">
<a href="/tensorflow/tensorflow_recommendations_for_neural_network_training.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/tensorflow/tensorflow_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="google-bottom-ads">
<div>Advertisements</div>
<script><!--
var width = 580;
var height = 400;
var format = "580x400_as";
if( window.innerWidth < 468 ){
   width = 300;
   height = 250;
   format = "300x250_as";
}
google_ad_client = "pub-7133395778201029";
google_ad_width = width;
google_ad_height = height;
google_ad_format = format;
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
<div class="space-bottom"></div>
</div>
</div>
<!-- Tutorial Content Ends Here -->
<!-- Right Column Starts Here -->
<div class="mui-col-md-2 google-right-ads">
<div class="space-top"></div>
<div class="google-right-ad" style="margin: 0px auto !important;margin-top:5px;">
<script><!--
google_ad_client = "pub-2537027957187252";
google_ad_width = 300;
google_ad_height = 250;
google_ad_format = "300x250_as";
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9012177"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9012177")})</script>
</div>
<div class="space-bottom"></div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9013289"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9013289")})</script>
</div>
<div class="space-bottom" style="margin-bottom:15px;"></div>
</div>
<!-- Right Column Ends Here -->
</div>
</div>
<div class="clear"></div>
<footer id="footer">
<div class="mui--text-center">
<div class="mui--text-caption mui--text-light">
<a href="/index.htm" class="logo"><img class="img-responsive" src="/images/logo-black.png" alt="Tutorials Point" title="Tutorials Point"></a>
</div>
<ul class="mui-list--inline mui--text-body2 mui--text-light">
<li><a href="/about/index.htm"><i class="fal fa-globe"></i> About us</a></li>
<li><a href="/about/about_terms_of_use.htm"><i class="fal fa-asterisk"></i> Terms of use</a></li>
<li><a href="/about/about_privacy.htm#cookies"> <i class="fal fa-shield-check"></i> Cookies Policy</a></li>
<li><a href="/about/faq.htm"><i class="fal fa-question-circle"></i> FAQ's</a></li>
<li><a href="/about/about_helping.htm"><i class="fal fa-hands-helping"></i> Helping</a></li>
<li><a href="/about/contact_us.htm"><i class="fal fa-map-marker-alt"></i> Contact</a></li>
</ul>
<div class="mui--text-caption mui--text-light bottom-copyright-text">&copy; Copyright 2019. All Rights Reserved.</div>
</div>
<div id="privacy-banner">
  <div>
    <p>
      We use cookies to provide and improve our services. By using our site, you consent to our Cookies Policy.
      <a id="banner-accept" href="#">Accept</a>
      <a id="banner-learn" href="/about/about_cookies.htm" target="_blank">Learn more</a>
    </p>
  </div>
</div>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-232293-17"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-232293-6');
</script>
</footer>
</body>
</html>
