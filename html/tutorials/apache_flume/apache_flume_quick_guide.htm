<!DOCTYPE html>
<html lang="en-US">
<head>
<title>Apache Flume - Quick Guide</title>
<meta charset="utf-8">
<meta name="description" content="Apache Flume - Quick Guide - Apache Flume is a tool/service/data ingestion mechanism for collecting aggregating and transporting large amounts of streaming data such as log files, events (e"/>
<meta name="keywords" content="C, C++, Python, Java, HTML, CSS, JavaScript, SQL, PHP, jQuery, XML, DOM, Bootstrap, Tutorials, Articles, Programming, training, learning, quiz, preferences, examples, code"/>
<link rel="canonical" href="https://www.tutorialspoint.com/apache_flume/apache_flume_quick_guide.htm" />
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<meta name="viewport" content="width=device-width,initial-scale=1.0,user-scalable=yes">
<script src="/theme/js/script-min-v2.js?v=3"></script>
<link rel="stylesheet" href="/theme/css/style-min-v2.css?v=6">
<script src="//services.bilsyndication.com/adv1/?d=901" defer="" async=""></script>
<script> var vitag = vitag || {};</script>
<script> vitag.outStreamConfig = { enablePC: false, enableMobile: false };</script>  
<style>
.right-menu .mui-btn {
    background-color:#246aaf;
}
a.demo {
    background:#246aaf;
}
li.heading {
    background:#246aaf;
}
.course-box{background:#246aaf}
.home-intro-sub p{color:#246aaf}
</style>
</head>
<body>
<header id="header">
<!-- Top sub-menu Starts Here -->
<div class="mui-appbar mui-container-fulid top-menu">
<div class="mui-container">
<div class="top-menu-item home">
<a href="https://www.tutorialspoint.com/index.htm" target="_blank" title="TutorialsPoint - Home"><i class="fal fa-home"></i> <span>Home</span></a>
</div>
<div class="top-menu-item qa">
<a href="https://www.tutorialspoint.com/about/about_careers.htm" target="_blank" title="Job @ Tutorials Point"><i class="fa fa-suitcase"></i> <span>Jobs</span></a>
</div>
<div class="top-menu-item tools">
<a href="https://www.tutorialspoint.com/online_dev_tools.htm" target="_blank" title="Tools - Online Development and Testing Tools"><i class="fal fa-cogs"></i> <span>Tools</span></a>
</div>
<div class="top-menu-item coding-ground">
<a href="https://www.tutorialspoint.com/codingground.htm" target="_blank" title="Coding Ground - Free Online IDE and Terminal"><i class="fal fa-code"></i> <span>Coding Ground </span></a> 
</div>
<div class="top-menu-item current-affairs">
<a href="https://www.tutorialspoint.com/current_affairs.htm" target="_blank" title="Daily Current Affairs"><i class="fal fa-layer-plus"></i> <span>Current Affairs</span></a>
</div>
<div class="top-menu-item upsc-notes">
<a href="https://www.tutorialspoint.com/upsc_ias_exams.htm" target="_blank" title="UPSC IAS Exams Notes - TutorialsPoint"><i class="fal fa-user-tie"></i> <span>UPSC Notes</span></a>
</div>      
<div class="top-menu-item online-tutoris">
<a href="https://www.tutorialspoint.com/tutor_connect/index.php" target="_blank" title="Top Online Tutors - Tutor Connect"><i class="fal fa-user"></i> <span>Online Tutors</span></a>
</div>
<div class="top-menu-item whiteboard">
<a href="https://www.tutorialspoint.com/whiteboard.htm" target="_blank" title="Free Online Whiteboard"><i class="fal fa-chalkboard"></i> <span>Whiteboard</span></a>
</div>
<div class="top-menu-item net-meeting">
<a href="https://www.tutorialspoint.com/netmeeting.php" target="_blank" title="A free tool for online video conferencing"><i class="fal fa-chalkboard-teacher"></i> <span>Net Meeting</span></a> 
</div>
<div class="top-menu-item articles">
<a href="https://www.tutorix.com" target="_blank" title="Tutorx - The Best Learning App" rel="nofollow"><i class="fal fa-video"></i> <span>Tutorix</span></a> 
</div>        
<div class="social-menu-item">
<a href="https://www.facebook.com/tutorialspointindia" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Facebook"><i class="fab fa-facebook-f"></i></a> 
<a href="https://www.twitter.com/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Twitter"><i class="fab fa-twitter"></i></a>
<a href="https://www.linkedin.com/company/tutorialspoint" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint @ Linkedin"><i class="fab fa-linkedin-in"></i></a>
<a href="https://www.youtube.com/channel/UCVLbzhxVTiTLiVKeGV7WEBg" target="_blank" rel="nofollow" data-placement="bottom" title="tutorialspoint YouTube"><i class="fab fa-youtube"></i></a>
</div>        
</div>
</div>
<!-- Top sub-menu Ends Here -->
<!-- Top main-menu Starts Here -->
<div class="mui-appbar mui-container-fulid mui--appbar-line-height mui--z1" id="logo-menu">
<div class="mui-container">
<div class="left-menu">
<a href="https://www.tutorialspoint.com/index.htm" title="Tutorialspoint">
<img class="tp-logo" alt="tutorialspoint" src="/apache_flume/images/logo.png">
</a>
<div class="mui-dropdown">
<a class="mui-btn mui-btn--primary categories" data-mui-toggle="dropdown"><i class="fa fa-th-large"></i> 
<span>Categories <span class="mui-caret"></span></span></a>            
<ul class="mui-dropdown__menu cat-menu">
<li>
<ul>
<li><a href="/academic_tutorials.htm"><i class="fa fa-caret-right"></i> Academic Tutorials</a></li>
<li><a href="/big_data_tutorials.htm"><i class="fa fa-caret-right"></i> Big Data &amp; Analytics </a></li>
<li><a href="/computer_programming_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Programming </a></li>
<li><a href="/computer_science_tutorials.htm"><i class="fa fa-caret-right"></i> Computer Science </a></li>
<li><a href="/database_tutorials.htm"><i class="fa fa-caret-right"></i> Databases </a></li>
<li><a href="/devops_tutorials.htm"><i class="fa fa-caret-right"></i> DevOps </a></li>
<li><a href="/digital_marketing_tutorials.htm"><i class="fa fa-caret-right"></i> Digital Marketing </a></li>
<li><a href="/engineering_tutorials.htm"><i class="fa fa-caret-right"></i> Engineering Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> Exams Syllabus </a></li>
<li><a href="/famous_monuments.htm"><i class="fa fa-caret-right"></i> Famous Monuments </a></li>
<li><a href="/gate_exams_tutorials.htm"><i class="fa fa-caret-right"></i> GATE Exams Tutorials</a></li>
<li><a href="/latest_technologies.htm"><i class="fa fa-caret-right"></i> Latest Technologies </a></li>
<li><a href="/machine_learning_tutorials.htm"><i class="fa fa-caret-right"></i> Machine Learning </a></li>
<li><a href="/mainframe_tutorials.htm"><i class="fa fa-caret-right"></i> Mainframe Development </a></li>
<li><a href="/management_tutorials.htm"><i class="fa fa-caret-right"></i> Management Tutorials </a></li>
<li><a href="/maths_tutorials.htm"><i class="fa fa-caret-right"></i> Mathematics Tutorials</a></li>
<li><a href="/microsoft_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Microsoft Technologies </a></li>
<li><a href="/misc_tutorials.htm"><i class="fa fa-caret-right"></i> Misc tutorials </a></li>
<li><a href="/mobile_development_tutorials.htm"><i class="fa fa-caret-right"></i> Mobile Development </a></li>
<li><a href="/java_technology_tutorials.htm"><i class="fa fa-caret-right"></i> Java Technologies </a></li>
<li><a href="/python_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> Python Technologies </a></li>
<li><a href="/sap_tutorials.htm"><i class="fa fa-caret-right"></i> SAP Tutorials </a></li>
<li><a href="/scripting_lnaguage_tutorials.htm"><i class="fa fa-caret-right"></i>Programming Scripts </a></li>
<li><a href="/selected_reading.htm"><i class="fa fa-caret-right"></i> Selected Reading </a></li>
<li><a href="/software_quality_tutorials.htm"><i class="fa fa-caret-right"></i> Software Quality </a></li>
<li><a href="/soft_skill_tutorials.htm"><i class="fa fa-caret-right"></i> Soft Skills </a></li>
<li><a href="/telecom_tutorials.htm"><i class="fa fa-caret-right"></i> Telecom Tutorials </a></li>
<li><a href="/upsc_ias_exams.htm"><i class="fa fa-caret-right"></i> UPSC IAS Exams </a></li>
<li><a href="/web_development_tutorials.htm"><i class="fa fa-caret-right"></i> Web Development </a></li>
<li><a href="/sports_tutorials.htm"><i class="fa fa-caret-right"></i> Sports Tutorials </a></li>
<li><a href="/xml_technologies_tutorials.htm"><i class="fa fa-caret-right"></i> XML Technologies </a></li>
<li><a href="/multi_language_tutorials.htm"><i class="fa fa-caret-right"></i> Multi-Language Tutorials</a></li>
<li><a href="/questions_and_answers.htm"><i class="fa fa-caret-right"></i> Interview Questions</a></li>
</ul>
</li>
</ul>
<div class="clear"></div>
</div> 
</div>
<div class="right-menu">
<div class="toc-toggle">
<a href="javascript:void(0);"><i class="fa fa-bars"></i></a>
</div>
<div class="mobile-search-btn">
<a href="https://www.tutorialspoint.com/search.htm"><i class="fal fa-search"></i></a>
</div>
<div class="search-box">
<form method="get" class="" name="searchform" action="https://www.google.com/search" target="_blank" novalidate="">
<input type="hidden" name="sitesearch" value="www.tutorialspoint.com" class="user-valid valid">
<input class="header-search-box" type="text" id="search-string" name="q" placeholder="Search your favorite tutorials..." onfocus="if (this.value == 'Search your favorite tutorials...') {this.value = '';}" onblur="if (this.value == '') {this.value = 'Search your favorite tutorials...';}" autocomplete="off">
<button><i class="fal fa-search"></i></button>
</form>
</div>
<div class="menu-btn library-btn">
<a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a>
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a> 
</div>
<div class="menu-btn videos-btn">
<a href="https://www.tutorialspoint.com/questions/index.php"><i class="fa fa-location-arrow"></i> <span>Q/A</span></a>
</div>
<div class="menu-btn ebooks-btn">
<a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a>
</div>
<div class="mui-dropdown">
<button class="mui-btn mui-btn--primary" data-mui-toggle="dropdown">
<span class="mui-caret"></span>
</button>
<ul class="mui-dropdown__menu">
<li><a href="https://www.tutorialspoint.com/tutorialslibrary.htm"><i class="fal fa-cubes"></i> <span>Library</span></a></li>
<li><a href="https://www.tutorialspoint.com/videotutorials/index.htm"><i class="fal fa-video"></i> <span>Videos</span></a></li>
<li><a href="https://store.tutorialspoint.com"><i class="fal fa-book"></i> <span>eBooks</span></a></li>
</ul>
</div>
</div>
</div>
</div>
<!-- Top main-menu Ends Here -->
</header>
<div class="mui-container-fluid content">
<div class="mui-container">
<!-- Tutorial ToC Starts Here -->
<div class="mui-col-md-3 tutorial-toc">
<div class="mini-logo">
<img src="/apache_flume/images/flume-mini-logo.jpg" alt="Apache Flume Tutorial" />
</div>
<ul class="toc chapters">
<li class="heading">Apache Flume Tutorial</li>
<li><a href="/apache_flume/index.htm">Apache Flume - Home</a></li>
<li><a href="/apache_flume/apache_flume_introduction.htm">Apache Flume - Introduction</a></li>
<li><a href="/apache_flume/data_transfer_in_hadoop.htm">Data Transfer in Hadoop</a></li>
<li><a href="/apache_flume/apache_flume_architecture.htm">Apache Flume - Architecture</a></li>
<li><a href="/apache_flume/apache_flume_data_flow.htm">Apache Flume - Data Flow</a></li>
<li><a href="/apache_flume/apache_flume_environment.htm">Apache Flume - Environment</a></li>
<li><a href="/apache_flume/apache_flume_configuration.htm">Apache Flume - configuration</a></li>
<li><a href="/apache_flume/fetching_twitter_data.htm">Apache Flume - Fetching Twitter Data</a></li>
<li><a href="/apache_flume/sequence_generator_source.htm">Sequence Generator Source</a></li>
<li><a href="/apache_flume/apache_flume_netcat_source.htm">Apache Flume - NetCat Source</a></li>
</ul>
<ul class="toc chapters">
<li class="heading">Apache Flume Resources</li>
<li><a href="/apache_flume/apache_flume_quick_guide.htm">Apache Flume - Quick Guide</a></li>
<li><a href="/apache_flume/apache_flume_useful_resources.htm">Apache Flume - Useful Resources</a></li>
<li><a href="/apache_flume/apache_flume_discussion.htm">Apache Flume - Discussion</a></li>
</ul>
<ul class="toc reading">
<li class="sreading">Selected Reading</li>
<li><a target="_top" href="/upsc_ias_exams.htm">UPSC IAS Exams Notes</a></li>
<li><a target="_top" href="/developers_best_practices/index.htm">Developer's Best Practices</a></li>
<li><a target="_top" href="/questions_and_answers.htm">Questions and Answers</a></li>
<li><a target="_top" href="/effective_resume_writing.htm">Effective Resume Writing</a></li>
<li><a target="_top" href="/hr_interview_questions/index.htm">HR Interview Questions</a></li>
<li><a target="_top" href="/computer_glossary.htm">Computer Glossary</a></li>
<li><a target="_top" href="/computer_whoiswho.htm">Who is Who</a></li>
</ul>
</div>
<!-- Tutorial ToC Ends Here -->
<!-- Tutorial Content Starts Here -->
<div class="mui-col-md-6 tutorial-content">
<h1>Apache Flume - Quick Guide</h1>
<hr />
<div class="top-ad-heading">Advertisements</div>
<div style="text-align: center;">
<script><!--
google_ad_client = "pub-7133395778201029";
var width = document.getElementsByClassName("tutorial-content")[0].clientWidth - 40;
google_ad_width = width;
google_ad_height = 150;
google_ad_format = width + "x150_as";
google_ad_type = "image";
google_ad_channel = "";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="mui-container-fluid button-borders">
<div class="pre-btn">
<a href="/apache_flume/apache_flume_netcat_source.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/apache_flume/apache_flume_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="clearer"></div>
<h1>Apache Flume - Introduction</h1>
<h2>What is Flume?</h2>
<p>Apache Flume is a tool/service/data ingestion mechanism for collecting aggregating and transporting large amounts of streaming data such as log files, events (etc...) from various sources to a centralized data store.</p>
<p>Flume is a highly reliable, distributed, and configurable tool. It is principally designed to copy streaming data (log data) from various web servers to HDFS.</p>
<img src="/apache_flume/images/apache_flume.jpg" alt="Apache Flume" />
<h2>Applications of Flume</h2>
<p>Assume an e-commerce web application wants to analyze the customer behavior from a particular region. To do so, they would need to move the available log data in to Hadoop for analysis. Here, Apache Flume comes to our rescue.</p>
<p>Flume is used to move the log data generated by application servers into HDFS at a higher speed.</p>
<h2>Advantages of Flume</h2>
<p>Here are the advantages of using Flume &minus;</p>
<ul class="list">
<li><p>Using Apache Flume we can store the data in to any of the centralized stores (HBase, HDFS).</p></li>
<li><p>When the rate of incoming data exceeds the rate at which data can be written to the destination, Flume acts as a mediator between data producers and the centralized stores and provides a steady flow of data between them.</p></li>
<li><p>Flume provides the feature of <b>contextual routing</b>.</p></li>
<li><p>The transactions in Flume are channel-based where two transactions (one sender and one receiver) are maintained for each message. It guarantees reliable message delivery.</p></li>
<li><p>Flume is reliable, fault tolerant, scalable, manageable, and customizable.</p></li>
</ul>
<h2>Features of Flume</h2>
<p>Some of the notable features of Flume are as follows &minus;</p>
<ul class="list">
<li><p>Flume ingests log data from multiple web servers into a centralized store (HDFS, HBase) efficiently.</p></li>
<li><p>Using Flume, we can get the data from multiple servers immediately into Hadoop.</p></li>
<li><p>Along with the log files, Flume is also used to import huge volumes of event data produced by social networking sites like Facebook and Twitter, and e-commerce websites like Amazon and Flipkart.</p></li>
<li><p>Flume supports a large set of sources and destinations types.</p></li>
<li><p>Flume supports multi-hop flows, fan-in fan-out flows, contextual routing, etc.</p></li>
<li><p>Flume can be scaled horizontally.</p></li>
</ul>
<h1>Apache Flume - Data Transfer In Hadoop</h1>
<p><b>Big Data,</b> as we know, is a collection of large datasets that cannot be processed using traditional computing techniques. Big Data, when analyzed, gives valuable results. <b>Hadoop</b> is an open-source framework that allows to store and process Big Data in a distributed environment across clusters of computers using simple programming models.</p>
<h2>Streaming / Log Data</h2>
<p>Generally, most of the data that is to be analyzed will be produced by various data sources like applications servers, social networking sites, cloud servers, and enterprise servers. This data will be in the form of <b>log files</b> and <b>events</b>.</p>
<p><b>Log file</b> &minus; In general, a log file is a <b>file</b> that lists events/actions that occur in an operating system. For example, web servers list every request made to the server in the log files.</p>
<p>On harvesting such log data, we can get information about &minus;</p>
<ul class="list">
<li>the application performance and locate various software and hardware failures.</li>
<li>the user behavior and derive better business insights.</li>
</ul>
<p>The traditional method of transferring data into the HDFS system is to use the <b>put</b> command. Let us see how to use the <b>put</b> command.</p>
<h2>HDFS put Command</h2>
<p>The main challenge in handling the log data is in moving these logs produced by multiple servers to the Hadoop environment.</p>
<p>Hadoop <b>File System Shell</b> provides commands to insert data into Hadoop and read from it. You can insert data into Hadoop using the <b>put</b> command as shown below.</p>
<pre class="result notranslate">
$ Hadoop fs –put /path of the required file  /path in HDFS where to save the file 
</pre>
<h3>Problem with put Command</h3>
<p>We can use the <b>put</b> command of Hadoop to transfer data from these sources to HDFS. But, it suffers from the following drawbacks &minus;</p>
<ul class="list">
<li><p>Using <b>put</b> command, we can transfer <b>only one file at a time</b> while the data generators generate data at a much higher rate. Since the analysis made on older data is less accurate, we need to have a solution to transfer data in real time.</p></li>
<li><p>If we use <b>put</b> command, the data is needed to be packaged and should be ready for the upload. Since the webservers generate data continuously, it is a very difficult task.</p></li>
</ul>
<p>What we need here is a solutions that can overcome the drawbacks of <b>put</b> command and transfer the "streaming data" from data generators to centralized stores (especially HDFS) with less delay.</p>
<h3>Problem with HDFS</h3>
<p>In HDFS, the file exists as a directory entry and the length of the file will be considered as zero till it is closed. For example, if a source is writing data into HDFS and the network was interrupted in the middle of the operation (without closing the file), then the data written in the file will be lost.</p>
<p>Therefore we need a reliable, configurable, and maintainable system to transfer the log data into HDFS.</p>
<p><b>Note</b> &minus; In POSIX file system, whenever we are accessing a file (say performing write operation), other programs can still read this file (at least the saved portion of the file). This is because the file exists on the disc before it is closed.</p>
<h2>Available Solutions</h2>
<p>To send streaming data (log files, events etc..,) from various sources to HDFS, we have the following tools available at our disposal &minus;</p>
<h3>Facebook’s Scribe</h3>
<p>Scribe is an immensely popular tool that is used to aggregate and stream log data. It is designed to scale to a very large number of nodes and be robust to network and node failures.</p>
<h3>Apache Kafka</h3>
<p>Kafka has been developed by Apache Software Foundation. It is an open-source message broker. Using Kafka, we can handle feeds with high-throughput and low-latency.</p>
<h3>Apache Flume</h3>
<p>Apache Flume is a tool/service/data ingestion mechanism for collecting aggregating and transporting large amounts of streaming data such as log data, events (etc...) from various webserves to a centralized data store.</p>
<p>It is a highly reliable, distributed, and configurable tool that is principally designed to transfer streaming data from various sources to HDFS.</p>
<p>In this tutorial, we will discuss in detail how to use Flume with some examples.</p>
<h1>Apache Flume - Architecture</h1>
<p>The following illustration depicts the basic architecture of Flume. As shown in the illustration, <b>data generators</b> (such as Facebook, Twitter) generate data which gets collected by individual Flume <b>agents</b> running on them. Thereafter, a <b>data collector</b> (which is also an agent) collects the data from the agents which is aggregated and pushed into a centralized store such as HDFS or HBase.</p>
<img src="/apache_flume/images/flume_architecture.jpg" alt="Flume Architecture" />
<h2>Flume Event</h2>
<p>An <b>event</b> is the basic unit of the data transported inside <b>Flume</b>. It contains a payload of byte array that is to be transported from the source to the destination accompanied by optional headers. A typical Flume event would have the following structure &minus;</p>
<img src="/apache_flume/images/flume_event.jpg" alt="Flume Event" />
<h2>Flume Agent</h2>
<p>An <b>agent</b> is an independent daemon process (JVM) in Flume. It receives the data (events) from clients or other agents and forwards it to its next destination (sink or agent). Flume may have more than one agent. Following diagram represents a <b>Flume Agent</b></p>
<img src="/apache_flume/images/flume_agent1.jpg" alt="Flume Agent" />
<p>As shown in the diagram a Flume Agent contains three main components namely, <b>source</b>, <b>channel</b>, and <b>sink</b>.</p>
<h3>Source</h3>
<p>A <b>source</b> is the component of an Agent which receives data from the data generators and transfers it to one or more channels in the form of Flume events.</p>
<p>Apache Flume supports several types of sources and each source receives events from a specified data generator.</p>
<p><b>Example</b> &minus; Avro source, Thrift source, twitter 1% source etc.</p>
<h3>Channel</h3>
<p>A <b>channel</b> is a transient store which receives the events from the source and buffers them till they are consumed by sinks. It acts as a bridge between the sources and the sinks.</p>
<p>These channels are fully transactional and they can work with any number of sources and sinks.</p> 
<p><b>Example</b> &minus; JDBC channel, File system channel, Memory channel, etc.</p>
<h3>Sink</h3>
<p>A <b>sink</b> stores the data into centralized stores like HBase and HDFS. It consumes the data (events) from the channels and delivers it to the destination. The destination of the sink might be another agent or the central stores.</p>
<p><b>Example</b> &minus; HDFS sink</p>
<p><b>Note</b> &minus; A flume agent can have multiple sources, sinks and channels. We have listed all the supported sources, sinks, channels in the Flume configuration chapter of this tutorial.</p>
<h2>Additional Components of Flume Agent</h2>
<p>What we have discussed above are the primitive components of the agent. In addition to this, we have a few more components that play a vital role in transferring the events from the data generator to the centralized stores.</p>
<h3>Interceptors</h3>
<p>Interceptors are used to alter/inspect flume events which are transferred between source and channel.</p>
<h3>Channel Selectors</h3>
<p>These are used to determine which channel is to be opted to transfer the data in case of multiple channels. There are two types of channel selectors &minus;</p>
<ul class="list">
<li><p><b>Default channel selectors</b> &minus; These are also known as replicating channel selectors they replicates all the events in each channel.</p></li>
<li><p><b>Multiplexing channel selectors</b> &minus; These decides the channel to send an event based on the address in the header of that event.</p></li>
</ul>
<h3>Sink Processors</h3>
<p>These are used to invoke a particular sink from the selected group of sinks. These are used to create failover paths for your sinks or load balance events across multiple sinks from a channel.</p>
<h1>Apache Flume - Data Flow</h1>
<p>Flume is a framework which is used to move log data into HDFS. Generally events and log data are generated by the log servers and these servers have Flume agents running on them. These agents receive the data from the data generators.</p>
<p>The data in these agents will be collected by an intermediate node known as <b>Collector</b>. Just like agents, there can be multiple collectors in Flume.</p>
<p>Finally, the data from all these collectors will be aggregated and pushed to a centralized store such as HBase or HDFS. The following diagram explains the data flow in Flume.</p>
<img src="/apache_flume/images/flume_dataflow.jpg" alt="Flume DataFlow" />
<h2>Multi-hop Flow</h2>
<p>Within Flume, there can be multiple agents and before reaching the final destination, an event may travel through more than one agent. This is known as <b>multi-hop flow</b>.</p>
<h2>Fan-out Flow</h2>
<p>The dataflow from one source to multiple channels is known as <b>fan-out flow</b>. It is of two types &minus;</p>
<ul class="list">
<li><p><b>Replicating</b> &minus; The data flow where the data will be replicated in all the configured channels.</p></li>
<li><p><b>Multiplexing</b> &minus; The data flow where the data will be sent to a selected channel which is mentioned in the header of the event.</p></li>
</ul>
<h2>Fan-in Flow</h2>
<p>The data flow in which the data will be transferred from many sources to one channel is known as <b>fan-in flow</b>.</p>
<h2>Failure Handling</h2>
<p>In Flume, for each event, two transactions take place: one at the sender and one at the receiver. The sender sends events to the receiver. Soon after receiving the data, the receiver commits its own transaction and sends a “received” signal to the sender.  After receiving the signal, the sender commits its transaction. (Sender will not commit its transaction till it receives a signal from the receiver.)</p>
<h1>Apache Flume - Environment</h1>
<p>We already discussed the architecture of Flume in the previous chapter. In this chapter, let us see how to download and setup Apache Flume.</p>
<p>Before proceeding further, you need to have a Java environment in your system. So first of all, make sure you have Java installed in your system. For some examples in this tutorial, we have used Hadoop HDFS (as sink). Therefore, we would recommend that you go install Hadoop along with Java. To collect more information, follow the link &minus;<a target="_blank" rel="nofollow" href="http://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm">http://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm</a></p>
<h2>Installing Flume</h2>
<p>First of all, download the latest version of Apache Flume software from the website <a target="_blank" rel="nofollow" href="https://flume.apache.org/"> https://flume.apache.org/</a>.</p>
<h3>Step 1</h3>
<p>Open the website. Click on the <b>download</b> link on the left-hand side of the home page. It will take you to the download page of Apache Flume.</p>
<img src="/apache_flume/images/installing_flume.jpg" alt="Installing Flume" />
<h3>Step 2</h3>
<p>In the Download page, you can see the links for binary and source files of Apache Flume. Click on the link <a target="_blank" rel="nofollow" href="http://www.apache.org/dyn/closer.cgi/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz">apache-flume-1.6.0-bin.tar.gz</a></p>
<p>You will be redirected to a list of mirrors where you can start your download by clicking any of these mirrors. In the same way, you can download the source code of Apache Flume by clicking on <a target="_blank" rel="nofollow" href="http://www.apache.org/dyn/closer.cgi/flume/1.6.0/apache-flume-1.6.0-src.tar.gz">apache-flume-1.6.0-src.tar.gz</a>.</p>
<h3>Step 3</h3>
<p>Create a directory with the name Flume in the same directory where the installation directories of <b>Hadoop</b>, <b>HBase</b>, and other software were installed (if you have already installed any) as shown below.</p>
<pre class="result notranslate">
$ mkdir Flume 
</pre>
<h3>Step 4</h3>
<p>Extract the downloaded tar files as shown below.</p>
<pre class="result notranslate">
$ cd Downloads/ 
$ tar zxvf apache-flume-1.6.0-bin.tar.gz  
$ tar zxvf apache-flume-1.6.0-src.tar.gz
</pre>
<h3>Step 5</h3>
<p>Move the content of apache-<b>flume-1.6.0-bin.tar</b> file to the <b>Flume</b> directory created earlier as shown below. (Assume we have created the Flume directory in the local user named Hadoop.)</p>
<pre class="result notranslate">
$ mv apache-flume-1.6.0-bin.tar/* /home/Hadoop/Flume/
</pre>
<h2>Configuring Flume</h2>
<p>To configure Flume, we have to modify three files namely, <b>flume-env.sh, flumeconf.properties,</b> and <b>bash.rc</b>.</p>
<h3>Setting the Path / Classpath</h3>
<p>In the <b>.bashrc</b> file, set the home folder, the path, and the classpath for Flume as shown below.</p>
<img src="/apache_flume/images/setting_the_path.jpg" alt="setting the path" />
<h3>conf Folder</h3>
<p>If you open the <b>conf</b> folder of Apache Flume, you will have the following four files &minus;</p>
<ul class="list">
<li>flume-conf.properties.template,</li>
<li>flume-env.sh.template,</li>
<li>flume-env.ps1.template, and</li>
<li>log4j.properties.</li>
</ul>
<img src="/apache_flume/images/conf_folder.jpg" alt="conf Folder" />
<p>Now rename</p>
<ul class="list">
<li><p><b>flume-conf.properties.template</b> file as <b>flume-conf.properties</b> and</p></li>
<li><p><b>flume-env.sh.template</b> as <b>flume-env.sh</b></p></li>
</ul>
<h3>flume-env.sh</h3>
<p>Open <b>flume-env.sh</b> file and set the <b>JAVA_Home</b> to the folder where Java was installed in your system.</p>
<img src="/apache_flume/images/flume_env_sh.jpg" alt="flume-env.sh" />
<h2>Verifying the Installation</h2>
<p>Verify the installation of Apache Flume by browsing through the <b>bin</b> folder and typing the following command.</p>
<pre class="result notranslate">
$ ./flume-ng 
</pre>
<p>If you have successfully installed Flume, you will get a help prompt of Flume as shown below.</p>
<img src="/apache_flume/images/verifying_the_installation.jpg" alt="Verifying the Installation" />
<h1>Apache Flume - Configuration</h1>
<p>After installing Flume, we need to configure it using the configuration file which is a Java property file having <b>key-value pairs</b>. We need to pass values to the keys in the file.</p>
<p>In the Flume configuration file, we need to &minus;</p>
<ul class="list">
<li>Name the components of the current agent.</li>
<li>Describe/Configure the source.</li>
<li>Describe/Configure the sink.</li>
<li>Describe/Configure the channel.</li>
<li>Bind the source and the sink to the channel.</li>
</ul>
<p>Usually we can have multiple agents in Flume. We can differentiate each agent by using a unique name. And using this name, we have to configure each agent.</p>
<h2>Naming the Components</h2>
<p>First of all, you need to name/list the components such as sources, sinks, and the channels of the agent, as shown below.</p>
<pre class="result notranslate">
agent_name.sources = source_name 
agent_name.sinks = sink_name 
agent_name.channels = channel_name 
</pre>
<p>Flume supports various sources, sinks, and channels. They are listed in the table given below.</p>
<table class="table table-bordered">
<tr>
<th>Sources</th>
<th>Channels</th>
<th>Sinks</th>
</tr>
<tr>
<td>
<ul class="list">
<li>Avro Source</li>
<li>Thrift Source</li>
<li>Exec Source</li>
<li>JMS Source</li>
<li>Spooling Directory Source</li>
<li>Twitter 1&#37; firehose Source</li>
<li>Kafka Source</li>
<li>NetCat Source</li>
<li>Sequence Generator Source</li>
<li>Syslog Sources</li>
<li>Syslog TCP Source</li>
<li>Multiport Syslog TCP Source</li>
<li>Syslog UDP Source</li>
<li>HTTP Source</li>
<li>Stress Source</li>
<li>Legacy Sources</li>
<li>Thrift Legacy Source</li>
<li>Custom Source</li>
<li>Scribe Source</li>
</ul>
</td>
<td>
<ul class="list">
<li>Memory Channel</li>
<li>JDBC Channel</li>
<li>Kafka Channel</li>
<li>File Channel</li>
<li>Spillable Memory Channel</li>
<li>Pseudo Transaction Channel</li>
</ul>
</td>
<td>
<ul class="list">
<li>HDFS Sink</li>
<li>Hive Sink</li>
<li>Logger Sink</li>
<li>Avro Sink</li>
<li>Thrift Sink</li>
<li>IRC Sink</li>
<li>File Roll Sink</li>
<li>Null Sink</li>
<li>HBaseSink</li>
<li>AsyncHBaseSink</li>
<li>MorphlineSolrSink</li>
<li>ElasticSearchSink</li>
<li>Kite Dataset Sink</li>
<li>Kafka Sink</li>
</ul>
</td>
</tr>
</table>
<p>You can use any of them. For example, if you are transferring Twitter data using Twitter source through a memory channel to an HDFS sink, and the agent name id <b>TwitterAgent</b>, then</p>
<pre class="result notranslate">
TwitterAgent.sources = Twitter 
TwitterAgent.channels = MemChannel 
TwitterAgent.sinks = HDFS 
</pre>
<p>After listing the components of the agent, you have to describe the source(s), sink(s), and channel(s) by providing values to their properties.</p>
<h2>Describing the Source</h2>
<p>Each source will have a separate list of properties. The property named “type” is common to every source, and it is used to specify the type of the source we are using.</p>
<p>Along with the property “type”, it is needed to provide the values of all the <b>required</b> properties of a particular source to configure it, as shown below.</p>
<pre class="result notranslate">
agent_name.sources. source_name.type = value 
agent_name.sources. source_name.property2 = value 
agent_name.sources. source_name.property3 = value 
</pre>
<p>For example, if we consider the <b>twitter source</b>, following are the properties to which we <i>must</i> provide values to configure it.</p>
<pre class="result notranslate">
TwitterAgent.sources.Twitter.type = Twitter (type name) 
TwitterAgent.sources.Twitter.consumerKey =  
TwitterAgent.sources.Twitter.consumerSecret = 
TwitterAgent.sources.Twitter.accessToken =   
TwitterAgent.sources.Twitter.accessTokenSecret = 
</pre>
<h2>Describing the Sink</h2>
<p>Just like the source, each sink will have a separate list of properties. The property named “type” is common to every sink, and it is used to specify the type of the sink we are using. Along with the property “type”, it is needed to provide values to all the <b>required</b> properties of a particular sink to configure it, as shown below.</p>
<pre class="result notranslate">
agent_name.sinks. sink_name.type = value 
agent_name.sinks. sink_name.property2 = value 
agent_name.sinks. sink_name.property3 = value
</pre>
<p>For example, if we consider <b>HDFS sink</b>, following are the properties to which we <i>must</i> provide values to configure it.</p>
<pre class="result notranslate">
TwitterAgent.sinks.HDFS.type = hdfs (type name)  
TwitterAgent.sinks.HDFS.hdfs.path = HDFS directory’s Path to store the data
</pre>
<h2>Describing the Channel</h2>
<p>Flume provides various channels to transfer data between sources and sinks. Therefore, along with the sources and the channels, it is needed to describe the channel used in the agent.</p>
<p>To describe each channel, you need to set the required properties, as shown below.</p>
<pre class="result notranslate">
agent_name.channels.channel_name.type = value 
agent_name.channels.channel_name. property2 = value 
agent_name.channels.channel_name. property3 = value 
</pre>
<p>For example, if we consider <b>memory channel</b>, following are the properties to which we <i>must</i> provide values to configure it.</p>
<pre class="result notranslate">
TwitterAgent.channels.MemChannel.type = memory (type name)
</pre>
<h2>Binding the Source and the Sink to the Channel</h2>
<p>Since the channels connect the sources and sinks, it is required to bind both of them to the channel, as shown below.</p>
<pre class="result notranslate">
agent_name.sources.source_name.channels = channel_name 
agent_name.sinks.sink_name.channels = channel_name 
</pre>
<p>The following example shows how to bind the sources and the sinks to a channel. Here, we consider <b>twitter source, memory channel,</b> and <b>HDFS sink</b>.</p>
<pre class="result notranslate">
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sinks.HDFS.channels = MemChannel 
</pre>
<h2>Starting a Flume Agent</h2>
<p>After configuration, we have to start the Flume agent. It is done as follows &minus;</p>
<pre class="result notranslate">
$ bin/flume-ng agent --conf ./conf/ -f conf/twitter.conf 
Dflume.root.logger=DEBUG,console -n TwitterAgent 
</pre>
<p>where &minus;</p>
<ul class="list">
<li><p><b>agent</b> &minus; Command to start the Flume agent</p></li>
<li><p><b>--conf ,-c&lt;conf&gt;</b> &minus; Use configuration file in the conf directory</p></li>
<li><p><b>-f&lt;file&gt;</b> &minus; Specifies a config file path, if missing</p></li>
<li><p><b>--name, -n &lt;name&gt;</b> &minus; Name of the twitter agent</p></li>
<li><p><b>-D property =value</b> &minus; Sets a Java system property value.</p></li>
</ul>
<h1>Apache Flume - Fetching Twitter Data</h1>
<p>Using Flume, we can fetch data from various services and transport it to centralized stores (HDFS and HBase). This chapter explains how to fetch data from Twitter service and store it in HDFS using Apache Flume.</p>
<p>As discussed in Flume Architecture, a webserver generates log data and this data is collected by an agent in Flume. The channel buffers this data to a sink, which finally pushes it to centralized stores.</p>
<p>In the example provided in this chapter, we will create an application and get the tweets from it using the experimental twitter source provided by Apache Flume. We will use the memory channel to buffer these tweets and HDFS sink to push these tweets into the HDFS.</p>
<img src="/apache_flume/images/fetch_data.jpg" alt="Fetch Data" />
<p>To fetch Twitter data, we will have to follow the steps given below &minus;</p>
<ul class="list">
<li>Create a twitter Application</li>
<li>Install / Start HDFS</li>
<li>Configure Flume</li>
</ul>
<h2>Creating a Twitter Application</h2>
<p>In order to get the tweets from Twitter, it is needed to create a Twitter application. Follow the steps given below to create a Twitter application.</p>
<h3>Step 1</h3>
<p>To create a Twitter application, click on the following link <a target="_blank" rel="nofollow" href="https://apps.twitter.com/"> https://apps.twitter.com/</a>. Sign in to your Twitter account. You will have a Twitter Application Management window where you can create, delete, and manage Twitter Apps.</p>
<img src="/apache_flume/images/application_management_window.jpg " alt="Application Management window" />
<h3>Step 2</h3>
<p>Click on the <b>Create New App</b> button. You will be redirected to a window where you will get an application form in which you have to fill in your details in order to create the App. While filling the website address, give the complete URL pattern, for example, <a target="_blank" rel="nofollow" href="http://example.com/">http://example.com.</a></p>
<img src="/apache_flume/images/create_an_application.jpg" alt="Create an Application" />
<h3>Step 3</h3>
<p>Fill in the details, accept the <b>Developer Agreement</b> when finished, click on the <b>Create your Twitter application button</b> which is at the bottom of the page. If everything goes fine, an App will be created with the given details as shown below.</p>
<img src="/apache_flume/images/application_created.jpg" alt="Application created" />
<h3>Step 4</h3>
<p>Under <b>keys and Access Tokens</b> tab at the bottom of the page, you can observe a button named <b>Create my access token</b>. Click on it to generate the access token.</p>
<img src="/apache_flume/images/key_access_tokens.jpg" alt="Key Access Tokens" />
<h3>Step 5</h3>
<p>Finally, click on the <b>Test OAuth</b> button which is on the right side top of the page. This will lead to a page which displays your <b>Consumer key, Consumer secret, Access token,</b> and <b>Access token secret</b>. Copy these details. These are useful to configure the agent in Flume.</p>
<img src="/apache_flume/images/oauth_tool.jpg" alt="OAuth Tool" />
<h2>Starting HDFS</h2>
<p>Since we are storing the data in HDFS, we need to install / verify Hadoop. Start Hadoop and create a folder in it to store Flume data. Follow the steps given below before configuring Flume.</p>
<h3>Step 1: Install / Verify Hadoop</h3>
<p>Install <a target="_blank" rel="nofollow" href="http://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm">Hadoop</a>. If Hadoop is already installed in your system, verify the installation using Hadoop version command, as shown below.</p>
<pre class="result notranslate">
$ hadoop version 
</pre>
<p>If your system contains Hadoop, and if you have set the path variable, then you will get the following output &minus;</p>
<pre class="result notranslate">
Hadoop 2.6.0 
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 
e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1 
Compiled by jenkins on 2014-11-13T21:10Z 
Compiled with protoc 2.5.0 
From source with checksum 18e43357c8f927c0695f1e9522859d6a 
This command was run using /home/Hadoop/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar
</pre> 
<h3>Step 2: Starting Hadoop</h3>
<p>Browse through the <b>sbin</b> directory of Hadoop and start yarn and Hadoop dfs (distributed file system) as shown below.</p>
<pre class="result notranslate">
cd /$Hadoop_Home/sbin/ 
$ start-dfs.sh 
localhost: starting namenode, logging to
   /home/Hadoop/hadoop/logs/hadoop-Hadoop-namenode-localhost.localdomain.out 
localhost: starting datanode, logging to 
   /home/Hadoop/hadoop/logs/hadoop-Hadoop-datanode-localhost.localdomain.out 
Starting secondary namenodes [0.0.0.0] 
starting secondarynamenode, logging to 
   /home/Hadoop/hadoop/logs/hadoop-Hadoop-secondarynamenode-localhost.localdomain.out
  
$ start-yarn.sh 
starting yarn daemons 
starting resourcemanager, logging to 
   /home/Hadoop/hadoop/logs/yarn-Hadoop-resourcemanager-localhost.localdomain.out 
localhost: starting nodemanager, logging to 
   /home/Hadoop/hadoop/logs/yarn-Hadoop-nodemanager-localhost.localdomain.out 
</pre>
<h3>Step 3: Create a Directory in HDFS</h3>
<p>In Hadoop DFS, you can create directories using the command <b>mkdir</b>. Browse through it and create a directory with the name <b>twitter_data</b> in the required path as shown below.</p>
<pre class="result notranslate">
$cd /$Hadoop_Home/bin/ 
$ hdfs dfs -mkdir hdfs://localhost:9000/user/Hadoop/twitter_data 
</pre>
<h2>Configuring Flume</h2>
<p>We have to configure the source, the channel, and the sink using the configuration file in the <b>conf</b> folder. The example given in this chapter uses an experimental source provided by Apache Flume named <b>Twitter 1&#37; Firehose</b> Memory channel and HDFS sink.</p>
<h3>Twitter 1&#37; Firehose Source</h3>
<p>This source is highly experimental. It connects to the 1&#37; sample Twitter Firehose using streaming API and continuously downloads tweets, converts them to Avro format, and sends Avro events to a downstream Flume sink.</p>
<p>We will get this source by default along with the installation of Flume. The <b>jar</b> files corresponding to this source can be located in the <b>lib</b> folder as shown below.</p>
<img src="/apache_flume/images/twitter_jarfiles.jpg" alt="Twitter Jar Files" />
<h3>Setting the classpath</h3>
<p>Set the <b>classpath</b> variable to the <b>lib</b> folder of Flume in <b>Flume-env.sh</b> file as shown below.</p>
<pre class="result notranslate">
export CLASSPATH=$CLASSPATH:/FLUME_HOME/lib/* 
</pre>
<p>This source needs the details such as <b>Consumer key, Consumer secret, Access token,</b> and <b>Access token secret</b> of a Twitter application. While configuring this source, you have to provide values to the following properties &minus;</p>
<ul class="list">
<li><p><b>Channels</b></p></li>
<li><p><b>Source type : org.apache.flume.source.twitter.TwitterSource</b></p></li>
<li><p><b>consumerKey</b> &minus; The OAuth consumer key</p></li>
<li><p><b>consumerSecret</b> &minus; OAuth consumer secret</b></p></li>
<li><p><b>accessToken</b> &minus; OAuth access token</p></li>
<li><p><b>accessTokenSecret</b> &minus; OAuth token secret</p></li>
<li><p><b>maxBatchSize</b> &minus; Maximum number of twitter messages that should be in a twitter batch. The default value is 1000 (optional).</p></li>
<li><p><b>maxBatchDurationMillis</b> &minus; Maximum number of milliseconds to wait before closing a batch. The default value is 1000 (optional).</p></li>
</ul>
<h3>Channel</h3>
<p>We are using the memory channel. To configure the memory channel, you <i>must</i> provide value to the type of the channel.</p>
<ul class="list">
<li><p><b>type</b> &minus; It holds the type of the channel. In our example, the type is <b>MemChannel</b>.</p></li>
<li><p><b>Capacity</b> &minus; It is the maximum number of events stored in the channel. Its default value is 100 (optional).</p></li>
<li><p><b>TransactionCapacity</b> &minus; It is the maximum number of events the channel accepts or sends. Its default value is 100 (optional).</p></li>
</ul>
<h3>HDFS Sink</h3>
<p>This sink writes data into the HDFS. To configure this sink, you <i>must</i> provide the following details.</p>
<ul class="list">
<li><p><b>Channel</b></p></li>
<li><p><b>type</b> &minus; hdfs</p></li>
<li><p><b>hdfs.path</b> &minus; the path of the directory in HDFS where data is to be stored.</p></li>
</ul>
<p>And we can provide some optional values based on the scenario. Given below are the optional properties of the HDFS sink that we are configuring in our application.</p>
<ul class="list">
<li><p><b>fileType</b> &minus; This is the required file format of our HDFS file. <b>SequenceFile, DataStream</b> and <b>CompressedStream</b> are the three types available with this stream. In our example, we are using the <b>DataStream</b>.</p></li>
<li><p><b>writeFormat</b> &minus; Could be either text or writable.</p></li>
<li><p><b>batchSize</b> &minus; It is the number of events written to a file before it is flushed into the HDFS. Its default value is 100.</p></li>
<li><p><b>rollsize</b> &minus; It is the file size to trigger a roll. It default value is 100.</p></li>
<li><p><b>rollCount</b> &minus; It is the number of events written into the file before it is rolled. Its default value is 10.</p></li>
</ul>
<h2>Example – Configuration File</h2>
<p>Given below is an example of the configuration file. Copy this content and save as <b>twitter.conf</b> in the conf folder of Flume.</p>
<pre class="result notranslate">
# Naming the components on the current agent. 
TwitterAgent.sources = Twitter 
TwitterAgent.channels = MemChannel 
TwitterAgent.sinks = HDFS
  
# Describing/Configuring the source 
TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource
TwitterAgent.sources.Twitter.consumerKey = Your OAuth consumer key
TwitterAgent.sources.Twitter.consumerSecret = Your OAuth consumer secret 
TwitterAgent.sources.Twitter.accessToken = Your OAuth consumer key access token 
TwitterAgent.sources.Twitter.accessTokenSecret = Your OAuth consumer key access token secret 
TwitterAgent.sources.Twitter.keywords = tutorials point,java, bigdata, mapreduce, mahout, hbase, nosql
  
# Describing/Configuring the sink 

TwitterAgent.sinks.HDFS.type = hdfs 
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://localhost:9000/user/Hadoop/twitter_data/
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream 
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text 
TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0 
TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000 
 
# Describing/Configuring the channel 
TwitterAgent.channels.MemChannel.type = memory 
TwitterAgent.channels.MemChannel.capacity = 10000 
TwitterAgent.channels.MemChannel.transactionCapacity = 100
  
# Binding the source and sink to the channel 
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sinks.HDFS.channel = MemChannel 
</pre>
<h2>Execution</h2>
<p>Browse through the Flume home directory and execute the application as shown below.</p>
<pre class="result notranslate">
$ cd $FLUME_HOME 
$ bin/flume-ng agent --conf ./conf/ -f conf/twitter.conf 
Dflume.root.logger=DEBUG,console -n TwitterAgent
</pre>
<p>If everything goes fine, the streaming of tweets into HDFS will start. Given below is the snapshot of the command prompt window while fetching tweets.</p>
<img src="/apache_flume/images/fetching_tweets.jpg" alt="Fetching Tweets" />
<h2>Verifying HDFS</h2>
<p>You can access the Hadoop Administration Web UI using the URL given below.</p>
<pre class="result notranslate">
http://localhost:50070/ 
</pre>
<p>Click on the dropdown named <b>Utilities</b> on the right-hand side of the page. You can see two options as shown in the snapshot given below.</p>
<img src="/apache_flume/images/verifying_the_hdfs.jpg" alt="Verifying HDFS" />
<p>Click on <b>Browse the file system</b> and enter the path of the HDFS directory where you have stored the tweets. In our example, the path will be <b>/user/Hadoop/twitter_data/</b>.  Then, you can see the list of twitter log files stored in HDFS as given below.</p>
<img src="/apache_flume/images/browse_file_system.jpg" alt="Browse the file system" />
<h1>Apache Flume - Sequence Generator Source</h1>
<p>In the previous chapter, we have seen how to fetch data from twitter source to HDFS. This chapter explains how to fetch data from <b>Sequence generator</b>.</p>
<h2>Prerequisites</h2>
<p>To run the example provided in this chapter, you need to install <b>HDFS</b> along with <b>Flume</b>. Therefore, verify Hadoop installation and start the HDFS before proceeding further. (Refer the previous chapter to learn how to start the HDFS).</p>
<h2>Configuring Flume</h2>
<p>We have to configure the source, the channel, and the sink using the configuration file in the <b>conf</b> folder. The example given in this chapter uses a <b>sequence generator source</b>, a <b>memory channel</b>, and an <b>HDFS sink</b>.</p>
<h3>Sequence Generator Source</h3>
<p>It is the source that generates the events continuously. It maintains a counter that starts from 0 and increments by 1. It is used for testing purpose. While configuring this source, you must provide values to the following properties &minus;</p>
<ul class="list">
<li><p><b>Channels</b></p></li>
<li><p><b>Source type</b> &minus; seq</p></li>
</ul>
<h3>Channel</h3>
<p>We are using the <b>memory</b> channel. To configure the memory channel, you <i>must</i> provide a value to the type of the channel. Given below are the list of properties that you need to supply while configuring the memory channel &minus;</p>
<ul class="list">
<li><p><b>type</b> &minus; It holds the type of the channel. In our example the type is MemChannel.</p></li>
<li><p><b>Capacity</b> &minus; It is the maximum number of events stored in the channel. Its default value is 100. (optional)</p></li>
<li><p><b>TransactionCapacity</b> &minus; It is the maximum number of events the channel accepts or sends. Its default is 100. (optional).</p></li>
</ul>
<h3>HDFS Sink</h3>
<p>This sink writes data into the HDFS. To configure this sink, you <i>must</i> provide the following details.</p>
<ul class="list">
<li><p><b>Channel</b></p></li>
<li><p><b>type</b> &minus; hdfs</p></li>
<li><p><b>hdfs.path</b> &minus; the path of the directory in HDFS where data is to be stored.</p></li>
</ul>
<p>And we can provide some optional values based on the scenario. Given below are the optional properties of the HDFS sink that we are configuring in our application.</p>
<ul class="list">
<li><p><b>fileType</b> &minus; This is the required file format of our HDFS file. <b>SequenceFile, DataStream</b> and <b>CompressedStream</b> are the three types available with this stream. In our example, we are using the <b>DataStream</b>.</p></li>
<li><p><b>writeFormat</b> &minus; Could be either text or writable.</p></li>
<li><p><b>batchSize</b> &minus; It is the number of events written to a file before it is flushed into the HDFS. Its default value is 100.</p></li>
<li><p><b>rollsize</b> &minus; It is the file size to trigger a roll. It default value is 100.</p></li>
<li><p><b>rollCount</b> &minus; It is the number of events written into the file before it is rolled. Its default value is 10.</p></li>
</ul>
<h2>Example – Configuration File</h2>
<p>Given below is an example of the configuration file. Copy this content and save as <b>seq_gen .conf</b> in the conf folder of Flume.</p>
<pre class="result notranslate">
# Naming the components on the current agent 

SeqGenAgent.sources = SeqSource   
SeqGenAgent.channels = MemChannel 
SeqGenAgent.sinks = HDFS 
 
# Describing/Configuring the source 
SeqGenAgent.sources.SeqSource.type = seq
  
# Describing/Configuring the sink
SeqGenAgent.sinks.HDFS.type = hdfs 
SeqGenAgent.sinks.HDFS.hdfs.path = hdfs://localhost:9000/user/Hadoop/seqgen_data/
SeqGenAgent.sinks.HDFS.hdfs.filePrefix = log 
SeqGenAgent.sinks.HDFS.hdfs.rollInterval = 0
SeqGenAgent.sinks.HDFS.hdfs.rollCount = 10000
SeqGenAgent.sinks.HDFS.hdfs.fileType = DataStream 
 
# Describing/Configuring the channel 
SeqGenAgent.channels.MemChannel.type = memory 
SeqGenAgent.channels.MemChannel.capacity = 1000 
SeqGenAgent.channels.MemChannel.transactionCapacity = 100 
 
# Binding the source and sink to the channel 
SeqGenAgent.sources.SeqSource.channels = MemChannel
SeqGenAgent.sinks.HDFS.channel = MemChannel 
</pre>
<h2>Execution</h2>
<p>Browse through the Flume home directory and execute the application as shown below.</p>
<pre class="result notranslate">
$ cd $FLUME_HOME 
$./bin/flume-ng agent --conf $FLUME_CONF --conf-file $FLUME_CONF/seq_gen.conf 
   --name SeqGenAgent 
</pre>
<p>If everything goes fine, the source starts generating sequence numbers which will be pushed into the HDFS in the form of log files.</p>
<p>Given below is a snapshot of the command prompt window fetching the data generated by the sequence generator into the HDFS.</p>
<img src="/apache_flume/images/data_generated.jpg" alt="Data Generated" />
<h2>Verifying the HDFS</h2>
<p>You can access the Hadoop Administration Web UI using the following URL &minus;</p>
<pre class="result notranslate">
http://localhost:50070/
</pre>
<p>Click on the dropdown named <b>Utilities</b> on the right-hand side of the page. You can see two options as shown in the diagram given below.</p>
<img src="/apache_flume/images/verifying_the_hdfs.jpg" alt="Verifying the HDFS" />
<p>Click on <b>Browse the file system</b> and enter the path of the HDFS directory where you have stored the data generated by the sequence generator.</p>
<p>In our example, the path will be <b>/user/Hadoop/ seqgen_data /</b>. Then, you can see the list of log files generated by the sequence generator, stored in the HDFS as given below.</p>
<img src="/apache_flume/images/browse_file_system.jpg" alt="Browse the file system" />
<h2>Verifying the Contents of the File</h2>
<p>All these log files contain numbers in sequential format. You can verify the contents of these file in the file system using the <b>cat</b> command as shown below.</p>
<img src="/apache_flume/images/verifying_the_contents_of_file.jpg" alt="Verifying the Contents of the File " />
<h1>Apache Flume - NetCat Source</h1>
<p>This chapter takes an example to explain how you can generate events and subsequently log them into the console. For this, we are using the <b>NetCat</b> source and the <b>logger</b> sink.</p>
<h2>Prerequisites</h2>
<p>To run the example provided in this chapter, you need to install <b>Flume</b>.</p>
<h2>Configuring Flume</h2>
<p>We have to configure the source, the channel, and the sink using the configuration file in the <b>conf</b> folder. The example given in this chapter uses a <b>NetCat Source, Memory channel</b>, and a <b>logger sink</b>.</p>
<h3>NetCat Source</h3>
<p>While configuring the NetCat source, we have to specify a port while configuring the source. Now the source (NetCat source) listens to the given port and receives each line we entered in that port as an individual event and transfers it to the sink through the specified channel.</p>
<p>While configuring this source, you have to provide values to the following properties &minus;</p>
<ul class="list">
<li><p><b>channels</b></p></li>
<li><p><b>Source type</b> &minus; netcat</p></li>
<li><p><b>bind</b> &minus; Host name or IP address to bind.</p></li>
<li><p><b>port</b> &minus; Port number to which we want the source to listen.</p></li>
</ul>
<h3>Channel</h3>
<p>We are using the <b>memory</b> channel. To configure the memory channel, you <i>must</i> provide a value to the type of the channel. Given below are the list of properties that you need to supply while configuring the memory channel &minus;</p>
<ul class="list">
<li><p><b>type</b> &minus; It holds the type of the channel. In our example, the type is <b>MemChannel</b>.</p></li>
<li><p><b>Capacity</b> &minus; It is the maximum number of events stored in the channel. Its default value is 100. (optional)</p></li>
<li><p><b>TransactionCapacity</b> &minus; It is the maximum number of events the channel accepts or sends. Its default value is 100. (optional).</p></li>
</ul>
<h3>Logger Sink</h3>
<p>This sink logs all the events passed to it. Generally, it is used for testing or debugging purpose. To configure this sink, you must provide the following details.</p>
<ul class="list">
<li><p><b>Channel</b></p></li>
<li><p><b>type</b> &minus; logger</p></li>
</ul>
<h2>Example Configuration File</h2>
<p>Given below is an example of the configuration file. Copy this content and save as <b>netcat.conf</b> in the conf folder of Flume.</p>
<pre class="result notranslate">
# Naming the components on the current agent
NetcatAgent.sources = Netcat   
NetcatAgent.channels = MemChannel 
NetcatAgent.sinks = LoggerSink  

# Describing/Configuring the source 
NetcatAgent.sources.Netcat.type = netcat 
NetcatAgent.sources.Netcat.bind = localhost
NetcatAgent.sources.Netcat.port = 56565  

# Describing/Configuring the sink 
NetcatAgent.sinks.LoggerSink.type = logger  

# Describing/Configuring the channel 
NetcatAgent.channels.MemChannel.type = memory 
NetcatAgent.channels.MemChannel.capacity = 1000 
NetcatAgent.channels.MemChannel.transactionCapacity = 100 
 
# Bind the source and sink to the channel 
NetcatAgent.sources.Netcat.channels = MemChannel
NetcatAgent.sinks. LoggerSink.channel = MemChannel
</pre>
<h2>Execution</h2>
<p>Browse through the Flume home directory and execute the application as shown below.</p>
<pre class="result notranslate">
$ cd $FLUME_HOME
$ ./bin/flume-ng agent --conf $FLUME_CONF --conf-file $FLUME_CONF/netcat.conf 
   --name NetcatAgent -Dflume.root.logger=INFO,console
</pre>
<p>If everything goes fine, the source starts listening to the given port. In this case, it is <b>56565</b>. Given below is the snapshot of the command prompt window of a NetCat source which has started and listening to the port 56565.</p>
<img src="/apache_flume/images/execution.jpg" alt="Execution" />
<h3>Passing Data to the Source</h3>
<p>To pass data to NetCat source, you have to open the port given in the configuration file. Open a separate terminal and connect to the source (56565) using the <b>curl</b> command.  When the connection is successful, you will get a message “<b>connected</b>” as shown below.</p>
<pre class="result notranslate">
$ curl telnet://localhost:56565 
connected 
</pre>
<p>Now you can enter your data line by line (after each line, you have to press Enter). The NetCat source receives each line as an individual event and you will get a received message “<b>OK</b>”.</p>
<p>Whenever you are done with passing data, you can exit the console by pressing (<b>Ctrl&plus;C</b>). Given below is the snapshot of the console where we have connected to the source using the <b>curl</b> command.</p>
<img src="/apache_flume/images/passing_data.jpg" alt="Passing Data" />
<p>Each line that is entered in the above console will be received as an individual event by the source. Since we have used the <b>Logger</b> sink, these events will be logged on to the console (source console) through the specified channel (memory channel in this case).</p>
<p>The following snapshot shows the NetCat console where the events are logged.</p>
<img src="/apache_flume/images/netcat_console.jpg" alt="NetCat console" />
<div class="mui-container-fluid button-borders show">
<div class="pre-btn">
<a href="/apache_flume/apache_flume_netcat_source.htm"><i class="fal fa-chevron-circle-left"></i> Previous Page</a>
</div>
<div class="nxt-btn">
<a href="/apache_flume/apache_flume_useful_resources.htm">Next Page <i class="fal fa-chevron-circle-right"></i>&nbsp;</a>
</div>
</div>
<div class="google-bottom-ads">
<div>Advertisements</div>
<script><!--
var width = 580;
var height = 400;
var format = "580x400_as";
if( window.innerWidth < 468 ){
   width = 300;
   height = 250;
   format = "300x250_as";
}
google_ad_client = "pub-7133395778201029";
google_ad_width = width;
google_ad_height = height;
google_ad_format = format;
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
<div class="space-bottom"></div>
</div>
</div>
<!-- Tutorial Content Ends Here -->
<!-- Right Column Starts Here -->
<div class="mui-col-md-2 google-right-ads">
<div class="space-top"></div>
<div class="google-right-ad" style="margin: 0px auto !important;margin-top:5px;">
<script><!--
google_ad_client = "pub-2537027957187252";
google_ad_width = 300;
google_ad_height = 250;
google_ad_format = "300x250_as";
google_ad_type = "image";
google_ad_channel ="";
//--></script>
<script src="https://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
</div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9012177"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9012177")})</script>
</div>
<div class="space-bottom"></div>
<div class="google-right-ad">
<div class="adsbyvli" data-ad-slot="vi_9013289"></div>
<script>(vitag.Init = window.vitag.Init || []).push(function(){viAPItag.display("vi_9013289")})</script>
</div>
<div class="space-bottom" style="margin-bottom:15px;"></div>
</div>
<!-- Right Column Ends Here -->
</div>
</div>
<div class="clear"></div>
<footer id="footer">
<div class="mui--text-center">
<div class="mui--text-caption mui--text-light">
<a href="/index.htm" class="logo"><img class="img-responsive" src="/images/logo-black.png" alt="Tutorials Point" title="Tutorials Point"></a>
</div>
<ul class="mui-list--inline mui--text-body2 mui--text-light">
<li><a href="/about/index.htm"><i class="fal fa-globe"></i> About us</a></li>
<li><a href="/about/about_terms_of_use.htm"><i class="fal fa-asterisk"></i> Terms of use</a></li>
<li><a href="/about/about_privacy.htm#cookies"> <i class="fal fa-shield-check"></i> Cookies Policy</a></li>
<li><a href="/about/faq.htm"><i class="fal fa-question-circle"></i> FAQ's</a></li>
<li><a href="/about/about_helping.htm"><i class="fal fa-hands-helping"></i> Helping</a></li>
<li><a href="/about/contact_us.htm"><i class="fal fa-map-marker-alt"></i> Contact</a></li>
</ul>
<div class="mui--text-caption mui--text-light bottom-copyright-text">&copy; Copyright 2019. All Rights Reserved.</div>
</div>
<div id="privacy-banner">
  <div>
    <p>
      We use cookies to provide and improve our services. By using our site, you consent to our Cookies Policy.
      <a id="banner-accept" href="#">Accept</a>
      <a id="banner-learn" href="/about/about_cookies.htm" target="_blank">Learn more</a>
    </p>
  </div>
</div>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-232293-17"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-232293-6');
</script>
</footer>
</body>
</html>
